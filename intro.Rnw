\documentclass[a4paper,10pt,twoside,francais]{report}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{varioref}
\usepackage{fancybox}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{lineno}
\usepackage{mparhack}
\usepackage{indentfirst}
\usepackage{makeidx}
\usepackage[nottoc]{tocbibind}
\usepackage{vmargin}
\usepackage{verbatim}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{picins}
\usepackage[sf,bf]{titlesec}
\usepackage{listings}
\usepackage{calc}
\usepackage[colorlinks=true,urlcolor=blue,pdfpagelabels]{hyperref}
\usepackage{ctable}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{lmodern}
\usepackage{epstopdf}


\definecolor{grisclair}{rgb}{0.97,0.97,0.97}
\definecolor{grisinterm}{rgb}{0.95,0.95,0.95}
\definecolor{grisfonce}{rgb}{0.5,0.5,0.5}
\definecolor{inputcol}{rgb}{0,0,0.5}
\definecolor{outputcol}{rgb}{0.5,0,0}


\FrenchFootnotes
\AddThinSpaceBeforeFootnotes
\VerbatimFootnotes
\vrefwarning

\makeindex


%%%% COMMANDES

\newcommand{\R}{\textsf{R}\xspace}
\newcommand{\rgrs}{\textsf{rgrs}\xspace}
\newcommand{\questionr}{\textsf{questionr}\xspace}
\newcommand{\margpar}[1]{\mbox{}\marginpar{\raggedright \hspace{0pt}\small\textit{#1}}}
\newcommand{\margrs}{\margpar{\rgrs}}
\newcommand{\marqr}{\margpar{\questionr}}
\newcommand{\rfunc}[1]{\texttt{#1}\index{#1@\texttt{#1}}}

%%%% ENVIRONNEMENTS

%\newenvironment{astuce}{\paragraph{\large \ding{'355} \normalsize}}{}
\newsavebox{\fcolbox}

\newenvironment{greyframe}%
{\setlength\fboxsep{10pt}%
  \setlength{\rightmargin}{\leftmargin}%
  \footnotesize\raggedright%
  \begin{lrbox}{\fcolbox}%
    \begin{minipage}{\linewidth-4\fboxsep}}%
    {\end{minipage}%
  \end{lrbox}%
  \vspace{3ex}%
  \begin{center}%
    \fcolorbox{grisfonce}{grisinterm}{\usebox{\fcolbox}}%
  \end{center}
  \vspace{3ex}%
}

\newenvironment{remarque}%
{\begin{greyframe}%
    \parpic[l]{\includegraphics[height=0.8cm,keepaspectratio=true]{img/note.png}}}%
{\end{greyframe}}

\newenvironment{astuce}%
{\begin{greyframe}%
    \parpic[l]{\includegraphics[height=0.8cm,keepaspectratio=true]{img/astuce.png}}}%
{\end{greyframe}}

\newenvironment{rstudio}%
{\begin{greyframe}%
    \parpic[l]{\includegraphics[height=0.8cm,keepaspectratio=true]{img/rstudio.png}}}%
{\end{greyframe}}

\newenvironment{important}%
{\begin{greyframe}%
    \parpic[l]{\includegraphics[height=0.8cm,keepaspectratio=true]{img/important.png}}}%
{\end{greyframe}}

\newenvironment{greyverb}%
{\setlength\fboxsep{10pt}%
  \setlength{\rightmargin}{\leftmargin}%
  \raggedright%
  \begin{lrbox}{\fcolbox}%
    \begin{minipage}{\linewidth-4\fboxsep}}%
    {\end{minipage}%
  \end{lrbox}%
  \vspace{3ex}%
  \begin{center}%
    \fcolorbox{grisinterm}{grisinterm}{\usebox{\fcolbox}}%
  \end{center}
  \vspace{3ex}%
}

%%%% EXERCICES
    
\newcounter{ex} 
\renewcommand\theex{\thechapter.\arabic{ex}}

\newenvironment{exo}[1]{%
  \begin{flushleft}%
    \refstepcounter{ex}%
    \noindent\textbf{\textsf{Exercice~\theex{}}}\\[0.5ex]%
    $\triangleright$ \textit{Solution page \pageref{sol_#1}\\[1ex] }%
    \label{exo_#1}}%
  {\end{flushleft}}

\newenvironment{solution}[1]{%
  \label{sol_#1}%
  \begin{flushleft}%
    \noindent\textbf{\textsf{\large{}Exercice~\ref{exo_#1}, page \pageref{exo_#1}}}\\[0.5ex]}%
  {\end{flushleft}}


\setpapersize[portrait]{A4}
%\setmarginsrb{left}{top}{right}{bottom}{headheight}{headsep}{footheight}{footsep}
\setmarginsrb{2.5cm}{1.5cm}{2.5cm}{2cm}{1cm}{1cm}{1cm}{1.5cm}
\setlength\marginparwidth{40pt}
%\setcounter{tocdepth}{2}

\title{\Huge{Introduction à l'analyse d'enquêtes avec \includegraphics[width=1.4cm]{img/Rlogo.jpg}}}
\author{à partir d'un document original de\\ Julien Barnier\\ \href{mailto:julien.barnier@ens-lyon.fr}{\texttt{julien.barnier@ens-lyon.fr}}\\ \\ complété par \\ Joseph Larmarange\\ \href{mailto:joseph.larmarange@ceped.org}{\texttt{joseph.larmarange@ceped.org}}}
\date{\today{}\\\vspace{3cm}\includegraphics[width=9cm]{img/xkcd_tech_chart.png}\\\footnotesize\texttt{http://xkcd.com/627/}}


\begin{document}

<<initialize,echo=FALSE,results="none",cache=FALSE>>=
options(width=75)
options(max.print="100")
options(prompt="R> ")
opts_chunk$set(fig.path="tmp/",
               dev="pdf",
               cache=TRUE,
               cache.path="cache/",
               prompt=TRUE,
               tidy=TRUE,
               highlight=TRUE,
               #size="footnotesize",
               comment=NA,
               background=".95;.95;.95",
               fig.align="center",
               out.width=".8\\textwidth")
@



\renewcommand{\chaptername}{Partie}
\renewcommand{\indexname}{Index des fonctions}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}

\maketitle
\thispagestyle{empty}

\pagestyle{fancy} 
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\fancyhead{}
\fancyhead[RO,LE]{\thepage}
\fancyhead[RE]{\textit{\nouppercase{\leftmark}}}
\fancyhead[LO]{\textit{\nouppercase{\rightmark}}}
\fancyfoot{}


\tableofcontents

\sloppy

\setlength{\parskip}{1ex plus0.1ex minus0.1ex}


\chapter{Introduction}


\section{À propos de ce document}

Ce document a pour objet de fournir une introduction à l'utilisation
du logiciel libre de traitement de données et d'analyse statistiques \R.
Il se veut le plus accessible possible, y compris pour ceux qui ne
sont pas particulièrement familiers avec l'informatique.

Ce document a été réalisé avec \Sexpr{R.Version()[["version.string"]]}.

Ce document est basé sur l'\textit{Introduction à \R} écrite par Julien Barnier
et accessible sur \url{http://alea.fr.eu.org/pages/intro-R}. Il a été complété 
de plusieurs chapitres par Joseph Larmarange. Cette version modifiée est
téléchargeable à l'adresse~:

\url{https://github.com/larmarange/intro-r/blob/CoursM2/intro.pdf?raw=true}

\section{Licence}

\parpic[r]{\includegraphics[width=2.5cm]{img/by-nc-sa_eu.eps}}
%\noindent
Ce document est diffusé sous licence \textit{Creative Commons
 Attribution - Pas d’utilisation commerciale - Partage dans les mêmes conditions}~:

%\noindent
\url{https://creativecommons.org/licenses/by-nc-sa/3.0/fr/}



\section{Remerciements}

Julien Barnier tient à remercier Mayeul Kauffmann, Julien Biaudet, Frédérique
Giraud, Joël Gombin et Joseph Larmarange pour leurs corrections et
suggestions. Et un remerciement plus particulier à Milan Bouchet-Valat pour sa
relecture très attentive et ses nombreuses et judicieuses remarques.

Joseph Larmarange tient à remercier Julien Barnier pour avoir mis son travail
sous licence \textit{Creative Commons} et Nicolas Robette pour avoir autorisé
la reproduction de son introduction à l'analyse de séquences (chapitre 
\ref{chapitre_sequences} \vpageref{chapitre_sequences}).


\section{Conventions typographiques}

Ce document suit un certain nombre de conventions typographiques
visant à en faciliter la lecture. Ainsi les noms de logiciel et
d'extensions sont indiqués en caractères sans empattement (\textsf{R},
\textsf{SAS}, \textsf{Linux}, \textsf{questionr}, \textsf{ade4}\ldots). Les
noms de fichiers sont imprimés avec une police à chasse fixe
(\texttt{test.R}, \texttt{data.txt}\ldots), tout comme les fonctions
\R (\rfunc{summary}, \rfunc{mean}, \rfunc{<-}\ldots).

Lorsqu'on présente des commandes saisies sous \R et leur résultat, la commande
saisie est indiquée avec une police à chasse fixe et précédée de l'invite de
commande \texttt{R>}~:

<<eval=FALSE>>=
summary(rnorm(100))
@

Le résultat de la commande tel qu'affiché par \R est également indiqué dans
une police à chasse fixe~:

<<echo=FALSE>>=
summary(rnorm(100))
@

Lorsque la commande \R est trop longue et répartie sur plusieurs
lignes, les lignes suivantes sont précédées du symbole \texttt{+}~:

<<eval=FALSE>>=
coo <- scatterutil.base(dfxy = dfxy, xax = xax, yax = yax, xlim = xlim, ylim = ylim, grid = grid, addaxes = addaxes, cgrid = cgrid, include.origin = include.origin)
@



\section{Présentation de \R}

\R est un langage orienté vers le traitement de données et l'analyse
statistique dérivé du langage \textsf{S}. Il est développé depuis une
vingtaine d'années par un groupe de volontaires de différents pays. C'est un
logiciel libre\footnotemark, publié sous licence GNU GPL.

\footnotetext{Pour plus d'informations sur ce qu'est un logiciel libre,
  voir~: \url{http://www.gnu.org/philosophy/free-sw.fr.html}}

L'utilisation de \R présente plusieurs avantages~:

\begin{itemize}
\item c'est un logiciel \textit{multiplateforme}, qui fonctionne aussi bien sur
  des sytèmes \textsf{Linux}, \textsf{Mac OS X} ou \textsf{Windows}~;
\item c'est un logiciel \textit{libre}, développé par ses utilisateurs et
  modifiable par tout un chacun~;
\item c'est un logiciel \textit{gratuit}~;
\item c'est un logiciel très puissant, dont les fonctionnalités de
  base peuvent être étendues à l'aide d'extensions\footnotemark~;
\item c'est un logiciel dont le développement est très actif et dont
  la communauté d'utilisateurs ne cesse de s'élargir~;
\item c'est un logiciel avec d'excellentes capacités graphiques.
\end{itemize}

\footnotetext{Il en existe actuellement plus de 4800, disponibles sur le
  \textit{Comprehensive R Archive Network} (CRAN)~: \url{http://cran.r-project.org/}}

Comme rien n'est parfait, on peut également trouver quelques inconvénients~:

\begin{itemize}
\item le logiciel, la documentation de référence et les principales
  ressources sont en anglais. Il est toutefois parfaitement possible
  d'utiliser \R sans spécialement maîtriser cette langue~;
\item il n'existe pas encore d'interface graphique pour \R équivalente à celle
  d'autres logiciels comme \textsf{SPSS} ou \textsf{Modalisa}\footnotemark. \R
  fonctionne à l'aide de scripts (des petits programmes) édités et exécutés au
  fur et à mesure de l'analyse, et se rapprocherait davantage de \textsf{SAS}
  dans son utilisation (mais avec une syntaxe et une philosophie très
  différentes). Ce point, qui peut apparaître comme un gros handicap, s'avère
  après un temps d'apprentissage être un mode d'utilisation d'une grande
  souplesse.
\item comme \R s'apparente davantage à un langage de programmation qu'à un
  logiciel proprement dite, la courbe d'apprentissage peut être un peu
  <<~raide~>>, notamment pour ceux n'ayant jamais programmé auparavant.
\end{itemize}

\footnotetext{Certaines extensions ou logiciels proposent cependant des
  interfaces graphiques plus ou moins généralistes. Voir la
  section~\ref{sec_guis}, \vpageref{sec_guis} }

\section{Philosophie de \R}

Deux points particuliers dans le fonctionnement de \R peuvent parfois
dérouter les utilisateurs habitués à d'autres logiciels~:

\begin{itemize}
\item sous \R, en général, on ne voit pas les données sur lesquelles
  on travaille~; on ne dispose pas en permanence d'une vue des données
  sous forme de tableau, comme sous \textsf{Modalisa} ou
  \textsf{SPSS}. Ceci peut être déroutant au début, mais on se
  rend vite compte qu'on n'a pas besoin de voir en permanence les
  données pour les analyser~;
\item avec les autres logiciels, en général la production d'une
  analyse génère un grand nombre de résultats de toutes sortes dans
  lesquels l'utilisateur est censé retrouver et isoler ceux qui
  l'intéressent. Avec \R, c'est l'inverse~: par défaut l'affichage est
  réduit au minimum, et c'est l'utilisateur qui demande à voir des
  résultats supplémentaires ou plus détaillés.
\end{itemize}

Inhabituel au début, ce fonctionnement permet en fait assez rapidement
de gagner du temps dans la conduite des analyses.



\chapter{Prise en main}

L'installation du logiciel proprement dite n'est pas décrite ici mais
indiquée dans l'annexe~\ref{sec_install},
page~\pageref{sec_install}. On part donc du principe que vous avez
sous la main un ordinateur avec une installation récente de \R, quel
que soit le système d'exploitation que vous utilisez (\textsf{Linux},
\textsf{Mac OS X} ou \textsf{Windows}).

\begin{rstudio}
  Le projet \textsf{RStudio} tend à s'imposer comme l'environnement de
  développement de référence pour \R, d'autant qu'il a l'avantage d'être
  libre, gratuit et multiplateforme. Son installation est décrite
  section~\ref{sec_rstudio} \vpageref{sec_rstudio}.
  
  Les astuces et informations spécifiques à \textsf{RStudio} seront présentées 
  tout au long de ce document dans des encadrés similaires à celui-là.
  
  \textsf{RStudio} peut tout à fait être utilisé pour découvrir et démarrer
  avec \R.
\end{rstudio}


\section{L'invite de commandes}

Une fois \R lancé, vous obtenez une fenêtre appelée
\textit{console}. Celle-ci contient un petit texte de bienvenue
ressemblant à peu près à ce qui suit\footnotemark~:

\footnotetext{La figure~\ref{fig_RGui} \vpageref{fig_RGui} montre
  l'interface par défaut sous \textsf{Windows}.}


\begin{figure}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{img/scr_RGui.png}
  \end{center}
  \caption{L'interface de \R sous \textsf{Windows} au démarrage}
  \label{fig_RGui}
\end{figure}


\begin{greyverb}
\begin{verbatim}
R version 3.0.1 (2013-05-16) -- "Good Sport"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R est un logiciel libre livré sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de détails.

(...)
\end{verbatim}
\end{greyverb}

\noindent
suivi d'une ligne commençant par le caractère \texttt{>} et sur laquelle
devrait se trouver votre curseur. Cette ligne est appelée
l'\textit{invite de commande} (ou \textit{prompt} en anglais). Elle
signifie que \R est disponible et en attente de votre prochaine
commande.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{img/scr_RStudio.png}
  \end{center}
  \caption{L'interface de \textsf{RStudio} sous \textsf{Windows} au démarrage}
  \label{fig_RStudio}
\end{figure}

\begin{rstudio}
  L'interface de \textsf{RStudio} se présente différement (voir 
  figure~\ref{fig_RStudio}). Elle est divisée en quatre parties. Le quadrant 
  haut-gauche est dédié aux fichiers sources (scripts). Le quadrant haut-droite
  fournit des informations sur vos données en mémoire et votre historique. Le
  quadrant bas-droite vous permet naviguer dans votre répertoire de travail, 
  affiche l'aide, vos graphiques et les extensions disponibles. Enfin, la 
  \textit{console} est affichée en bas à gauche. C'est elle qui nous intéresse
  pour le moment. Nous aborderons les autres quadrants plus loin dans ce 
  document.
\end{rstudio}

Nous allons tout de suite lui fournir une première commande~:

<<>>=
2 + 3  
@

Bien, nous savons désormais que \R sait faire les additions à un
chiffre\footnotemark. Nous pouvons désormais continuer avec d'autres
opérations arithmétiques de base~:

\footnotetext{La présence du \texttt{[1]} en début de ligne sera
  expliquée par la suite, \vpageref{explication_crochets}.}

<<>>=
8 - 12
14 * 25
-3 / 10
@

\begin{astuce}
  Une petite astuce très utile lorsque vous tapez des commandes
  directement dans la console~: en utilisant les flèches \textit{Haut}
  et \textit{Bas} du clavier, vous pouvez naviguer dans l'historique
  des commandes tapées précédemment, que vous pouvez alors facilement
  réexécuter ou modifier.
\end{astuce}

\begin{rstudio}
  Sous \textsf{RStudio}, l'onglet \textit{History} du quadrant haut-droite vous 
  permet de consulter l'historique des commandes que vous avez transmises à \R.
  Un double-clic sur une commande la recopiera automatiquement dans la console.
  Vous pouvez également sélectionner une ou plusieurs commandes puis cliquer sur
  \textit{To Console}. Voir également (en anglais) : \url{http://www.rstudio.com/ide/docs/using/history}
\end{rstudio}

Lorsqu'on fournit à \R une commande incomplète, celui-ci nous propose
de la compléter en nous présentant une invite de commande spéciale
utilisant les signe \texttt{+}. Imaginons par exemple que nous avons
malencontreusement tapé sur \texttt{Entrée} alors que nous souhaitions
calculer \texttt{4*3}:

\begin{greyverb}
\begin{verbatim}
4 *
\end{verbatim}
\end{greyverb}

On peut alors compléter la commande en saisissant simplement
\texttt{3}~:

<<incomp2,eval=TRUE,prompt=TRUE,tidy=FALSE>>=    
4 *
3
@ 

\begin{astuce}
  Pour des commandes plus complexes, il arrive parfois qu'on se
  retrouve coincé avec un invite \texttt{+} sans plus savoir comment
  compléter la saisie correctement. On peut alors annuler la commande
  en utilisant la touche \texttt{Echap} ou \texttt{Esc} sous
  \textsf{Windows}. Sous \textsf{Linux} on utilise le traditionnel
  \texttt{Control + C}.
\end{astuce}

À noter que les espaces autour des opérateurs n'ont pas d'importance
lorsque l'on saisit les commandes dans \R. Les trois commandes
suivantes sont donc équivalentes, mais on privilégie en général la
deuxième pour des raisons de lisibilité du code.

<<espaces,eval=FALSE,prompt=TRUE,tidy=FALSE>>=  
10+2
10 + 2
10       +       2
@ 



\section{Des objets}


\subsection{Objets simples}

Faire des opérations arithmétiques, c'est bien, mais sans doute pas
totalement suffisant. Notamment, on aimerait pouvoir réutiliser le
résultat d'une opération sans avoir à le resaisir ou à le
copier/coller.

Comme tout langage de programmation, \R permet de faire cela en
utilisant des \textit{objets}. Prenons tout de suite un exemple~:

<<>>=
x <- 2
@

Que signifie cette commande~? L'opérateur \rfunc{<-} est appelé
\textit{opérateur d'assignation}. Il prend une valeur quelconque à
droite et la place dans l'objet indiqué à gauche. La commande pourrait
donc se lire \textit{mettre la valeur 2 dans l'objet nommé} \texttt{x}.

On va ensuite pouvoir réutiliser cet objet dans d'autres calculs ou
simplement afficher son contenu~:

<<>>=
x + 3
x
@

\begin{remarque}
Par défaut, si on donne à \R seulement le nom d'un objet, il va se
débrouiller pour nous présenter son contenu d'une manière plus ou
moins lisible.
\end{remarque}

On peut utiliser autant d'objets qu'on veut. Ceux-ci peuvent contenir
des nombres, des chaînes de caractères (indiquées par des guillemets
droits \texttt{"}) et bien d'autres choses encore~:

<<>>=
x <- 27
y <- 10
foo <- x + y
foo
x <- "Hello"
foo <- x
foo
@

\begin{remarque}
  \label{rq_noms}
  Les noms d'objets peuvent contenir des lettres, des chiffres (mais
  ils ne peuvent pas commencer par un chiffre), les symboles
  \texttt{.} et \texttt{\_}, et doivent commencer par une lettre. \R
  fait la différence entre les majuscules et les minuscules, ce qui
  signifie que \texttt{x} et \texttt{X} sont deux objets
  différents. On évitera également d'utiliser des caractères accentués
  dans les noms d'objets, et comme les espaces ne sont pas autorisés
  on pourra les remplacer par un point ou un tiret bas.
  
Enfin, signalons que certains noms courts sont réservés par \R pour
son usage interne et doivent être évités. On citera notamment
\texttt{c, q, t, C, D, F, I, T, max, min}\ldots
\end{remarque}



\subsection{Vecteurs}

Imaginons maintenant que nous avons interrogé dix personnes au hasard
dans la rue et que nous avons relevé pour chacune d'elle sa taille en
centimètres. Nous avons donc une série de dix nombres que nous
souhaiterions pouvoir réunir de manière à pouvoir travailler sur
l'ensemble de nos mesures.

Un ensemble de données de même nature constituent pour \R un
\textit{vecteur} (en anglais \textit{vector}) et se construit à l'aide
d'un opérateur nommé \rfunc{c}\footnotemark. On l'utilise en lui
donnant la liste de nos données, entre parenthèses, séparées par des
virgules~:

\footnotetext{\rfunc{c} est l'abbréviation de \textit{combine}. Le
  nom de cette fonction est très court car on l'utilise très souvent.}

<<>>=
tailles <- c(167, 192, 173, 174, 172, 167, 171, 185, 163, 170)
@

Ce faisant, nous avons créé un objet nommé \texttt{tailles} et
comprenant l'ensemble de nos données, que nous pouvons afficher~:

<<>>=
tailles
@

Dans le cas où notre vecteur serait beaucoup plus grand, et
comporterait par exemple 40 tailles, on aurait le résultat suivant~:

<<echo=FALSE>>=
tailles <- c(144, 168, 179, 175, 182, 188, 167, 152, 163, 145, 176, 155, 
156, 164, 167, 155, 157, 185, 155, 169, 124, 178, 182, 195, 151, 
185, 159, 156, 184, 172, 156, 160, 183, 148, 182, 126, 177, 159, 
143, 161, 180, 169, 159, 185, 160)
@

<<>>=
tailles
@

\label{explication_crochets}
On a bien notre suite de quarante tailles, mais on peut remarquer la
présence de nombres entre crochets au début de chaque ligne
(\texttt{[1]}, \texttt{[18]} et \texttt{[35]}). En fait ces nombres
entre crochets indiquent la position du premier élément de la ligne
dans notre vecteur. Ainsi, le 185 en début de deuxième ligne est le
18\up{e} élément du vecteur, tandis que le 182 de la troisième ligne est
à la 35\up{e} position.

On en déduira d'ailleurs que lorsque l'on fait~:
<<>>=
2
@
\R considère en fait le nombre 2 comme un vecteur à un seul élément.

On peut appliquer des opérations arithmétiques simples directement sur
des vecteurs~:

<<>>=
tailles <- c(167, 192, 173, 174, 172, 167, 171, 185, 163, 170)
tailles + 20
tailles / 100
tailles ^ 2
@

On peut aussi combiner des vecteurs entre eux. L'exemple suivant
calcule l'indice de masse corporelle à partir de la taille et du
poids~:

<<>>=
tailles <- c(167, 192, 173, 174, 172, 167, 171, 185, 163, 170)
poids <- c(86, 74, 83, 50, 78, 66, 66, 51, 50, 55)
tailles.m <- tailles / 100
imc <- poids / (tailles.m ^ 2)
imc
@


\begin{remarque}
  Quand on fait des opérations sur les vecteurs, il faut veiller à
  soit utiliser un vecteur et un chiffre (dans des opérations du type
  \texttt{v * 2} ou \texttt{v + 10}), soit à utiliser des vecteurs de
  même longueur (dans des opérations du type \texttt{u + v}).
  
  Si on utilise des vecteurs de longueur différentes, on peut avoir
  quelques surprises\footnotemark.
\end{remarque}

\footnotetext{Quand \R effectue une opération avec deux vecteurs de
  longueurs différentes, il recopie le vecteur le plus court de
  manière à lui donner la même taille que le plus long, ce qui
  s'appelle la \textit{règle de recyclage} (\textit{recycling
    rule}). Ainsi, \texttt{c(1,2) + c(4,5,6,7,8)} vaudra l'équivalent
  de \texttt{c(1,2,1,2,1) + c(4,5,6,7,8)}.}


On a vu jusque-là des vecteurs composés de nombres, mais on peut tout
à fait créer des vecteurs composés de chaînes de caractères,
représentant par exemple les réponses à une question ouverte ou fermée~:

<<>>=
reponse <- c("Bac+2", "Bac", "CAP", "Bac", "Bac", "CAP", "BEP")
@


Enfin, notons que l'on peut accéder à un élément particulier du
vecteur en faisant suivre le nom du vecteur de crochets contenant le
numéro de l'élément désiré. Par exemple~:

<<>>=
reponse <- c("Bac+2", "Bac", "CAP", "Bac", "Bac", "CAP", "BEP")
reponse[2]
@

Cette opération s'appelle \textit{l'indexation} d'un vecteur. Il
s'agit ici de sa forme la plus simple, mais il en existe d'autres
beaucoup plus complexes. L'indexation des vecteurs et des tableaux
dans \R est l'un des éléments particulièrement souples et puissants du
langage (mais aussi l'un des plus délicats à comprendre et à
maîtriser). Nous en reparlerons section~\ref{sec_indexation}
\vpageref{sec_indexation}.

\begin{rstudio}
  Sous \textsf{RStudio}, vous avez du remarquer que ce dernier effectue une 
  coloration syntaxique. Lorsque vous tapez une commande, les valeurs numériques
  sont affichées dans une certaine couleur, les valeurs textuelles dans une 
  autre et les noms des fonctions dans une troisième. De plus, si vous tapez une
  parenthèse ouvrante, \textsf{RStudio} va créer automatiquement après le 
  curseur la parenthèse fermante correspondante (de même avec les guillements). 
  De plus, si vous placez le curseur juste après une parenthèse fermante, la
  parenthèse ouvrante correspondante sera surlignée, ce qui sera bien pratique
  lors de la rédaction de commandes complexes.
\end{rstudio}

\section{Des fonctions}

Nous savons désormais faire des opérations simples sur des nombres et
des vecteurs, stocker ces données et résultats dans des objets pour
les réutiliser par la suite.

Pour aller un peu plus loin nous allons aborder, après les
\textit{objets}, l'autre concept de base de \R, à savoir les
\textit{fonctions}. Une fonction se caractérise de la manière
suivante~:

\begin{itemize}
\item elle a un nom~;
\item elle accepte des arguments (qui peuvent avoir un nom ou pas)~;
\item elle retourne un résultat et peut effectuer une action comme
  dessiner un graphique, lire un fichier, etc.~;
\end{itemize}

En fait rien de bien nouveau puisque nous avons déjà utilisé plusieurs
fonctions jusqu'ici, dont la plus visible est la fonction
\rfunc{c}. Dans la ligne suivante~:

<<>>=
reponse <- c("Bac+2", "Bac", "CAP", "Bac", "Bac", "CAP", "BEP")
@

\noindent
on fait appel à la fonction nommée \rfunc{c}, on lui passe en
arguments (entre parenthèses et séparées par des virgules) une série
de chaînes de caractères, et elle retourne comme résultat un vecteur
de chaînes de caractères, que nous stockons dans l'objet
\texttt{tailles}.

Prenons tout de suite d'autres exemples de fonctions courantes~:

<<>>=
tailles <- c(167, 192, 173, 174, 172, 167, 171, 185, 163, 170)
length(tailles)
mean(tailles)
var(tailles)
@

Ici, la fonction \rfunc{length} nous renvoie le nombre d'éléments du
vecteur, la fonction \rfunc{mean} nous donne la moyenne des éléments
du vecteur et la fonction \rfunc{var} sa variance.

\subsection{Arguments}

Les arguments de la fonction lui sont indiqués entre parenthèses,
juste après son nom. En général les premiers arguments passés à la
fonction sont des données servant au calcul, et les suivants des
paramètres influant sur ce calcul. Ceux-ci sont en général transmis
sous la forme d'argument nommés.

Reprenons l'exemple des tailles précédent~:

<<>>=
tailles <- c(167, 192, 173, 174, 172, 167, 171, 185, 163, 170)
@ 

Imaginons que le deuxième enquêté n'ait pas voulu nous répondre. Nous
avons alors dans notre vecteur une valeur manquante. Celle-ci est
symbolisée dans \R par le code \texttt{NA}~:

<<>>=
tailles <- c(167, NA, 173, 174, 172, 167, 171, 185, 163, 170)
@

Recalculons notre taille moyenne~:

<<>>=
mean(tailles)
@ 

Et oui, par défaut, \R renvoie \texttt{NA} pour un grand nombre de
calculs (dont la moyenne) lorsque les données comportent une valeur
manquante. On peut cependant modifier ce comportement en fournissant
un paramètre supplémentaire à la fonction \rfunc{mean}, nommé \texttt{na.rm}~:

<<>>=
mean(tailles, na.rm=TRUE)
@ 

Positionner le paramètre \texttt{na.rm} à \texttt{TRUE} (vrai) indique
à la fonction \rfunc{mean} de ne pas tenir compte des valeurs
manquantes dans le calcul.

Lorsqu'on passe un argument à une fonction de cette manière,
c'est-à-dire sous la forme \texttt{nom=valeur}, on parle
d'\textit{argument nommé}.

\begin{important}
  \texttt{NA} signifie \textit{not available}. Cette valeur particulière peut
  être utilisée pour indiquer une valeur manquante pour tout type de liste
  (nombres, textes, valeurs logique, etc.).
\end{important}

\subsection{Quelques fonctions utiles}

Récapitulons la liste des fonctions que nous avons déjà rencontrées~:

\begin{center}
  \begin{tabular}{rl}
    \textbf{Fonction} & \textbf{Description} \\
    \hline
    \rfunc{c} & construit un vecteur à partir d'une série de valeurs \\
    \rfunc{length} & nombre d'éléments d'un vecteur \\
    \rfunc{mean} & moyenne d'un vecteur de type numérique \\
    \rfunc{var} & variance d'un vecteur de type numérique \\
    \rfunc{+}, \rfunc{-}, \rfunc{*}, \rfunc{/} & opérateurs mathématiques de base \\
    \texttt{\^}\index{^@\texttt{\^}} & passage à la puissance \\
  \end{tabular}
\end{center}

On peut rajouter les fonctions de base suivantes~:

\begin{center}
  \begin{tabular}{rl}
    \textbf{Fonction} & \textbf{Description} \\
    \hline
    \rfunc{min} & valeur minimale d'un vecteur numérique \\
    \rfunc{max} & valeur maximale d'un vecteur numérique \\
    \rfunc{sd} & écart-type d'un vecteur numérique \\
    \rfunc{:} & génère une séquence de nombres. \verb|1:4| équivaut à \texttt{c(1,2,3,4)} \\
  \end{tabular}
\end{center}

\begin{rstudio}
  Autre outil bien utile de \textsf{RStudio}, l'auto-complétion. Tapez les 
  premières lettres d'une fonction, par exemple \texttt{me} puis appuyez sur la 
  touche \texttt{<Tabulation>}. \textsf{RStudio} affichera la liste des fonctions
  dont le nom commence par \texttt{me} ainsi qu'un court descriptif de chacune.
  Un appui sur la touche \textit{Entrée} provoquera la saisie du nom complet de 
  la fonction choisie. Vous pouvez également utiliser l'auto-complétion pour
  retrouver un objet que vous avez créé --- par exemple, appuyez sur la touche 
  \texttt{<Tabulation>} après avoir saisi \texttt{mean(t} --- ou bien pour retrouver
  un argument nommé d'une fonction --- par exemple, appuyez sur la touche 
  \texttt{<Tabulation>} après avoir saisi \texttt{mean(taille,}.
\end{rstudio}

\subsection{Aide sur une fonction}

Il est très fréquent de ne plus se rappeler quels sont les paramètres
d'une fonction ou le type de résultat qu'elle retourne. Dans ce cas on
peut très facilement accéder à l'aide décrivant une fonction
particulière en tapant (remplacer \texttt{fonction} par le nom de la
fonction)~:

<<eval=FALSE>>=
help("fonction")
@ 

Ou, de manière équivalente, \texttt{?fonction}\footnote{L'utilisation
  du raccourci \texttt{?fonction} ne fonctionne pas pour certains
  opérateurs comme \texttt{*}. Dans ce cas on pourra utiliser
  \texttt{?'*'} ou bien simplement \texttt{help("*")}.}.

Ces deux commandes affichent une page (en anglais) décrivant la
fonction, ses paramètres, son résultat, le tout accompagné de diverses
notes, références et exemples. Ces pages d'aide contiennent à peu près
tout ce que vous pourrez chercher à savoir, mais elles ne sont pas
toujours d'une lecture aisée.

Un autre cas très courant dans \R est de ne pas se souvenir ou de ne
pas connaître le nom de la fonction effectuant une tâche donnée. Dans
ce cas on se reportera aux différentes manières de trouver de l'aide
décrites dans l'annexe~\ref{sec_aide}, page~\pageref{sec_aide}.

\begin{rstudio}
  Dans \textsf{RStudio}, les pages d'aide en ligne s'ouvriront dans le quadrant
  bas-droite sous l'onglet \textit{Help}. Un clic sur l'icône en forme de maison
  vous affichera la page d'accueil de l'aide.
\end{rstudio}

\section{Exercices}


\begin{exo}{simplec}
  Construire le vecteur suivant~: 
<<echo=FALSE>>=
c(120, 134, 256, 12)
@ 
\end{exo}

\begin{exo}{genseq}
  Générez les vecteurs suivants chacun de deux manières différentes~:
<<echo=FALSE>>=
1:4
c(1:4,8:11)
1:4*2
@ 
\end{exo}


\begin{exo}{sommevect}
  On a demandé à 4 ménages le revenu du chef de ménage, celui de son
  conjoint, et le nombre de personnes du ménage~:
<<eval=FALSE>>=
chef <- c(1200, 1180, 1750, 2100)
conjoint <- c(1450, 1870, 1690, 0)
nb.personnes <- c(4, 2, 3, 2)
@ 
Calculez le revenu total par personne du ménage.
\end{exo}

\begin{exo}{maxvect}
  Dans l'exercice précédent, calculez le revenu minimum et le revenu
  maximum parmi ceux du chef de ménage~:
<<eval=FALSE>>=
chef <- c(1200, 1180, 1750, 2100)
@ 
  Recommencer avec les revenus suivants, parmi lesquels l'un des
  enquêtés n'a pas voulu répondre~:
<<eval=FALSE>>=
chef.na <- c(1200, 1180, 1750, NA)
@ 
\end{exo}






\chapter{Premier travail avec des données}



\section{Regrouper les commandes dans des scripts}


Jusqu'à maintenant nous avons utilisé uniquement la console pour
communiquer avec \R \textit{via} l'invite de commandes. Le principal
problème de ce mode d'interaction est qu'une fois qu'une commande est
tapée, elle est pour ainsi dire <<~perdue~>>, c'est-à-dire qu'on doit
la saisir à nouveau si on veut l'exécuter une seconde
fois. L'utilisation de la console est donc restreinte aux petites
commandes <<~jetables~>>, le plus souvent utilisées comme test.

La plupart du temps, les commandes seront stockées dans un fichier à
part, que l'on pourra facilement ouvrir, éditer et exécuter en tout ou
partie si besoin. On appelle en général ce type de fichier un
\textit{script}.

Pour comprendre comment cela fonctionne, dans le menu
\textit{Fichier}, sélectionnez l'entrée \textit{Nouveau script}\footnotemark. Une
nouvelle fenêtre (vide) apparaît. Nous pouvons désormais y
saisir des commandes. Par exemple, tapez sur la première ligne la
commande suivante~:

\begin{greyverb}
\begin{verbatim}
2+2
\end{verbatim} 
\end{greyverb}

\footnotetext{Les indications données ici concernent l'interface par
  défaut de \R sous \textsf{Windows}. Elles sont très semblables sous
  \textsf{Mac OS X}.}

Ensuite, allez dans le menu \textit{Éditon}, et choisissez
\textit{Exécuter la ligne ou sélection}. Apparement rien ne se passe,
mais si vous jetez un \oe{}il à la fenêtre de la console, les lignes
suivantes ont dû faire leur apparition~:

<<exec,tidy=FALSE>>=
2+2
@ 

Voici donc comment soumettre rapidement à \R les commandes saisies dans
votre fichier. Vous pouvez désormais l'enregistrer, l'ouvrir plus
tard, et en exécuter tout ou partie. À noter que vous avez plusieurs
possibilités pour soumettre des commandes à \R~:

\begin{itemize}
\item vous pouvez exécuter la ligne sur laquelle se trouve votre
  curseur en sélectionnant \textit{Éditon} puis \textit{Exécuter la
    ligne ou sélection}, ou plus simplement en appuyant simultanément
  sur les touches \texttt{<Ctrl>} et \texttt{<R>}\footnote{Sous
    \textsf{Mac OS X}, on utilise les touches \texttt{<Pomme>} et
    \texttt{<Entrée>}.}~;
\item vous pouvez sélectionner plusieurs lignes contenant des
  commandes et les exécuter toutes en une seule fois exactement de la
  même manière~;
\item vous pouvez exécuter d'un coup l'intégralité de votre fichier en
  choisissant \textit{Édition} puis \textit{Exécuter tout}.
\end{itemize}

La plupart du travail sous \R consistera donc à éditer un ou plusieurs
fichiers de commandes et à envoyer régulièrement les commandes saisies
à \R en utilisant les raccourcis clavier \textit{ad hoc}. 

\begin{rstudio}
  Les commandes sont légèrement différentes avec \textsf{RStudio} mais le 
  principe est le même. Pour créer un nouveau script \R, faire \textit{File > 
  New > R Script}. Votre nouveau fichier apparaitra dans le quadrant haut-gauche.
  Pour exécuter une ou plusieurs lignes de code, sélectionnez les lignes en
  question puis cliquez sur l'icône \textit{Run} ou bien appuyez simultanément
  sur les touches \texttt{<Ctrl>} et \texttt{<Entrée>}. 
  
  Pour plus d'astuces (en anglais) : \url{http://www.rstudio.com/ide/docs/using/source}
\end{rstudio}

\section{Ajouter des commentaires}

Un commentaire est une ligne ou une portion de ligne qui sera ignorée
par \R. Ceci signifie qu'on peut y écrire ce qu'on veut, et qu'on va
les utiliser pour ajouter tout un tas de commentaires à notre code
permettant de décrire les différentes étapes du travail, les choses à
se rappeler, les questions en suspens, etc.


Un commentaire sous \R commence par un ou plusieurs symboles
\texttt{\#} (qui s'obtient avec les touches \texttt{<Alt~Gr>} et
\texttt{<3>} sur les claviers de type PC). Tout ce qui suit ce symbole
jusqu'à la fin de la ligne est considéré comme un commentaire. On peut
créer une ligne entière de commentaire, par exemple en la faisant
débuter par \texttt{\#\#}~:

<<commentaire1,prompt=FALSE,eval=FALSE,tidy=FALSE>>=
## Tableau croisé de la CSP par le nombre de livres lus
## Attention au nombre de non réponses !
@ 

On peut aussi créer des commentaires pour une ligne en cours~:


<<commentaire2,prompt=FALSE,eval=FALSE,tidy=FALSE>>=
x <- 2  # On met 2 dans x, parce qu'il le vaut bien
@

\begin{important}
Dans tous les cas, il est très important de documenter ses fichiers \R
au fur et à mesure, faute de quoi on risque de ne plus y comprendre
grand chose si on les reprend ne serait-ce que quelques semaines plus
tard.
\end{important}

\begin{rstudio}
  Avec \textsf{RStudio}, vous pouvez également utiliser les commentaires pour
  créer des sections au sein de votre script et naviguer plus rapidement.
  
  Voir (en anglais) : \url{http://www.rstudio.com/ide/docs/using/code_folding}
\end{rstudio}

\section{Tableaux de données}

Dans cette partie nous allons utiliser un jeu de données inclus dans
l'extension \questionr.\marqr{} Cette extension et son
installation sont décrites dans la partie~\ref{sec_questionr},
page~\pageref{sec_questionr}.

Le jeu de données en question est un extrait de l'enquête
\textit{Histoire de vie} réalisée par l'INSEE en 2003. Il contient
2000 individus et 20 variables. Le descriptif des variables est
indiqué dans l'annexe~\ref{sec_hdv2003}, page~\pageref{sec_hdv2003}.

Pour pouvoir utiliser ces données, il faut d'abord charger l'extension
\questionr (après l'avoir installée, bien entendu)~:

<<>>=
library(questionr)
@ 

Puis indiquer à \R que nous souhaitons accéder au jeu de données à
l'aide de la commande \rfunc{data}~:

<<>>=
data(hdv2003)
@ 

Bien. Et maintenant, elles sont où mes données ? Et bien elles se
trouvent dans un objet nommé \texttt{hdv2003} désormais accessible
directement. Essayons de taper son nom à l'invite de commande~:

<<eval=FALSE>>=
hdv2003
@ 

Le résultat (non reproduit ici) ne ressemble pas forcément à
grand-chose\ldots Il faut se rappeler que par défaut, lorsqu'on lui
fournit seulement un nom d'objet, \R essaye de l'afficher de la
manière la meilleure (ou la moins pire) possible. La réponse à la
commande \texttt{hdv2003} n'est donc rien moins que l'affichage des
données brutes contenues dans cet objet.

Ce qui signifie donc que l'intégralité de notre jeu de données est
inclus dans l'objet nommé \texttt{hdv2003}~! En effet, dans \R, un
objet peut très bien contenir un simple nombre, un vecteur ou bien le
résultat d'une enquête tout entier. Dans ce cas, les objets sont
appelés des \textit{data frames}, ou tableaux de données. Ils peuvent
être manipulés comme tout autre objet. Par exemple~:

<<>>=
d <- hdv2003
@  

va entraîner la copie de l'ensemble de nos données dans un nouvel
objet nommé \texttt{d}, ce qui peut paraître parfaitement inutile mais
a en fait l'avantage de fournir un objet avec un nom beaucoup plus
court, ce qui diminuera la quantité de texte à saisir par la suite.

\paragraph{Résumons} Comme nous avons désormais décidé de saisir nos
commandes dans un script et non plus directement dans la console, les
premières lignes de notre fichier de travail sur les données de
l'enquête \textit{Histoire de vie} pourraient donc ressembler à ceci~:


<<resumtab,prompt=FALSE,eval=FALSE,tidy=FALSE>>=
## Chargement des extensions nécessaires
library(questionr)

## Jeu de données hdv2003
data(hdv2003)
d <- hdv2003
@ 


\section{Inspecter les données}

\subsection{Structure du tableau}

Avant de travailler sur les données, nous allons essayer de voir à
quoi elles ressemblent. Dans notre cas il s'agit de se familiariser
avec la stucture du fichier. Lors de l'import de données depuis un
autre logiciel, il s'agira souvent de vérifier que l'importation s'est
bien déroulée.

Les fonctions \rfunc{nrow}, \rfunc{ncol} et \rfunc{dim} donnent
respectivement le nombre de lignes, le nombre de colonnes et les
dimensions de notre tableau. Nous pouvons donc d'ores et déjà vérifier
que nous avons bien 2000 lignes et 20 colonnes~:

<<>>=
nrow(d)
ncol(d)
dim(d)
@ 

La fonction \rfunc{names} donne les noms des colonnes de notre
tableau, c'est-à-dire les noms des variables~:

<<>>=
names(d)
@ 

La fonction \rfunc{str} est plus complète. Elle liste les différentes
variables, indique leur type et donne le cas échéant des informations
supplémentaires ainsi qu'un échantillon des premières valeurs prises
par cette variable~:

<<>>=
str(d)
@ 

La première ligne nous informe qu'il s'agit bien d'un tableau de
données avec 2000 observations et 20 variables. Vient ensuite la liste
des variables. La première se nomme \texttt{id} et est de type
\textit{nombre entier} (\texttt{int}). La seconde se nomme
\texttt{age} et est de type \textit{numérique}. La troisième se nomme
\texttt{sexe}, il s'agit d'un \textit{facteur} (\textit{factor}).

Un \textit{facteur} et une variable pouvant prendre un nombre limité
de modalités (\textit{levels}). Ici notre variable a deux modalités
possibles~: \texttt{Homme} et \texttt{Femme}. Ce type de variable est
décrit plus en détail section~\ref{sec_factor} \vpageref{sec_factor}.

\begin{astuce}
  La fonction \rfunc{str} peut s'appliquer à n'importe quel type d'objet. C'est 
  un excellent moyen de connaître la structure d'un objet.
\end{astuce}

\subsection{Inspection visuelle}

La particularité de \R par rapport à d'autres logiciels comme
\textsf{Modalisa} ou \textsf{SPSS} est de ne pas proposer, par défaut,
de vue des données sous forme de tableau. Ceci peut parfois être un
peu déstabilisant dans les premiers temps d'utilisation, même si on
perd vite l'habitude et qu'on finit par se rendre compte que
<<~voir~>> les données n'est pas forcément un gage de productivité ou
de rigueur dans le traitement.

Néanmoins, \R propose une visualisation assez rudimentaire des données
sous la forme d'une fenêtre de type tableur, \textit{via} la fonction \rfunc{edit}~:

<<eval=FALSE>>=
edit(d)
@ 

La fenêtre qui s'affiche permet de naviguer dans le tableau, et même
d'éditer le contenu des cases et donc de modifier les données. Lorsque
vous fermez la fenêtre, le contenu du tableau s'affiche dans la
console~: il s'agit en fait du tableau comportant les éventuelles
modifications effectuées, \texttt{d} restant inchangé. Si vous
souhaitez appliquer ces modifications, vous pouvez le faire en créant
un nouveau tableau~:

<<eval=FALSE>>=
d.modif <- edit(d)
@  

ou en remplaçant directement le contenu de \texttt{d}\footnote{Dans ce
  cas on peut utiliser la fonction \rfunc{fix} sous la forme
  \texttt{fix(d)}, qui est équivalente à \texttt{d <- edit(d)}.}~:

<<eval=FALSE>>=
d <- edit(d)
@  

\begin{important}
  La fonction \rfunc{edit} peut être utile pour un avoir un aperçu
  visuel des données, par contre il est \textbf{très fortement}
  déconseillé de l'utiliser pour modifier les données. Si on souhaite
  effectuer des modifications, on remonte en général aux données
  originales (retouches ponctuelles dans un tableur par exemple) ou on
  les effectue à l'aide de commandes (qui seront du coup
  reproductibles).
\end{important}

\begin{rstudio}
  Sous \textsf{RStudio}, la liste des objets en mémoire est affichée dans le
  quadrant haut-droite sous l'onglet \textit{Workspace}. Un clic sur un tableau
  de données permet d'afficher son contenu sous un onglet dédié dans le quadrant
  haut-gauche. Cette manière de procéder est plus simple que le recours à la 
  fonction \rfunc{edit}.
\end{rstudio}

\subsection{Accéder aux variables}

\texttt{d} représente donc l'ensemble de notre tableau de
données. Nous avons vu que si l'on saisit simplement \texttt{d} à
l'invite de commandes, on obtient un affichage du tableau en
question. Mais comment accéder aux variables, c'est à dire aux
colonnes de notre tableau~?

La réponse est simple~: on utilise le nom de l'objet, suivi de
l'opérateur \rfunc{\$}, suivi du nom de la variable, comme ceci~:

<<>>=
d$sexe
@ 

On constate alors que \R a bien accédé au contenu de notre variable
\texttt{sexe} du tableau \texttt{d} et a affiché son contenu,
c'est-à-dire l'ensemble des valeurs prises par la variable.

Les fonctions \rfunc{head} et \rfunc{tail} permettent d'afficher
seulement les premières (respectivement les dernières) valeurs prises
par la variable. On peut leur passer en argument le nombre d'éléments
à afficher~:

<<>>=
head(d$sport)
tail(d$age,10)
@ 

À noter que ces fonctions marchent aussi pour afficher les lignes du
tableau \texttt{d}~:

<<>>=
head(d,2)
@ 


\section{Analyser une variable}

\subsection{Variable quantitative}

\subsubsection{Principaux indicateurs}

Comme la fonction \rfunc{str} nous l'a indiqué, notre tableau
\texttt{d} contient plusieurs valeurs numériques, dont la variable
\texttt{heures.tv} qui représente le nombre moyen passé par les
enquêtés à regarder la télévision quotidiennement. On peut essayer de
déterminer quelques caractéristiques de cette variable, en utilisant
des fonctions déjà vues précédemment~:

<<>>=
mean(d$heures.tv)
mean(d$heures.tv, na.rm=TRUE)
sd(d$heures.tv, na.rm=TRUE)
min(d$heures.tv, na.rm=TRUE)
max(d$heures.tv, na.rm=TRUE)
range(d$heures.tv, na.rm=TRUE)
@ 

On peut lui ajouter la fonction \rfunc{median}, qui donne la valeur
médiane, et le très utile \rfunc{summary} qui donne toutes ces
informations ou presque en une seule fois, avec en plus les valeurs des
premier et troisième quartiles et le nombre de valeurs manquantes (\texttt{NA})~:

<<>>=
median(d$heures.tv,na.rm=TRUE)
summary(d$heures.tv)
@ 

\begin{astuce}
  La fonction \rfunc{summary} peut-être utilisée sur tout type d'objet, y 
  compris un tableau de données. Essayez donc \texttt{summary(d)}.
\end{astuce}

\subsubsection{Histogramme}

Tout cela est bien pratique, mais pour pouvoir observer la
distribution des valeurs d'une variable quantitative, il n'y a quand
même rien de mieux qu'un bon graphique.

On peut commencer par un histogramme de la répartition des
valeurs. Celui-ci peut être généré très facilement avec la fonction
\rfunc{hist}, comme indiqué figure~\ref{fig_hist1}
  \vpageref{fig_hist1}.

\begin{figure}
<<hist1>>=
hist(d$heures.tv, main="Nombre d'heures passées devant la télé par jour", xlab="Heures", ylab="Effectif")
@ 
\caption{Exemple d'histogramme}
\label{fig_hist1}
\end{figure}


Ici, les options \texttt{main}, \texttt{xlab} et \texttt{ylab}
permettent de personnaliser le titre du graphique, ainsi que les
étiquettes des axes. De nombreuses autres options existent pour
personnaliser l'histogramme, parmi celles-ci on notera~:

\begin{description}
\item[probability] si elle vaut \texttt{TRUE}, l'histogramme indique
  la proportion des classes de valeurs au lieu des effectifs.
\item[breaks] permet de contrôler les classes de valeurs. On peut lui
  passer un chiffre, qui indiquera alors le nombre de classes, un
  vecteur, qui indique alors les limites des différentes classes, ou
  encore une chaîne de caractère ou une fonction indiquant comment les
  classes doivent être calculées.
\item[col] la couleur de l'histogramme\footnotemark.
\end{description}

\footnotetext{Il existe un grand nombre de couleurs prédéfinies dans
  \R. On peut récupérer leur liste en utilisant la fonction
  \rfunc{colors} en tapant simplement \texttt{colors()} dans la
  console, ou en consultant le document suivant~:
  \url{http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf}}

Deux exemples sont donnés figure~\ref{fig_hist2} \vpageref{fig_hist2}
et figure~\ref{fig_hist3} \vpageref{fig_hist3}.

\begin{figure}
<<>>=
hist(d$heures.tv, main="Heures de télé en 7 classes", breaks=7,
     xlab="Heures", ylab="Proportion", probability=TRUE, col="orange")
@ 
\caption{Un autre exemple d'histogramme}
\label{fig_hist2}
\end{figure}

\begin{figure}
<<>>=
hist(d$heures.tv, main="Heures de télé avec classes spécifiées", breaks=c(0,1,4,9,12),
     xlab="Heures", ylab="Proportion", col="red")
@ 
\caption{Encore un autre exemple d'histogramme}
\label{fig_hist3}
\end{figure}

Voir la page d'aide de la fonction \rfunc{hist} pour plus de détails
sur les différentes options.

\subsubsection{Boîtes à moustaches}

Les boîtes à moustaches, ou \rfunc{boxplot} en anglais, sont une
autre représentation graphique de la répartition des valeurs d'une
variable quantitative. Elles sont particulièrement utiles pour
comparer les distributions de plusieurs variables ou d'une même
variable entre différents groupes, mais peuvent aussi être utilisées
pour représenter la dispersion d'une unique variable. La fonction qui
produit ces graphiques est la fonction \rfunc{boxplot}. On trouvera un
exemple figure~\ref{fig_boxplot1} \vpageref{fig_boxplot1}.

\begin{figure}
<<>>=
boxplot(d$heures.tv, main="Nombre d'heures passées devant la télé par jour", ylab="Heures")
@ 
\caption{Exemple de boîte à moustaches}
\label{fig_boxplot1}
\end{figure}


Comment interpréter ce graphique~? On le comprendra mieux à partir de
la figure~\ref{fig_boxplot2} \vpageref{fig_boxplot2}\footnote{Le code
  ayant servi à générer cette figure est une copie quasi conforme de
  celui présenté dans l'excellent document de Jean Lobry sur les
  graphiques de base avec \R, téléchargeable sur le site du Pôle
  bioinformatique lyonnais~:
  \url{http://pbil.univ-lyon1.fr/R/pdf/lang04.pdf}.}.

\begin{figure}
<<>>=
boxplot(d$heures.tv, col = grey(0.8), main="Nombre d'heures passées devant la télé par jour", ylab="Heures")
abline(h = median(d$heures.tv, na.rm = TRUE), col = "navy", lty=2)
text(1.35, median(d$heures.tv, na.rm = TRUE)+0.15, "Médiane", col = "navy")
Q1 <- quantile(d$heures.tv, probs = 0.25, na.rm = TRUE)
abline(h = Q1, col = "darkred")
text(1.35, Q1+0.15, "Q1 : premier quartile", col = "darkred", lty=2)
Q3 <- quantile(d$heures.tv, probs = 0.75, na.rm = TRUE)
abline(h = Q3, col = "darkred")
text(1.35, Q3+0.15, "Q3 : troisième quartile", col = "darkred", lty=2)
arrows(x0 = 0.7, y0 = quantile(d$heures.tv, probs = 0.75,
    na.rm = TRUE), x1 = 0.7, y1 = quantile(d$heures.tv,
    probs = 0.25, na.rm = TRUE), length = 0.1, code = 3)
text(0.7, Q1 + (Q3-Q1)/2+0.15, "h", pos = 2)
mtext("L'écart inter-quartile h contient 50 % des individus", side = 1)
abline(h = Q1 - 1.5 * (Q3 - Q1), col = "darkgreen")
text(1.35, Q1 - 1.5 * (Q3 - Q1) + 0.15, "Q1 -1.5 h", col = "darkgreen", lty=2)
abline(h = Q3 + 1.5 * (Q3 - Q1), col = "darkgreen")
text(1.35, Q3 + 1.5 * (Q3 - Q1) + 0.15, "Q3 +1.5 h", col = "darkgreen", lty=2)
@ 
\caption{Interprétation d'une boîte à moustaches}
\label{fig_boxplot2}
\end{figure}


Le carré au centre du graphique est délimité par les premiers et
troisième quartiles, avec la médiane représentée par une ligne plus
sombre au milieu. Les <<~fourchettes~>> s'étendant de part et d'autres
vont soit jusqu'à la valeur minimale ou maximale, soit jusqu'à une
valeur approximativement égale au quartile le plus proche plus 1,5
fois l'écart inter-quartile. Les points se situant en-dehors de cette
fourchette sont représentés par des petits ronds et sont généralement
considérés comme des valeurs extrêmes, potentiellement aberrantes.

On peut ajouter la représentation des valeurs sur le graphique pour en
faciliter la lecture avec des petits traits dessinés sur l'axe
vertical (fonction \rfunc{rug}), comme sur la
figure~\ref{fig_boxplotrug} \vpageref{fig_boxplotrug}.

\begin{figure}
<<>>=
boxplot(d$heures.tv, main="Nombre d'heures passées devant la télé par
jour", ylab="Heures")
rug(d$heures.tv, side=2)
@ 
\caption{Boîte à moustaches avec représentation des valeurs}
\label{fig_boxplotrug}
\end{figure}

\subsubsection{Intervalle de confiance}

L'intervalle de confiance d'une moyenne peut être calculé avec la fonction 
\rfunc{t.test} (fonction qui permet également de réaliser un test t de Student
comme nous le verrons section \ref{deux_var_quant} \vpageref{deux_var_quant})~:

<<>>=
t.test(d$heures.tv)
@ 

Le niveau de confiance peut être précisé via l'argument \texttt{conf.level}~:

<<>>=
t.test(d$heures.tv, conf.level=0.9)
@ 

Le nombre d'heures moyennes à regarder la télévision parmi les enquêtés s'avère
être de 2,2 heures, avec un intervalle de confiance à 95~\% de [2,17~-~2,33] et un
intervalle de confiance à 90~\% de [2,18~-~2,31].

\subsection{Variable qualitative}

\subsubsection{Tris à plat}

La fonction la plus utilisée pour le traitement et l'analyse des
variables qualitatives (variable prenant ses valeurs dans un ensemble
de modalités) est sans aucun doute la fonction \rfunc{table}, qui
donne les effectifs de chaque modalité de la variable.

<<>>=
table(d$sexe)
@ 

La tableau précédent nous indique que parmi nos enquêtés on trouve 899
hommes et 1101 femmes.

Quand le nombre de modalités est élevé, on peut ordonner le tri à plat
selon les effectifs à l'aide de la fonction \rfunc{sort}.

<<>>=
table(d$occup)
sort(table(d$occup))
sort(table(d$occup), decreasing=TRUE)
@ 



À noter que la fonction \rfunc{table} exclut par défaut les
non-réponses du tableau résultat. L'argument \texttt{useNA} de cette fonction
permet de modifier ce comportement~:

\begin{itemize}
\item avec \texttt{useNA="no"} (valeur par défaut), les valeurs manquantes ne sont
  jamais incluses dans le tri à plat~;
\item avec \texttt{useNA="ifany"}, une colonne \texttt{NA} est ajoutée si des
  valeurs manquantes sont présentes dans les données~;
\item avec \texttt{useNA="always"}, une colonne \texttt{NA} est toujours
  ajoutée, même s'il n'y a pas de valeurs manquantes dans les données.
\end{itemize}

On peut donc utiliser~:

<<>>=
table(d$trav.satisf, useNA="ifany")
@ 

L'utilisation de \rfunc{summary} permet également l'affichage du tri à plat et
du nombre de non-réponses~:

<<>>=
summary(d$trav.satisf)
@ 

Pour obtenir un tableau avec la répartition en pourcentages, on peut
utiliser la fonction \rfunc{freq} de \marqr l'extension \textsf{questionr}\footnotemark.

\footnotetext{
 En l'absence de l'extension \texttt{questionr}, on pourra se rabattre sur la 
 fonction \rfunc{prop.table} avec la commande suivante : 
 \texttt{prop.table(table(d\$qualif))}. 
}

<<>>=
freq(d$qualif)
@ 

La colonne \texttt{n} donne les effectifs bruts, et la colonne
\texttt{\%} la répartition en pourcentages. La fonction accepte
plusieurs paramètres permettant d'afficher les totaux, les pourcentages
cumulés, de trierselon les effectifs ou de contrôler l'affichage. Par
exemple~:

<<>>=
freq(d$qualif, cum=TRUE, total=TRUE, sort="inc", digits=2, exclude=NA)
@ 

La colonne \texttt{\%cum} indique ici le pourcentage cumulé, ce qui
est ici une très mauvaise idée puisque pour ce type de variable cela
n'a aucun sens. Les lignes du tableau résultat ont été triés par
effectifs croissants, les totaux ont été ajoutés, les non-réponses
exclues, et les pourcentages arrondis à deux décimales.

Pour plus d'informations sur la commande \rfunc{freq}, consultez sa
page d'aide en ligne avec \texttt{?freq} ou \texttt{help("freq")}.

\subsubsection{Représentation graphique}

Pour représenter la répartition des effectifs parmi les modalités
d'une variable qualitative, on a souvent tendance à utiliser des
diagrammes en secteurs (camemberts). Ceci est possible sous \R avec la
fonction \rfunc{pie}, mais la page d'aide de ladite fonction nous le
déconseille assez vivement~: les diagrammes en secteur sont en effet
une mauvaise manière de présenter ce type d'information, car l'\oe{}il
humain préfère comparer des longueurs plutôt que des surfaces\footnotemark.

\footnotetext{On trouvera des exemples illustrant cette idée dans le
  document de Jean Lobry cité précédemment.}

On privilégiera donc d'autres formes de représentations, à savoir les
diagrammes en bâtons et les diagrammes de Cleveland.

Les diagrammes en bâtons sont utilisés automatiquement par \R
lorsqu'on applique la fonction générique \rfunc{plot} à un tri à plat
obtenu avec \rfunc{table}. On privilégiera cependant ce type de
représentations pour les variables de type numérique comportant un
nombre fini de valeurs. Le nombre de frères, s\oe{}urs, demi-frères et
demi-s\oe{}urs est un bon exemple, indiqué figure~\ref{fig_barplot}
\vpageref{fig_barplot}.

\begin{figure}
<<>>=
plot(table(d$freres.soeurs), main="Nombre de frères, soeurs, demi-frères et demi-soeurs", ylab="Effectif")
@ 
\caption{Exemple de diagramme en bâtons}
\label{fig_barplot}
\end{figure}


Pour les autres types de variables qualitatives, on privilégiera les
diagrammes de Cleveland, obtenus avec la fonction \rfunc{dotchart}. On doit
appliquer cette fonction au tri à plat de la variable, obtenu avec la fonction
\rfunc{table}\footnote{Pour des raisons liées au fonctionnement interne de la
  fonction \rfunc{dotchart}, on doit l'appliquer à la transposition du tri à
  plat obtenu, d'où l'appel à la fonction \rfunc{t}.}. Le résultat se trouve
figure~\ref{fig_dotchart} \vpageref{fig_dotchart}.

\begin{figure}
<<>>=
dotchart(t(table(d$clso)), main="Sentiment d'appartenance à une classe sociale", pch=19)
@ 
\caption{Exemple de diagramme de Cleveland}
\label{fig_dotchart}
\end{figure}


Quand la variable comprend un grand nombre de modalités, il est
préférable d'ordonner le tri à plat obtenu à l'aide de la fonction
\rfunc{sort} (voir figure~\ref{fig_dotchartsort} \vpageref{fig_dotchartsort}). 

\begin{figure}
<<>>=
dotchart(sort(table(d$qualif)), main="Niveau de qualification")
@ 
\caption{Exemple de diagramme de Cleveland ordonné}
\label{fig_dotchartsort}
\end{figure}

\begin{astuce}
  L'agument \texttt{pch}, qui est utilisé par la plupart des graphiques de type 
  points, permet de spécifier le symbole à utiliser. Il peut prendre soit un
  nombre entier compris entre 0 et 25, soit un charactère textuel 
  (voir figure~\ref{fig_pch} \vpageref{fig_pch}).
\end{astuce}

\begin{figure}
<<figure_pch, echo=FALSE>>=
source("pchShow.R")
pchShow(c("*","+","a","x"),main="", symbolsize=2.5, linewidth=2,fillcolor="palegreen3",symbolcolor="palevioletred3")
@ 
\caption{Différentes valeurs possibles pour l'argument \texttt{pch}}
\label{fig_pch}
\end{figure}

\subsubsection{Intervalle de confiance}

La fonction \rfunc{prop.test} permet de calculer l'intervalle de confiance d'une
proportion. Une première possibilité consiste à lui transmettre une table à une
dimension et deux entrées. Par exemple, si l'on s'intéresse à la proportion de
personnes ayant pratiqué une activité physique au cours des douze derniers mois~:


<<>>=
freq(d$sport)
prop.test(table(d$sport))
@ 

On remarquera que la fonction a calculé l'intervalle de confiance correspondant
à la première entrée du tableau, autrement dit celui de la proportion d'enquêtés
n'ayant pas pratiqué une activité sportive.

Or, nous sommes intéressé par la proportion complémentaire, à savoir celle 
d'enquêtés ayant pratiqué une activité sportive. On peut dès lors modifier
l'ordre de la table en indiquant notre modalité d'intérêt avec la fonction
\rfunc{relevel} ou bien indiquer à \rfunc{prop.test} d'abord le nombre de succès
puis l'effectif total~:

<<>>=
prop.test(table(relevel(d$sport,"Oui")))
prop.test(sum(d$sport=="Oui"),length(d$sport))
@

Enfin, le niveau de confiance peut être modifié via l'argument 
\texttt{conf.level}~:

<<>>=
prop.test(table(relevel(d$sport,"Oui")), conf.level=0.9)
@


\section{Exercices}

\begin{exo}{simplescript}
  Créer un script qui effectue les actions suvantes et exécutez-le~:
\begin{itemize}
\item charger l'extension \questionr
\item charger le jeu de données \texttt{hdv2003}
\item placer le jeu de données dans un objet nommé \texttt{df}
\item afficher la liste des variables de \texttt{df} et leur type
\end{itemize}
\end{exo}

\begin{exo}{editdf}
  Des erreurs se sont produites lors de la saisie des données de
  l'enquête. En fait le premier individu du jeu de données n'a pas 42
  ans mais seulement 24, et le second individu n'est pas un homme mais
  une femme. Corrigez les erreurs et stockez les données corrigées
  dans un objet nommé \texttt{df.ok}.
  
  Affichez ensuite les 4 premières lignes de \texttt{df.ok} pour
  vérifier que les modifications ont bien été prises en compte.
  
\end{exo}

\begin{exo}{varquanti}
  Nous souhaitons étudier la répartition des âges des enquêtés
  (variable \texttt{age}). Pour
  cela, affichez les principaux indicateurs de cette
  variable. Représentez ensuite sa distribution par un histogramme en
  10 classes, puis sous forme de boîte à moustache, et enfin sous la
  forme d'un diagramme en bâtons représentant les effectifs de chaque
  âge. Quel est l'intervalle de confiance à 95~\% de l'âge moyen des enquêtés~? 
\end{exo}

\begin{exo}{varquali}
  On s'intéresse maintenant à l'importance accordée par les enquêtés à
  leur travail (variable \texttt{trav.imp}). Faites un tri à plat des
  effectifs des modalités de cette variable avec la commande
  \rfunc{table}. Y'a-t-il des valeurs manquantes~?
 
  Faites un tri à plat affichant à la fois les effectifs et les
  pourcentages de chaque modalité.
  
  Représentez graphiquement les effectifs des modalités à l'aide d'un
  diagramme de Cleveland.
  
\end{exo}


\chapter{Import/export de données}

L'import et l'export de données depuis ou vers d'autres applications
est couvert en détail dans l'un des manuels officiels (en anglais)
nommé \textit{R Data Import/Export} et accessible, comme les autres
manuels, à l'adresse suivante~:

\url{http://cran.r-project.org/manuals.html}

Cette partie est très largement tirée de ce document, et on pourra s'y
reporter pour plus de détails.

\begin{important}
  Importer des données est souvent l'une des première opérations que
  l'on effectue lorsque l'on débute sous \R, et ce n'est pas la moins
  compliquée. En cas de problème il ne faut donc pas hésiter à
  demander de l'aide par les différents moyens disponibles (voir
  partie~\ref{sec_aide} \vpageref{sec_aide}) avant de se
  décourager.
\end{important}

\begin{remarque}
  Un des points délicats pour l'importation de données dans \R
  concerne le nom des variables. Pour être utilisables dans \R ceux-ci
  doivent être à la fois courts et explicites, ce qui n'est pas le cas
  dans d'autres applications comme \textsf{Modalisa} par exemple. La
  plupart des fonctions d'importation s'occupent de convertir les noms
  de manières à ce qu'ils soient compatibles avec les règles de \R
  (remplacement des espaces par des points par exemple), mais un
  renommage est souvent à prévoir, soit au sein de l'application
  d'origine, soit une fois les données importées dans \R.
\end{remarque}


\section{Accès aux fichiers et répertoire de travail}

Dans ce qui suit, puisqu'il s'agit d'importer des données externes,
nous allons avoir besoin d'accéder à des fichiers situés sur le disque
dur de notre ordinateur.

Par exemple, la fonction \rfunc{read.table}, très utilisée pour
l'import de fichiers texte, prend comme premier argument le nom du
fichier à importer, ici \texttt{fichier.txt}~:

<<eval=FALSE>>=
donnees <- read.table("fichier.txt")
@ 

Cependant, ceci ne fonctionnera que si le fichier se trouve dans le
\textit{répertoire de travail} de \R. De quoi s'agit-il~? Tout simplement du
répertoire dans lequel \R est actuellement en train de s'exécuter. Pour savoir
quel est le répertoire de travail actuel, on peut utiliser la fonction
\rfunc{getwd}\footnote{Le résultat indiqué ici correspond à un système
  \textsf{Linux}, sous \textsf{Windows} vous devriez avoir quelque chose de la
  forme \verb|C:/Documents and Settings/| \ldots}~:

<<>>=
getwd()
@ 

Si on veut modifier le répertoire de travail, on utilise
\rfunc{setwd} en lui indiquant le chemin complet. Par exemple sous
\textsf{Linux}~:

<<eval=FALSE>>=
setwd("/home/julien/projets/R")
@ 

Sous \textsf{Windows} le chemin du répertoire est souvent un peu plus
compliqué. Si vous utilisez l'interface graphique par défaut, vous pouvez
utiliser la fonction \textit{Changer le répertoire courant} du menu 
\textit{Fichier}. Celle-ci vous permet de sélectionner le répertoire de 
travail de la session en cours en le sélectionnant via une boîte de dialogue.

Si vous utilisez \textsf{RStudio}, Vous pouvez utiliser une des commandes \textit{set 
working directory} du menu \textit{session} ou, mieux, utiliser les 
fonctionnalités de gestion de projet qui vous permettent de mémoriser, projet 
par projet, le répertoire de travail, la liste des fichiers ouverts ainsi que
différents paramétrages spécifiques.

Une fois le répertoire de travail fixé, on pourra accéder aux fichiers
qui s'y trouvent directement, en spécifiant seulement leur nom. On
peut aussi créer des sous-répertoires dans le répertoire de travail~;
une potentielle bonne pratique peut être de regrouper tous les
fichiers de données dans un sous-répertoire nommé \texttt{donnees}. On
pourra alors accéder aux fichiers qui s'y trouvent de la manière suivante~:

<<eval=FALSE>>=
donnees <- read.table("donnees/fichier.txt")
@ 

Dans ce qui suit on supposera que les fichiers à importer se trouvent
directement dans le répertoire de travail, et on n'indiquera donc que
le nom du fichier, sans indication de chemin ou de répertoire
supplémentaire.

\begin{rstudio}
  Si vous utilisez l'environnement de développement \textsf{RStudio}, vous 
  pouvez vous débarasser du problème des répertoires de travail en utilisant sa
  fonctionnalité de gestion de \textit{projets}. Les projets sont accessibles
  en haut à droite de l'écran. Un projet permet de centraliser tout ses 
  fichiers dans un même répertoire. De plus, il est très facile de basculer 
  très rapidement d'un projet à un autre, en retrouvrant sa session de travail
  dans l'était où elle était.
\end{rstudio}

\section{Import de données depuis un tableur}

Il est assez courant de vouloir importer des données saisies ou
traitées avec un tableur du type \textsf{OpenOffice/LibreOffice} ou
\textsf{Excel}. En général les données prennent alors la forme d'un
tableau avec les variables en colonne et les individus en ligne.

\begin{center}
  \includegraphics[width=0.8\textwidth]{img/donnees_tableur.png}
\end{center}

\subsection{Depuis \textsf{Excel}}

La démarche pour importer ces données dans \R est d'abord de les
enregistrer dans un format de type texte. Sous \textsf{Excel}, on peut
ainsi sélectionner \textit{Fichier}, \textit{Enregistrer sous}, puis
dans la zone \textit{Type de fichier} choisir soit \textit{Texte
  (séparateur tabulation)}, soit \textit{CSV (séparateur: point-virgule)}.

\begin{center}
  \includegraphics[width=0.8\textwidth]{img/selecteur_fichier_excel.png}
\end{center}

Dans le premier cas, on peut importer le fichier en utilisant la
fonction \rfunc{read.delim2}, de la manière suivante~:

<<eval=FALSE>>=
donnees <- read.delim2("fichier.txt")
@ 

Dans le second cas, on utilise \rfunc{read.csv2}, de la même manière~:

<<eval=FALSE>>=
donnees <- read.csv2("fichier.csv")
@ 

\subsection{Depuis \textsf{OpenOffice} ou \textsf{LibreOffice}}

Depuis \textsf{OpenOffice} on procédera de la même manière, en
sélectionnant le type de fichier \textit{Texte CSV}.

\begin{center}
  \includegraphics[width=0.8\textwidth]{img/selecteur_fichier_ooo.png}
\end{center}

On importe ensuite les données dans \R à l'aide de la fonction
\rfunc{read.csv}~:

<<eval=FALSE>>=
read.csv("fichier.csv", dec=",")
@ 


\subsection{Autres sources / en cas de problèmes}

Les fonctions \rfunc{read.csv} et compagnie sont en fait des dérivées
de la fonction plus générique \rfunc{read.table}. Celle-ci contient
de nombreuses options permettant d'adapter l'import au format du
fichier texte. On pourra se reporter à la page d'aide de
\rfunc{read.table} si on recontre des problèmes ou si on souhaite
importer des fichiers d'autres sources.

Parmi les options disponibles, on citera notamment~:

\begin{description}
\item[\texttt{header}] indique si la première ligne du fichier
  contient les noms des variables (valeur \texttt{TRUE}) ou non
  (valeur \texttt{FALSE}).
\item[\texttt{sep}] indique le caractère séparant les champs. En
  général soit une virgule, soit un point-virgule, soit une
  tabulation. Pour cette dernière l'option est
  \texttt{sep="\textbackslash{}t"}.
\item[\texttt{quote}] indique le caractère utilisé pour délimiter les
  champs. En général on utilise soit des guillemets doubles
  (\texttt{quote="\textbackslash{}""}) soit rien du tout
  (\texttt{quote=""}).
\item[\texttt{dec}] indique quel est le caractère utilisé pour séparer
  les nombres et leurs décimales. Il s'agit le plus souvent de la
  virgule lorsque les données sont en français
  (\texttt{dec=","}), et le point pour les données anglophones
  (\texttt{dec="."}).
\end{description}


D'autres options sont disponibles, pour gérer le format d'encodage du
fichier source ou de nombreux autres paramètres d'importation. On se
réfèrera alors à la page d'aide de \rfunc{read.table} et à la section
\textit{Spreadsheet-like data} de \textit{R Data Import/Export}~:

\url{http://cran.r-project.org/doc/manuals/R-data.html#Spreadsheet_002dlike-data}


\section{Import depuis d'autres logiciels}

La plupart des fonctions permettant l'import de fichiers de données
issus d'autres logiciels font partie d'une extension nommée
\textsf{foreign}, présente à l'installation de \R mais qu'il est
nécessaire de charger en mémoire avant utilisation avec l'instruction~:

<<eval=FALSE>>=
library(foreign)
@ 


\subsection{\textsf{SAS}}

Les fichiers au format \textsf{SAS} se présentent en général sous deux
format~: format \textsf{SAS} export (extension \texttt{.xport} ou
\texttt{.xpt}) ou format \textsf{SAS} natif (extension \texttt{.sas7bdat}).

\R peut lire directement les fichiers au format export \textit{via} la fonction
\rfunc{read.xport} de l'extension \textsf{foreign}.

Celle-ci s'utilise très simplement, en lui passant le nom du fichier
en argument~:

<<eval=FALSE>>=
donnees <- read.xport("fichier.xpt")
@ 

En ce qui concerne les fichiers au format \textsf{SAS} natif, il
existe des fonctions permettant de les importer, mais elles
nécessitent d'avoir une installation de \textsf{SAS} fonctionnelle sur
sa machine (il s'agit des fonctions \rfunc{read.ssd} de l'extension
\textsf{foreign}, et \rfunc{sas.get} de l'extension \textsf{Hmisc}).

Si on ne dispose que des fichiers au format \textsf{SAS} natif, le
plus simple est d'utiliser l'application \textsf{SAS System Viewer},
qui permet de lire des fichiers \textsf{SAS} natif, de les visualiser
et de les enregistrer dans un format texte. Cette application est
téléchargeable gratuitement, mais ne fonctionne que sous
\textsf{Windows}\footnote{Ou sous \textsf{Linux} et \textsf{Mac OS X}
  avec \textsf{wine}.}~:

\url{http://www.sas.com/apps/demosdownloads/setupcat.jsp?cat=SAS+System+Viewer}

Une fois le fichier de données au format \textsf{SAS} natif ouvert on
peut l'enregistrer au format texte tabulé. L'import dans \R se fait
alors avec la commande suivante :

<<eval=FALSE>>=
donnees <- read.delim("fichier.txt", na.strings=".")
@


\subsection{\textsf{SPSS}}

Les fichiers générés par \textsf{SPSS} sont accessibles depuis \R avec
la fonction \rfunc{read.spss} de l'extension
\texttt{foreign}. Celle-ci peut lire aussi bien les fichiers
sauvegardés avec la fonction \textit{Enregistrer} que ceux générés par
la fonction \textit{Exporter}.

La syntaxe est également très simple~:

<<eval=FALSE>>=
donnees <- read.spss("fichier.sav", to.data.frame = TRUE)
@ 

Plusieurs options permettant de contrôler l'importation des données
sont disponibles. On se reportera à la page d'aide de la fonction pour
plus d'informations. Il est vivement recommandé d'utiliser systématiquement
l'option \texttt{to.data.frame=TRUE}.

\subsection{\textsf{Stata}}

Les fichiers générés par \textsf{Stata} sont accessibles depuis \R avec
la fonction \rfunc{read.dta} de l'extension
\texttt{foreign}.

La syntaxe est également très simple~:

<<eval=FALSE>>=
donnees <- read.data("fichier.dta", to.data.frame = TRUE)
@ 

\begin{astuce}
 L'importation des dates est parfois mal gérées. Dans ces cas là, l'opération
 suivante peut fonctionner. Sans garantie néanmoins, il est toujours vivement
 conseillé de vérifier le résultat obtenu !
 
 <<eval=FALSE>>=
  donnees$date <- as.Date(donnees$Date/(1000*3600*24),origin='1960-01-01')
 @ 
\end{astuce}

\subsection{Fichiers \texttt{dbf}}

L'Insee diffuse ses fichiers détails depuis son site Web au format
\textsf{dBase} (extension \texttt{.dbf}). Ceux-ci sont directement lisibles
dans \R avec la fonction \rfunc{read.dbf} de l'extension \texttt{foreign}.

<<eval=FALSE>>=
donnees <- read.dbf("fichier.dbf")
@ 

La principale limitation des fichiers \texttt{dbf} est de ne pas gérer
plus de 256 colonnes. Les tables des enquêtes de l'Insee sont donc
parfois découpées en plusieurs fichiers \texttt{dbf} qu'il convient de
fusionner avec la fonction \rfunc{merge}. L'utilisation de cette
fonction est détaillée dans la section~\ref{sec_merge}
\vpageref{sec_merge}.

\section{Autres sources}

\R offre de très nombreuses autres possibilités pour accéder aux
données. Il est ainsi possible d'importer des données depuis d'autres
applications qui n'ont pas été évoquées (\textsf{Epi Info},
\textsf{S-Plus}, etc.), de se connecter à un système de base de
données relationelle type \textsf{MySql}, de lire des données
\textit{via} \texttt{ODBC} ou des connexions réseau, etc.

Pour plus d'informations on consultera le manuel \textit{R Data Import/Export}~:

\url{http://cran.r-project.org/manuals.html}

\section{Sauver ses données}

\R dispose également de son propre format pour sauvegarder et échanger des 
données. On peut sauver n'importe quel objet créé avec \R et il est possible de
sauver plusieurs objets dans un même fichier. L'usage est d'utiliser l'extension
\texttt{.RData} pour les fichiers de données \R. La fonction à utiliser 
s'appelle tout simplement \rfunc{save}.

Par exemple, si l'on souhaite sauvegarder son tableau de données \texttt{d} 
ainsi que les objets \texttt{tailles} et \texttt{poids} dans un fichier 
\texttt{export.RData} :

<<eval=FALSE>>=
save(d,tailles,poids,file="export.RData")
@ 

À tout moment, il sera toujours possible de recharger ces données en mémoire à
l'aide de la fonction \rfunc{load} :

<<eval=FALSE>>=
load("export.RData")
@

\begin{important}
  Si entre temps vous aviez modifié votre tableau \texttt{d}, vos modifications
  seront perdus. En effet, si lors du chargement de données, un objet du même 
  nom existe en mémoire, ce dernier sera remplacé par l'objet importé.
\end{important}

La fonction \rfunc{save.image} est un raccourci pour sauvergarder tous les 
objets de la session de travail dans le fichier \texttt{.RData} (un fichier un 
peu étrange car il n'a pas de nom mais juste une extension). Lors de  la 
fermeture de \R ou de \textsf{RStudio}, il vous sera demandé si vous souhaitez 
enregistrer votre session. Si vous répondez \textit{Oui}, c'est cette fonction
\rfunc{save.image} qui sera appliquée.

<<eval=FALSE>>=
save.image()
@

\section{Exporter des données}

\R propose également différentes fonctions permettant d'exporter des
données vers des formats variés.

\begin{itemize}
\item \rfunc{write.table} est l'équivalent de \rfunc{read.table} et
  permet d'enregistrer des tableaux de données au format texte, avec
  de nombreuses options~;
\item \rfunc{write.foreign}, de l'extension \texttt{foreign}, permet
  d'exporter des données aux formats \textsf{SAS}, \textsf{SPSS} ou
  \textsf{Stata}~;
\item \rfunc{write.dbf}, de l'extension \texttt{foreign}, permet
  d'exporter des données au format \textsf{dBase}~;
\end{itemize}

À nouveau, pour plus de détails on se référera aux pages d'aide de ces
fonctions et au manuel \textit{R Data Import/Export}.

\section{Exercices}

\begin{exo}{import_tableur}
  Saisissez quelques données fictives dans une application de type
  tableur, enregistrez-les dans un format texte et importez-les dans
  \R.
  
  Vérifiez que l'importation s'est bien déroulée.
\end{exo}

\begin{exo}{import_dbf}
  L'adresse suivante permet de télécharger un fichier au format
  \textsf{dBase} contenant une partie des données de l'enquête
  \textit{EPCV Vie associative} de l'INSEE (2002)~:
  
  \url{http://telechargement.insee.fr/fichiersdetail/epcv1002/dbase/epcv1002_BENEVOLAT_dbase.zip}
  
  Téléchargez le fichier, décompressez-le et importez les données dans \R.
\end{exo}  
 


\chapter{Manipulation de données}

\begin{important}
  Cette partie est un peu aride et pas forcément très intuitive. Elle
  aborde cependant la base de tous les traitements et manipulation de
  données sous \R, et mérite donc qu'on s'y arrête un moment, ou qu'on
  y revienne un peu plus tard en cas de saturation\ldots
\end{important}

\section{Variables}

Le type d'objet utilisé par \R pour stocker des tableaux de données
s'appelle un \textit{data frame}. Celui-ci comporte des observations
en ligne et des variables en colonnes. On accède aux variables d'un
\textit{data frame} avec l'opérateur \rfunc{\$}.

Dans ce qui suit on travaillera sur le jeu de données tiré de
l'enquête \textit{Histoire de vie}, fourni avec l'extension \questionr et
décrit dans l'annexe~\ref{sec_hdv2003}, page~\pageref{sec_hdv2003}.

<<eval=FALSE>>=
library(questionr)
data(hdv2003)
d <- hdv2003
@ 

Mais aussi sur le jeu de données tiré du recensement 1999, décrit
\vpageref{sec_rp99}~:

<<>>=
data(rp99)
@ 



\subsection{Types de variables}

On peut considérer qu'il existe quatre type de variables dans \R~:

\begin{itemize}
\item les variables \textbf{numériques}, ou quantitatives~;
\item les \textbf{facteurs}, qui prennent leurs valeurs dans un
  ensemble défini de modalités. Elles correspondent en général aux
  questions fermées d'un questionnaire~;
\item les variables \textbf{caractères}, qui contiennent des chaînes de
  caractères plus ou moins longues. On les utilise pour les questions
  ouvertes ou les champs libres~;
\item les variables \textbf{booléennes}, qui ne peuvent prendre que la
  valeur \textit{vrai} (\texttt{TRUE}) ou \textit{faux}
  (\texttt{FALSE}). On les utilise dans \R pour les calculs et les
  recodages.
\end{itemize}

Pour connaître le type d'une variable donnée, on peut utiliser la
fonction \rfunc{class}. 


\begin{center}
  \begin{tabular}{ll}
    \textbf{Résultat de \rfunc{class}} & \textbf{Type de variable} \\
    \hline
    \texttt{factor} & Facteur \\
    \texttt{integer} & Numérique \\
    \texttt{double} & Numérique \\
    \texttt{numeric} & Numérique \\
    \texttt{character} & Caractères \\
    \texttt{logical} & Booléenne \\
  \end{tabular}
\end{center}

<<>>=
class(d$age)
class(d$sexe)
class(c(TRUE,TRUE,FALSE))

@ 

La fonction \rfunc{str} permet également d'avoir un listing de toutes
les variables d'un tableau de données et indique le type de chacune
d'elle.

\subsection{Renommer des variables}

Une opération courante lorsqu'on a importé des variables depuis une
source de données externe consiste à renommer les variables
importées. Sous \R les noms de variables doivent être à la fois courts
et explicites tout en obéissant à certaines règles décrites dans la
remarque \vpageref{rq_noms}.

On peut lister les noms des variables d'un \textit{data frame} à
l'aide de la fonction \rfunc{names}~:

<<>>=
names(d)
@ 

Cette fonction peut également être utilisée pour renommer l'ensemble
des variables. Si par exemple on souhaitait passer les noms de toutes
les variables en majuscules, on pourrait faire~:

<<>>=
d.maj <- d
names(d.maj) <- c("ID", "AGE", "SEXE", "NIVETUD", "POIDS", "OCCUP", "QUALIF", 
"FRERES.SOEURS", "CLSO", "RELIG", "TRAV.IMP", "TRAV.SATISF", 
"HARD.ROCK", "LECTURE.BD", "PECHE.CHASSE", "CUISINE", "BRICOL", 
"CINEMA", "SPORT", "HEURES.TV")
summary(d.maj$SEXE)

@ 

Ce type de renommage peut être utile lorsqu'on souhaite passer en
revue tous les noms de variables d'un fichier importé pour les
corriger le cas échéant. Pour faciliter un peu ce travail pas
forcément passionant, on peut utiliser la fonction \rfunc{dput}~:

<<>>=
dput(names(d))
@ 

On obtient en résultat la liste des variables sous forme de vecteur
déclaré. On n'a plus alors qu'à copier/coller cette chaîne, rajouter
\texttt{names(d) <- } devant, et modifier un à un les noms des variables.

Si on souhaite seulement modifier le nom d'une variable, on peut
utiliser la fonction \rfunc{rename.variable} de \marqr l'extension
\questionr. Celle-ci prend en argument le tableau de données, le nom actuel
de la variable et le nouveau nom. Par exemple, si on veut renommer la
variable \texttt{bricol} du tableau de données \texttt{d} en
\texttt{bricolage}~:

<<>>=
d <- rename.variable(d, "bricol", "bricolage")
table(d$bricolage)
@ 

<<echo=FALSE,results="hide">>=
d <- rename.variable(d, "bricolage", "bricol")
@ 


\subsection{Facteurs}
\label{sec_factor}

Parmi les différents types de variables, les \textit{facteurs}
(\texttt{factor}) sont à la fois à part et très utilisés, car ils vont
correspondre à la plupart des variables issues d'une question fermée
dans un questionnaire.

Les facteurs prennent leurs valeurs dans un ensemble de modalités
prédéfinies, et ne peuvent en prendre d'autres. La liste des valeurs
possibles est donnée par la fonction \rfunc{levels}~:

<<>>=
levels(d$sexe)
@ 

Si on veut modifier la valeur du sexe du premier individu de notre
tableau de données avec une valeur différente, on obient un message
d'erreur et une valeur manquante est utilisée à la place~:

<<>>=
d$sexe[1] <- "Chihuahua"
d$sexe[1]
@ 

<<results="hide",echo=FALSE>>=
d <- hdv2003
@ 


On peut très facilement créer un facteur à partir d'une variable de
type caractères avec la commande \rfunc{factor}~:

<<>>=
v <- factor(c("H","H","F","H"))
v
@ 

Par défaut, les niveaux d'un facteur nouvellement créés sont
l'ensemble des valeurs de la variable caractères, ordonnées par ordre
alphabétique. Cette ordre des niveaux est utilisé à chaque fois qu'on
utilise des fonctions comme \rfunc{table}, par exemple~:

<<>>=
table(v)
@ 

On peut modifier cet ordre au moment de la création du facteur en
utilisant l'option \texttt{levels}~:

<<>>=
v <- factor(c("H","H","F","H"), levels=c("H","F"))
table(v)
@ 

On peut aussi modifier l'ordre des niveaux d'une variable déjà existante~:

<<>>=
d$qualif <- factor(d$qualif, levels=c("Ouvrier specialise", "Ouvrier qualifie", "Employe", "Technicien", 
"Profession intermediaire", "Cadre",  "Autre"))
table(d$qualif)                   
@ 

On peut également modifier les niveaux eux-mêmes. Imaginons que l'on souhaite
créer une nouvelle variable \texttt{qualif.abr} contenant les noms abrégés des
catégories socioprofessionnelles de \texttt{qualif}. On peut alors procéder
comme suit~:

<<r factorlabels>>=
d$qualif.abr <- factor(d$qualif, 
                   levels=c("Ouvrier specialise", "Ouvrier qualifie", "Employe", "Technicien", 
"Profession intermediaire", "Cadre",  "Autre"),
                   labels=c("OS","OQ","Empl","Tech","Interm","Cadre","Autre"))
table(d$qualif.abr)                   
@ 

Dans ce qui précède, le paramètre \texttt{levels} de \rfunc{factor} permet de
spécifier quels sont les niveaux retenus dans le facteur résultat, ainsi que
leur ordre. Le paramètre \texttt{labels}, lui, permet de modifier les noms de
ces niveaux dans le facteur résultat. Il est donc capital d'indiquer les noms
de \texttt{labels} exactement dans le même ordre que les niveaux de
\texttt{levels}. Pour s'assurer de ne pas avoir commis d'erreur, il est
recommandé d'effectuer un tableau croisé entre l'ancien et le nouveau facteur~:

<<r factorcrosstabverif>>=
table(d$qualif, d$qualif.abr)
@ 

On a donc ici un premier moyen d'effectuer un recodage des modalités d'une
variable de type facteur. D'autres méthodes existent, elles sont notamment
détaillées section~\ref{sec_recodages} \vpageref{sec_recodages}.

À noter que par défaut, les valeurs manquantes ne sont pas considérées comme un
niveau de facteur. On peut cependant les transformer en niveau en
utilisant la fonction \rfunc{addNA}. Ceci signifie cependant
qu'elle ne seront plus considérées comme manquantes par \R~:

<<>>=
summary(d$trav.satisf)
summary(addNA(d$trav.satisf))
@ 





\section{Indexation}
\label{sec_indexation}
L'indexation est l'une des fonctionnalités les plus puissantes mais
aussi les plus difficiles à maîtriser de \R. Il s'agit d'opérations
permettant de sélectionner des sous-ensembles d'observations et/ou de
variables en fonction de différents critères. L'indexation peut porter
sur des vecteurs, des matrices ou des tableaux de données.

Le principe est toujours le même~: on indique, entre crochets et à la
suite du nom de l'objet à indexer, une série de conditions indiquant
ce que l'on garde ou non. Ces conditions peuvent être de différents
types.

\subsection{Indexation directe}

Le mode le plus simple d'indexation consiste à indiquer la position
des éléments à conserver. Dans le cas d'un vecteur cela permet de
sélectionner un ou plusieurs éléments de ce vecteur.

Soit le vecteur suivant~:

<<>>=
v <- c("a","b","c","d","e","f","g")
@ 

Si on souhaite le premier élément du vecteur, on peut faire~:

<<>>=
v[1]
@ 

Si on souhaite les trois premiers éléments ou les éléments 2, 6 et 7~:

<<>>=
v[1:3]
v[c(2,6,7)]
@ 

Si on veut le dernier élément~:

<<>>=
v[length(v)]
@ 

Dans le cas de matrices ou de tableaux de données, l'indexation prend
deux arguments séparés par une virgule~: le premier concerne les
\textit{lignes} et le second les \textit{colonnes}. Ainsi, si on veut
l'élément correspondant à la troisième ligne et à la cinquième colonne
du tableau de données \texttt{d}~:

<<>>=
d[3,5]
@ 

On peut également indiquer des vecteurs~:

<<>>=
d[1:3,1:2]
@ 

Si on laisse l'un des deux critères vides, on sélectionne
l'intégralité des lignes ou des colonnes. Ainsi si l'on veut seulement
la cinquième colonne ou les deux premières lignes~:

<<>>=
d[,5]
d[1:2,]
@ 

Enfin, si on préfixe les arguments avec le signe <<~\texttt{-}~>>, ceci
signifie <<~tous les éléments sauf ceux indiqués~>>. Si par exemple on
veut tous les éléments de \texttt{v} sauf le premier~:


<<>>=
v[-1]
@ 

Bien sûr, tous ces critères se combinent et on peut stocker le
résultat dans un nouvel objet. Dans cet exemple \texttt{d2} contiendra
les trois premières lignes de \texttt{d} mais sans les colonnes 2, 6
et 8.

<<>>=
d2 <- d[1:3, -c(2,6,8)]
@ 


\subsection{Indexation par nom}

Un autre mode d'indexation consiste à fournir non pas un numéro mais
un nom sous forme de chaîne de caractères. On l'utilise couramment
pour sélectionner les variables d'un tableau de données. Ainsi, les
deux fonctions suivantes sont équivalentes\footnotemark~:

<<>>=
d$clso
d[,"clso"]
@ 

\footnotetext{Une différence entre les deux est que \texttt{\$} admet une
  correspondance partielle du nom de variable, si celle-ci est unique. Ainsi,
  \texttt{d\$cls} renverra bien la variable \texttt{clso}, tandis que
  \texttt{d\$c} renverra \texttt{NULL}, du fait que plusieurs variables de
  \texttt{d} commencent par \texttt{c}.}

Là aussi on peut utiliser un vecteur pour sélectionner plusieurs noms
et récupérer un <<~sous-tableau~>> de données~:

<<>>=
d2 <- d[,c("id","sexe","age")]
@ 

Les noms peuvent également être utilisés pour les observations
(lignes) d'un tableau de données si celles-ci ont été munies d'un nom
avec la fonction \rfunc{row.names}. Par défaut les noms de ligne sont
leur numéro d'ordre, mais on peut leur assigner comme nom la valeur
d'une variable d'identifiant. Ainsi, on peut assigner aux lignes du
jeu de données \texttt{rp99} le nom des communes correspondantes~:

<<>>=
row.names(rp99) <- rp99$nom
@ 

On peut alors accéder directement aux communes en donnant leur nom~:

<<>>=
rp99[c("VILLEURBANNE","OULLINS"),]
@ 

Par contre il n'est pas possible d'utiliser directement l'opérateur
<<~\texttt{-}~>> comme pour l'indexation directe. Pour exclure une colunne en
fonction de son nom, on doit uiliser une autre forme d'indexation,
\textit{l'indexation par condition}, expliquée dans la section suivante. On
peut ainsi faire~:

<<>>=
d2 <- d[, names(d) != "qualif"]
@ 

Pour sélectionner toutes les colonnes sauf celle qui s'appelle
\texttt{qualif}.

\subsection{Indexation par conditions}

\subsubsection{Tests et conditions}

Une condition est une expression logique dont le résultat est soit
\texttt{TRUE} (vrai) soit \texttt{FALSE} (faux).

Une condition comprend la plupart du temps un opérateur de
comparaison. Les plus courants sont les suivants~:

\begin{center}
  \begin{tabular}{cl}
    \textbf{Opérateur} & \textbf{Signification} \\
    \hline
    \rfunc{==} & égal à \\
    \texttt{!=}\index{"!=@\texttt{"!=}} & différent de \\
    \rfunc{>} & strictement supérieur à \\
    \rfunc{<} & strictement inférieur à \\
    \rfunc{>=} & supérieur ou égal à \\
    \rfunc{<=} & inférieur ou égal à \\
  \end{tabular}
\end{center}

Voyons tout de suite un exemple~:

<<>>=
d$sexe == "Homme"
@ 

Que s'est-il passé~? Nous avons fourni à \R une condition qui signifie
<<~la valeur de la variable \texttt{sexe} vaut "Homme"~>>. Et il nous
a renvoyé un vecteur avec autant d'éléments qu'il y'a d'observations
dans \texttt{d}, et dont la valeur est \texttt{TRUE} si l'observation
correspond à un homme, et \texttt{FALSE} dans les autres cas.

Prenons un autre exemple. On n'affichera cette fois que les premiers
éléments de notre variable d'intérêt à l'aide de la fonction \rfunc{head}~:
<<>>=
head(d$age)
head(d$age>40)
@ 

On voit bien ici qu'à chaque élément du vecteur \texttt{d\$age} dont la
valeur est supérieure à 40 correspond un élément \texttt{TRUE} dans le
résultat de la condition.

On peut combiner ou modifier des conditions à l'aide des opérateurs
logiques habituels~:

\begin{center}
  \begin{tabular}{cl}
    \textbf{Opérateur} & \textbf{Signification} \\
    \hline
    \rfunc{\&} & et logique \\
    \rfunc{|} & ou logique \\
    \texttt{!}\index{"!@\texttt{"!}} & négation logique \\
  \end{tabular}
\end{center}

Comment les utilise-t-on~? Voyons tout de suite des
exemples. Supposons que je veuille déterminer quels sont dans mon
échantillon les hommes ouvriers spécialisés~:

<<>>=
d$sexe=="Homme" & d$qualif=="Ouvrier specialise"
@ 

Si je souhaite identifier les personnes qui bricolent ou qui font la
cuisine~:

<<>>=
d$bricol=="Oui" | d$cuisine=="Oui"
@ 

Si je souhaite isoler les femmes qui ont entre 20 et 34 ans~:

<<>>=
d$sexe=="Femme" & d$age>=20 & d$age <=34
@ 

Si je souhaite récupérer les enquêtés qui ne sont pas cadres, on peut
utiliser l'une des deux formes suivantes~:

<<>>=
d$qualif != "Cadre"
!(d$qualif == "Cadre")
@ 



Lorsqu'on mélange <<~et~>> et <<~ou~>> il est nécessaire d'utiliser des
parenthèses pour différencier les blocs. La condition suivante
identifie les femmes qui sont soit cadre, soit employée~:

<<>>=
d$sexe=="Femme" & (d$qualif=="Employe" | d$qualif=="Cadre")
@ 

L'opérateur \rfunc{\%in\%} peut être très utile~: il teste si une
valeur fait partie des éléments d'un vecteur. Ainsi on pourrait
remplacer la condition précédente par~:

<<>>=
d$sexe=="Femme" & d$qualif %in% c("Employe", "Cadre")
@ 

Enfin, signalons qu'on peut utiliser les fonctions \rfunc{table} ou
\rfunc{summary} pour avoir une idée du résultat de notre condition~:

<<>>=
table(d$sexe)
table(d$sexe == "Homme")
summary(d$sexe == "Homme")
@ 




\subsubsection{Utilisation pour l'indexation}

L'utilisation des conditions pour l'indexation est assez simple~: si
on indexe un vecteur avec un vecteur booléen, seuls les éléments
correspondant à \texttt{TRUE} seront conservés.

Ainsi, si on fait~:

<<>>=
dh <- d[d$sexe == "Homme", ]
@ 

On obtiendra un nouveau tableau de données comportant l'ensemble des
variables de \texttt{d}, mais seulement les observations pour
lesquelles \texttt{d\$sexe} vaut <<~Homme~>>.

La plupart du temps ce type d'indexation s'applique aux lignes, mais
on peut aussi l'utiliser sur les colonnes d'un tableau de données.
L'exemple suivant, un peu compliqué, sélectionne uniquement les
variables dont le nom commence par \texttt{a} ou \texttt{s}~:

<<>>=
d[, substr(names(d),0,1) %in% c("a", "s")]
@ 

On peut évidemment combiner les différents type
d'indexation. L'exemple suivant sélectionne les femmes de plus de 40
ans et ne conserve que les variables \texttt{qualif} et \texttt{bricol}.

<<>>=
d2 <- d[d$sexe=="Femme" & d$age > 40, c("qualif", "bricol")]
@ 

\subsubsection{Valeurs manquantes dans les conditions}

Une remarque importante~: quand l'un des termes d'une condition
comporte une valeur manquante (\texttt{NA}), le résultat de cette
condition n'est pas toujours \texttt{TRUE} ou \texttt{FALSE}, il peut
aussi être à son tour une valeur manquante.

<<>>=
v <- c(1:5,NA)
v
v > 3
@ 

On voit que le test \texttt{NA > 3} ne renvoie ni vrai ni faux, mais
\texttt{NA}.

Le résultat d'une condition peut donc comporter un grand nombre de
valeurs manquantes~:

<<>>=
summary(d$trav.satisf=="Satisfaction")
@ 

Une autre conséquence importante de ce comportement est qu'on ne peut
pas utiliser l'opérateur \texttt{== NA} pour tester la présence de
valeurs manquantes. On utilisera à la place la fonction \textit{ad
  hoc} \rfunc{is.na}.

On comprendra mieux le problème avec l'exemple suivant~:

<<>>=
v <- c(1, NA)
v
v == NA
is.na(v)
@ 

Pour compliquer encore un peu le tout, lorsqu'on utilise une condition pour
l'indexation, si la condition renvoie \texttt{NA}, \R ne sélectionne pas
l'élément mais retourne quand même la valeur \texttt{NA}. Ceci aura donc des
conséquences pour l'extraction de sous-populations, comme indiqué
section~\ref{sec_souspopNA} \vpageref{sec_souspopNA}.



\subsection{Indexation et assignation}

Dans tous les exemples précédents, on a utilisé l'indexation pour
extraire une partie d'un vecteur ou d'un tableau de données, en
plaçant l'opération d'indexation à droite de l'opérateur \texttt{<-}.

Mais l'indexation peut également être placée à gauche de cet
opérateur. Dans ce cas, les éléments sélectionnés par l'indexation
sont alors remplacés par les valeurs indiquées à droite de
l'opérateur \texttt{<-}.

Ceci est parfaitement incompréhensible. Prenons donc un exemple
simple~:

<<>>=
v <- 1:5
v
v[1] <- 3
v
@ 

Cette fois, au lieu d'utiliser quelque chose comme \texttt{x <- v[1]},
qui aurait placé la valeur du premier élément de \texttt{v} dans
\texttt{x}, on a utilisé \texttt{v[1] <- 3}, ce qui a \textit{mis à
  jour} le premier élément de \texttt{v} avec la valeur 3.


Ceci fonctionne également pour les tableaux de données et pour les
différents types d'indexation évoqués précédemment~:

<<>>=
d[257, "sexe"] <- "Homme"
@ 

Enfin on peut modifier plusieurs éléments d'un seul coup soit en
fournissant un vecteur, soit en profitant du mécanisme de
recyclage. Les deux commandes suivantes sont ainsi rigoureusement
équivalentes~:

<<>>=
d[c(257,438,889), "sexe"] <- c("Homme", "Homme", "Homme")
d[c(257,438,889), "sexe"] <- "Homme"
@ 

On commence à voir comment l'utilisation de l'indexation par
conditions et de l'assignation va nous permettre de faire des recodages.

<<>>=
d$age[d$age >= 20 & d$age <= 30] <- "20-30 ans"
d$age[is.na(d$age)] <- "Inconnu"
@ 



\section{Sous-populations}

<<echo=FALSE, results="hide">>=
d <- hdv2003
@ 


\subsection{Par indexation}

La première manière de construire des sous-populations est d'utiliser
l'indexation par conditions. On peut ainsi facilement sélectionner une
partie des observations suivant un ou plusieurs critères et placer le
résultat dans un nouveau tableau de données.

Par exemple si on souhaite isoler les hommes et les femmes~:

<<>>=
dh <- d[d$sexe=="Homme",]
df <- d[d$sexe=="Femme",]
table(d$sexe)
dim(dh)
dim(df)
@ 

On a à partir de là trois tableaux de données, \texttt{d} comportant la
population totale, \texttt{dh} seulement les hommes et \texttt{df}
seulement les femmes.

On peut évidemment combiner plusieurs critères~:

<<>>=
dh.25 <- d[d$sexe=="Homme" & d$age <=25, ]
dim(dh.25)
@ 

\label{sec_souspopNA}
Si on utilise directement l'indexation, il convient cependant d'être
extrêmement prudent avec les valeurs manquantes. Comme indiqué
précédemment, la présence d'une valeur manquante dans une condition
fait que celle-ci est évaluée en \texttt{NA} et qu'au final la ligne
correspondante est conservée par l'indexation~:

<<>>=
summary(d$trav.satisf)
d.satisf <- d[d$trav.satisf=="Satisfaction", ]
dim(d.satisf)
@ 

Comme on le voit, ici \texttt{d.satisf} contient les individus ayant
la modalité \textit{Satisfaction} mais aussi ceux ayant une valeur
manquante \texttt{NA}. C'est pourquoi il faut toujours soit vérifier
au préalable qu'on n'a pas de valeurs manquantes dans les variables de
la condition, soit exclure explicitement les \texttt{NA} de la manière suivante~:

<<>>=
d.satisf <- d[d$trav.satisf=="Satisfaction" & !is.na(d$trav.satisf), ]
dim(d.satisf)
@ 

C'est notamment pour cette raison qu'on préfèrera le plus souvent
utiliser la fonction \rfunc{subset}.


\subsection{Fonction \rfunc{subset}}
\label{subset}

La fonction \rfunc{subset} permet d'extraire des sous-populations de
manière plus simple et un peu plus intuitive que l'indexation directe.

Celle-ci prend trois arguments principaux~:

\begin{itemize}
\item le nom de l'objet de départ~;
\item une condition sur les observations (\texttt{subset})~;
\item éventuellement une condition sur les colonnes
  (\texttt{select}).
\end{itemize}

Reprenons tout de suite un exemple déjà vu~:

<<>>=
dh <- subset(d, sexe=="Homme")
df <- subset(d, sexe=="Femme")
@ 

L'utilisation de \rfunc{subset} présente plusieurs avantages. Le
premier est d'économiser quelques touches. On n'est en effet pas
obligé de saisir le nom du tableau de données dans la condition sur
les lignes. Ainsi les deux commandes suivantes sont équivalentes~:

<<>>=
dh <- subset(d, d$sexe=="Homme")
dh <- subset(d, sexe=="Homme")
@ 

Le second avantage est que \rfunc{subset} s'occupe du problème des
valeurs manquantes évoquées précédemment et les exclut de lui-même,
contrairement au comportement par défaut~:

<<>>=
summary(d$trav.satisf)
d.satisf <- d[d$trav.satisf=="Satisfaction", ]
dim(d.satisf)
d.satisf <- subset(d, trav.satisf=="Satisfaction")
dim(d.satisf)
@ 

Enfin, l'utilisation de l'argument \texttt{select} est simplifié pour
l'expression de condition sur les colonnes. On peut ainsi spécifier
les noms de variable sans guillemets et leur appliquer directement
l'opérateur d'exclusion \texttt{-}~:

<<>>=
d2 <- subset(d, select=c(sexe, sport))
d2 <- subset(d, age > 25, select=-c(id, age, bricol))
@ 

\subsection{Fonction \rfunc{tapply}}
\label{sec_tapply}

\begin{remarque}
  Cette section documente une fonction qui peut être très utile, mais
  pas forcément indispensable au départ.
\end{remarque}

La fonction \rfunc{tapply} n'est qu'indirectement liée à la notion de
sous-population, mais peut permettre d'éviter d'avoir à créer ces
sous-populations dans certains cas.

Son fonctionnement est assez simple, mais pas forcément intuitif. La
fonction prend trois arguments~: un vecteur, un facteur et une
fonction. Elle applique ensuite la fonction aux éléments du vecteur
correspondant à un même niveau du facteur. Vite, un exemple~!

<<>>=
tapply(d$age, d$sexe, mean)
@ 

Qu'est-ce que ça signifie~? Ici \rfunc{tapply} a sélectionné toutes
les observations correspondant à <<~Homme~>>, puis appliqué la
fonction \rfunc{mean} aux valeurs de \texttt{age}
correspondantes. Puis elle a fait de même pour les observations
correspondant à <<~Femme~>>. On a donc ici la moyenne d'âge chez les
hommes et chez les femmes.

On peut fournir à peu près n'importe quelle fonction à \rfunc{tapply}~:

<<>>=
tapply(d$bricol, d$sexe, freq)
@ 

Les arguments supplémentaires fournis à \rfunc{tapply} sont en fait
fournis directement à la fonction appelée.

<<>>=
tapply(d$bricol, d$sexe, freq, total=TRUE)
@ 

\begin{remarque}
  La fonction \rfunc{by} est un équivalent (pour les tableaux de données) de
  \rfunc{tapply}. La présentation des résultats diffère légèrement.
  
  <<>>=
  tapply(d$age, d$sexe, mean)
  by(d$age, d$sexe, mean)
  @
\end{remarque}


\section{Recodages}
\label{sec_recodages}

Le recodage de variables est une opération extrêmement fréquente lors
du traitement d'enquête. Celui-ci utilise soit l'une des formes
d'indexation décrites précédemment, soit des fonctions \textit{ad hoc}
de \R. 

On passe ici en revue différents types de recodage parmi les plus
courants. Les exemples s'appuient, comme précédemment, sur l'extrait
de l'enquête \textit{Histoire de vie}~:

<<>>=
data(hdv2003)
d <- hdv2003
@ 

\subsection{Convertir une variable}

Il peut arriver qu'on veuille transformer une variable d'un type dans
un autre.

Par exemple, on peut considérer que la variable numérique
\texttt{freres.soeurs} est une <<~fausse~>> variable numérique et
qu'une représentation sous forme de facteur serait plus adéquate. Dans
ce cas il suffit de faire appel à la fonction \rfunc{factor}~:

<<>>=
d$fs.fac <- factor(d$freres.soeurs)
levels(d$fs.fac)
@ 

La conversion d'une variable caractères en facteur se fait de la même
manière.

La conversion d'un facteur ou d'une variable numérique en variable
caractères peut se faire à l'aide de la fonction \rfunc{as.character}~:

<<>>=
d$fs.char <- as.character(d$freres.soeurs)
d$qualif.char <- as.character(d$qualif)
@ 

La conversion d'un facteur en caractères est fréquemment utilisé lors des
recodages du fait qu'il est impossible d'ajouter de nouvelles modalités à un
facteur de cette manière. Par exemple, la première des commandes suivantes
génère un message d'avertissement, tandis que les deux autres fonctionnent~:

<<eval=FALSE>>=
d$qualif[d$qualif=="Ouvrier specialise"] <- "Ouvrier"
d$qualif.char <- as.character(d$qualif)
d$qualif.char[d$qualif.char=="Ouvrier specialise"] <- "Ouvrier"
@ 

Dans le premier cas, le message d'avertissement indique que toutes les
modalités <<~Ouvrier specialise~>> de notre variable
\texttt{qualif} ont été remplacées par des valeurs manquantes \texttt{NA}.

Enfin, une variable de type caractères dont les valeurs seraient des
nombres peut être convertie en variable numérique avec la fonction
\rfunc{as.numeric}. Si on souhaite convertir un facteur en variable
numérique, il faut d'abord le convertir en variable de classe
caractère~:

<<>>=
d$fs.num <- as.numeric(as.character(d$fs.fac))
@ 






\subsection{Découper une variable numérique en classes}
\label{decouper_var_num}

Le premier type de recodage consiste à découper une variable de type
numérique en un certain nombre de classes. On utilise pour cela la
fonction \rfunc{cut}.

Celle-ci prend, outre la variable à découper, un certain nombre d'arguments~:
\begin{itemize}
\item \texttt{breaks} indique soit le nombre de classes souhaité,
  soit, si on lui fournit un vecteur, les limites des classes~;
\item \texttt{labels} permet de modifier les noms de modalités
  attribués aux classes~;
\item \texttt{include.lowest} et \texttt{right} influent sur la
  manière dont les valeurs situées à la frontière des classes seront
  inclues ou exclues~;
\item \texttt{dig.lab} indique le nombre de chiffres après la virgule
  à conserver dans les noms de modalités.
\end{itemize}

Prenons tout de suite un exemple et tentons de découper notre variable
\texttt{age} en cinq classes et de placer le résultat dans une
nouvelle variable nommée \texttt{age5cl}~:

<<>>=
d$age5cl <- cut(d$age, 5)
table(d$age5cl)
@ 

Par défaut \R nous a bien créé cinq classes d'amplitudes égales. La
première classe va de 16,9 à 32,2 ans (en fait de 17 à 32), etc.

Les frontières de classe seraient plus présentables si elles
utilisaient des nombres entiers. On va donc spécifier manuellement le
découpage souhaité, par tranches de 20 ans~:

<<>>=
d$age20 <- cut(d$age, c(0,20,40,60,80,100))
table(d$age20)
@ 

On aurait pu tenir compte des âges extrêmes pour la première et la
dernière valeur~:

<<>>=
range(d$age)
d$age20 <- cut(d$age, c(17,20,40,60,80,93))
table(d$age20)
@ 

Les symboles dans les noms attribués aux classes ont leur importance~:
\texttt{(} signifie que la frontière de la classe est exclue, tandis
que \texttt{[} signifie qu'elle est incluse. Ainsi, \texttt{(20,40]}
signifie <<~strictement supérieur à 20 et inférieur ou égal à 40~>>.

On remarque que du coup, dans notre exemple précédent, la valeur
minimale, 17, est exclue de notre première classe, et qu'une
observation est donc absente de ce découpage. Pour résoudre ce
problème on peut soit faire commencer la première classe à 16, soit
utiliser l'option \texttt{include.lowest=TRUE}~:

<<>>=
d$age20 <- cut(d$age, c(16,20,40,60,80,93))
table(d$age20)
d$age20 <- cut(d$age, c(17,20,40,60,80,93), include.lowest=TRUE)
table(d$age20)
@ 

On peut également modifier le sens des intervalles avec l'option
\texttt{right=FALSE}, et indiquer manuellement les noms des modalités
avec \texttt{labels}~:

<<>>=
d$age20 <- cut(d$age, c(16,20,40,60,80,93), right=FALSE, include.lowest=TRUE)
table(d$age20)
d$age20 <- cut(d$age, c(17,20,40,60,80,93), include.lowest=TRUE, labels=c("<20ans", "21-40 ans", "41-60ans", "61-80ans", ">80ans"))
table(d$age20)
@ 

Enfin, l'extension \questionr \marqr propose une fonction
\rfunc{quant.cut} permettant de découper une variable numérique en un
nombre de classes donné ayant des efffectifs semblables. Il suffit de
lui passer le nombre de classes en argument~:

<<>>=
d$age6cl <- quant.cut(d$age, 6)
table(d$age6cl)
@ 

\rfunc{quant.cut} admet les mêmes autres options que \rfunc{cut}
(\texttt{include.lowest, right, labels}\ldots).


\subsection{Regrouper les modalités d'une variable}
\label{regrouper_modalites}

Pour regrouper les modalités d'une variable qualitative (d'un facteur
le plus souvent), on peut utiliser directement l'indexation.

Ainsi, si on veut recoder la variable \texttt{qualif} dans une
variable \texttt{qualif.reg} plus <<~compacte~>>, on peut utiliser~:

<<>>=
table(d$qualif)
d$qualif.reg[d$qualif=="Ouvrier specialise"] <- "Ouvrier"
d$qualif.reg[d$qualif=="Ouvrier qualifie"] <- "Ouvrier"
d$qualif.reg[d$qualif=="Employe"] <- "Employe"
d$qualif.reg[d$qualif=="Profession intermediaire"] <- "Intermediaire"
d$qualif.reg[d$qualif=="Technicien"] <- "Intermediaire"
d$qualif.reg[d$qualif=="Cadre"] <- "Cadre"
d$qualif.reg[d$qualif=="Autre"] <- "Autre"
table(d$qualif.reg)
@ 

On aurait pu représenter ce recodage de manière plus compacte,
notamment en commençant par copier le contenu de \texttt{qualif} dans
\texttt{qualif.reg}, ce qui permet de ne pas s'occuper de ce qui ne
change pas. Il est cependant nécessaire de ne pas copier
\texttt{qualif} sous forme de facteur, sinon on ne pourrait ajouter de
nouvelles modalités. On copie donc la version \textit{caractères} de
\texttt{qualif} grâce à la fonction \rfunc{as.character}~:

<<>>=
d$qualif.reg <- as.character(d$qualif)
d$qualif.reg[d$qualif=="Ouvrier specialise"] <- "Ouvrier"
d$qualif.reg[d$qualif=="Ouvrier qualifie"] <- "Ouvrier"
d$qualif.reg[d$qualif=="Profession intermediaire"] <- "Intermediaire"
d$qualif.reg[d$qualif=="Technicien"] <- "Intermediaire"
table(d$qualif.reg)
@ 

On peut faire une version encore plus compacte en utilisant
l'opérateur logique \textit{ou} (\rfunc{|})~:

<<>>=
d$qualif.reg <- as.character(d$qualif)
d$qualif.reg[d$qualif=="Ouvrier specialise" | d$qualif=="Ouvrier qualifie"] <- "Ouvrier"
d$qualif.reg[d$qualif=="Profession intermediaire" | d$qualif=="Technicien"] <- "Intermediaire"
table(d$qualif.reg)
@ 

Enfin, pour terminer ce petit tour d'horizon, on peut également
remplacer l'opérateur \rfunc{|} par \rfunc{\%in\%}, qui peut parfois
être plus lisible~:

<<>>=
d$qualif.reg <- as.character(d$qualif)
d$qualif.reg[d$qualif %in% c("Ouvrier specialise", "Ouvrier qualifie")] <- "Ouvrier"
d$qualif.reg[d$qualif %in% c("Profession intermediaire", "Technicien")] <- "Intermediaire"
table(d$qualif.reg)
@ 

Dans tous les cas le résultat obtenu est une variable de type
\textit{caractère}. On pourra la convertir en \textit{facteur} par un
simple~:

<<>>=
d$qualif.reg <- factor(d$qualif.reg)
@ 

Si on souhaite recoder les valeurs manquantes, il suffit de faire
appel à la fonction \rfunc{is.na}~:

<<>>=
table(d$trav.satisf)
d$trav.satisf.reg <- as.character(d$trav.satisf)
d$trav.satisf.reg[is.na(d$trav.satisf)] <- "Valeur manquante"
table(d$trav.satisf.reg)
@ 



\subsection{Variables calculées}

La création d'une variable numérique à partir de calculs sur une ou
plusieurs autres variables numériques se fait très simplement.

Supposons que l'on souhaite calculer une variable indiquant l'écart
entre le nombre d'heures passées à regarder la télévision et 
la moyenne globale de cette variable. On pourrait alors faire~:

<<>>=
range(d$heures.tv, na.rm=TRUE)
mean(d$heures.tv, na.rm=TRUE)
d$ecart.heures.tv <- d$heures.tv - mean(d$heures.tv, na.rm=TRUE)
range(d$ecart.heures.tv, na.rm=TRUE)
mean(d$ecart.heures.tv, na.rm=TRUE)
@ 

Autre exemple tiré du jeu de données \texttt{rp99}~: si on souhaite
calculer le pourcentage d'actifs dans chaque commune, on peut diviser
la population active \texttt{pop.act} par la population totale
\texttt{pop.tot}.

<<>>=
rp99$part.actifs <- rp99$pop.act / rp99$pop.tot * 100
@ 



\subsection{Combiner plusieurs variables}

La combinaison de plusieurs variables se fait à l'aide des techniques
d'indexation déjà décrites précédemment. Le plus compliqué est
d'arriver à formuler des conditions parfois complexes de manière rigoureuse.

On peut ainsi vouloir combiner plusieurs variables qualitatives en une seule~:

<<>>=
d$act.manuelles <- NA
d$act.manuelles[d$cuisine=="Oui" & d$bricol=="Oui"] <- "Cuisine et Bricolage"
d$act.manuelles[d$cuisine=="Oui" & d$bricol=="Non"] <- "Cuisine seulement"
d$act.manuelles[d$cuisine=="Non" & d$bricol=="Oui"] <- "Bricolage seulement"
d$act.manuelles[d$cuisine=="Non" & d$bricol=="Non"] <- "Ni cuisine ni bricolage"
table(d$act.manuelles)
@ 

On peut également combiner variables qualitatives et variables quantitatives~:

<<>>=
d$age.sexe <- NA
d$age.sexe[d$sexe=="Homme" & d$age < 40] <- "Homme moins de 40 ans"
d$age.sexe[d$sexe=="Homme" & d$age >= 40] <- "Homme plus de 40 ans"
d$age.sexe[d$sexe=="Femme" & d$age < 40] <- "Femme moins de 40 ans"
d$age.sexe[d$sexe=="Femme" & d$age >= 40] <- "Femme plus de 40 ans"
table(d$age.sexe)
@ 

Les combinaisons de variables un peu complexes nécessitent parfois un
petit travail de réflexion. En particulier, l'ordre des commandes de
recodage a parfois une influence dans le résultat final.


\subsection{Variables scores}

Une variable score est une variable calculée en additionnant des poids
accordés aux modalités d'une série de variables qualitatives.

Pour prendre un exemple tout à fait arbitraire, imaginons que nous
souhaitons calculer un score d'activités extérieures. Dans ce score on
considère que le fait d'aller au cinéma <<~pèse~>> 10, celui de pêcher
ou chasser vaut 30 et celui de faire du sport vaut 20. On pourrait
alors calculer notre score de la manière suivante~:

 
<<>>=
d$score.ext <- 0
d$score.ext[d$cinema=="Oui"] <- d$score.ext[d$cinema=="Oui"] + 10
d$score.ext[d$peche.chasse=="Oui"] <- d$score.ext[d$peche.chasse=="Oui"] + 30
d$score.ext[d$sport=="Oui"] <- d$score.ext[d$sport=="Oui"] + 20
table(d$score.ext)
@ 

Cette notation étant un peu lourde, on peut l'alléger un peu en
utilisant la fonction \rfunc{ifelse}. Celle-ci prend en argument une
condition et deux valeurs. Si la condition est vraie elle retourne la
première valeur, sinon elle retourne la seconde.

 
<<>>=
d$score.ext <- 0
d$score.ext <- ifelse(d$cinema=="Oui", 10, 0) +
               ifelse(d$peche.chasse=="Oui", 30, 0) +
               ifelse(d$sport=="Oui", 20, 0)
table(d$score.ext)
@ 



\subsection{Vérification des recodages}

Il est très important de vérifier, notamment après les recodages les
plus complexes, qu'on a bien obtenu le résultat escompté. Les deux
points les plus sensibles étant les valeurs manquantes et les erreurs
dans les conditions.

Pour vérifier tout cela le plus simple est sans doute de faire des
tableaux croisés entre la variable recodée et celles ayant servi au
recodage, à l'aide de la fonction \rfunc{table}, et de vérifier le
nombre de valeurs manquantes dans la variable recodée avec
\rfunc{summary}, \rfunc{freq} ou \rfunc{table}.

Par exemple~:

 
<<>>=
d$act.manuelles <- NA
d$act.manuelles[d$cuisine=="Oui" & d$bricol=="Oui"] <- "Cuisine et Bricolage"
d$act.manuelles[d$cuisine=="Oui" & d$bricol=="Non"] <- "Cuisine seulement"
d$act.manuelles[d$cuisine=="Non" & d$bricol=="Oui"] <- "Bricolage seulement"
d$act.manuelles[d$cuisine=="Non" & d$bricol=="Non"] <- "Ni cuisine ni bricolage"
table(d$act.manuelles, d$cuisine)
table(d$act.manuelles, d$bricol)
@ 

\section{Tri de tables}

On a déjà évoqué l'existence de la fonction \rfunc{sort}, qui permet
de trier les éléments d'un vecteur.

 
<<>>=
sort(c(2,5,6,1,8))
@ 

On peut appliquer cette fonction à une variable, mais celle-ci ne
permet que d'ordonner les valeurs de cette variable, et pas l'ensemble
du tableau de données dont elle fait partie. Pour cela nous avons
besoin d'une autre fonction, nommée \rfunc{order}. Celle-ci ne
renvoie pas les valeurs du vecteur triées, mais les emplacements de
ces valeurs.

Un exemple pour comprendre~:

 
<<>>=
order(c(15,20,10))
@ 

Le résultat renvoyé signifie que la plus petite valeur est la valeur
située en 3ème position, suivie de celle en 1ère position et de celle
en 2ème position. Tout cela ne paraît pas passionnant à première vue,
mais si on mélange ce résultat avec un peu d'indexation directe, ça
devient intéressant\ldots

 
<<>>=
order(d$age)
@ 

Ce que cette fonction renvoie, c'est l'ordre dans lequel on doit
placer les éléments de \texttt{age}, et donc par extension les lignes
de \texttt{d}, pour que la variable soit triée par ordre
croissant. Par conséquent, si on fait~:

 
<<>>=
d.tri <- d[order(d$age),]
@ 

Alors on a trié les lignes de \texttt{d} par ordre d'âge croissant~!
Et si on fait un petit~:

 
<<>>=
head(d.tri, 3)
@ 

On a les caractéristiques des trois enquêtés les plus jeunes.

On peut évidemment trier par ordre décroissant en utilisant l'option
\texttt{decreasing=TRUE}. On peut donc afficher les caractéristiques
des trois individus les plus âgés avec~:

 
<<eval=FALSE>>=
head(d[order(d$age, decreasing=TRUE),], 3)
@ 



\section{Fusion de tables}
\label{sec_merge}

Lorsqu'on traite de grosses enquêtes, notamment les enquêtes de
l'INSEE, on a souvent à gérer des données réparties dans plusieurs
tables, soit du fait de la construction du questionnaire, soit du fait
de contraintes techniques (fichiers \texttt{dbf} ou \textsf{Excel}
limités à 256 colonnes, par exemple).

Une opération relativement courante consiste à \textit{fusionner}
plusieurs tables pour regrouper tout ou partie des données dans un
unique tableau.

Nous allons simuler artificiellement une telle situation en créant deux
tables à partir de l'extrait de l'enquête \textit{Histoire de vie}~:

 
<<>>=
data(hdv2003)
d <- hdv2003
dim(d)
d1 <- subset(d, select=c("id","age","sexe"))
dim(d1)             
d2 <- subset(d, select=c("id","clso"))
dim(d2)             

@ 

On a donc deux tableaux de données, \texttt{d1} et \texttt{d2},
comportant chacun 2000 lignes et respectivement 3 et 2
colonnes. Comment les rassembler pour n'en former qu'un~?

Intuitivement, cela paraît simple. Il suffit de <<~coller~>>
\texttt{d2} à la droite de \texttt{d1}, comme dans l'exemple suivant.

\begin{center}
  \begin{tabular}{ccccc}
    \begin{tabular}{r|r|r}
      \textsf{Id} & \textsf{V1} & \textsf{V2} \\
      \hline
      1 & H & 12 \\
      2 & H & 17 \\
      3 & F & 41 \\
      4 & F & 9 \\
      \vdots & \vdots & \vdots \\
    \end{tabular}
    &
    \huge{+}
    &
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V3}  \\
      \hline
      1 & Rouge \\
      2 & Bleu \\
      3 & Bleu\\
      4 & Rouge \\
      \vdots & \vdots  \\
    \end{tabular}
    &
    \huge{=}
    &
    \begin{tabular}{r|r|r|r}
      \textsf{Id} & \textsf{V1} & \textsf{V2} & \textsf{V3}\\
      \hline
      1 & H & 12 & Rouge\\
      2 & H & 17 & Bleu \\
      3 & F & 41 & Bleu\\
      4 & F & 9  & Rouge \\
      \vdots & \vdots & \vdots & \vdots \\
    \end{tabular}
    \\
  \end{tabular}
\end{center}

Cela semble fonctionner. La fonction qui permet d'effectuer cette
opération sous \R s'appelle \rfunc{cbind}, elle <<~colle~>> des
tableaux côte à côte en regroupant leurs
colonnes\footnote{L'équivalent de \rfunc{cbind} pour les lignes
  s'appelle \rfunc{rbind}.}.

 
<<>>=
cbind(d1,d2)
@ 

À part le fait qu'on a une colonne \texttt{id} en double, le résultat
semble satisfaisant. À première vue seulement. Imaginons maintenant
que nous avons travaillé sur \texttt{d1} et \texttt{d2}, et que nous
avons ordonné les lignes de \texttt{d1} selon l'âge des enquêtés~:

 
<<>>=
d1 <- d1[order(d1$age),]
@ 

Répétons l'opération de collage~:

 
<<>>=
cbind(d1,d2)
@ 

Que constate-t-on~? La présence de la variable \texttt{id} en double
nous permet de voir que les identifiants ne coïncident plus~! En
regroupant nos colonnes nous avons donc attribué à des individus les
réponses d'autres individus.

La commande \rfunc{cbind} ne peut en effet fonctionner que si les deux
tableaux ont exactement le même nombre de lignes, et dans le même
ordre, ce qui n'est pas le cas ici.

On va donc être obligé de pocéder à une \textit{fusion} des deux
tableaux, qui va permettre de rendre à chaque ligne ce qui lui
appartient. Pour cela nous avons besoin d'un identifiant qui permet
d'identifier chaque ligne de manière unique et qui doit être présent
dans tous les tableaux. Dans notre cas, c'est plutôt rapide, il s'agit
de la variable \texttt{id}.

Une fois l'identifiant identifié\footnote{Si vous me passez
  l'expression\ldots}, on peut utiliser la commande
\rfunc{merge}. Celle-ci va fusionner les deux tableaux en supprimant
les colonnes en double et en regroupant les lignes selon leurs identifiants~:


<<dcomplet>>=
d.complet <- merge(d1,d2, by="id")
head(d.complet)
@ 

Ici l'utilisation de la fonction est plutôt simple car nous sommes
dans le cas de figure idéal~: les lignes correspondent parfaitement et
l'identifiant est clairement identifié. Parfois les choses peuvent
être un peu plus compliquées~:

\begin{itemize}
\item parfois les identifiants n'ont pas le même nom dans les deux
  tableaux. On peut alors les spécifier par les options \texttt{by.x}
  et \texttt{by.y}~;
\item parfois les deux tableaux comportent des colonnes (hors
  identifiants) ayant le même nom. \rfunc{merge} conserve dans ce
  cas ces deux colonnes mais les renomme en les suffixant par
  \texttt{.x} pour celles provenant du premier tableau, et \texttt{.y}
  pour celles du second~;
\item parfois on n'a pas d'identifiant unique préétabli, mais on en
  construit un à partir de plusieurs variables. On peut alors donner
  un vecteur en paramètres de l'option \texttt{by}, par exemple
  \texttt{by=c("nom","prenom","date.naissance")}.
\end{itemize}

Une subtilité supplémentaire intervient lorsque les deux tableaux
fusionnés n'ont pas exactement les mêmes lignes. Par défaut,
\rfunc{merge} ne conserve que les lignes présentes dans les deux tableaux~:

\begin{center}
  \begin{tabular}{ccccc}
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V1}  \\
      \hline
      1 & H  \\
      2 & H  \\
      3 & F  \\
    \end{tabular}
    &
    \huge{+}
    &
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V2}  \\
      \hline
      1 & 10 \\
      2 & 15 \\
      5 & 31 \\
    \end{tabular}
    &
    \huge{=}
    &
    \begin{tabular}{r|r|r}
      \textsf{Id} & \textsf{V1} & \textsf{V2} \\
      \hline
      1 & H & 10\\
      2 & H & 15\\
    \end{tabular}
    \\
  \end{tabular}
\end{center}

On peut cependant modifier ce comportement avec les options
\texttt{all.x=TRUE} et \texttt{all.y=TRUE}. La première option indique
de conserver toutes les lignes du premier tableau. Dans ce cas
\rfunc{merge} donne une valeur \texttt{NA} pour ces lignes aux
colonnes provenant du second tableau. Ce qui donnerait~:

\begin{center}
  \begin{tabular}{ccccc}
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V1}  \\
      \hline
      1 & H  \\
      2 & H  \\
      3 & F  \\
    \end{tabular}
    &
    \huge{+}
    &
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V2}  \\
      \hline
      1 & 10 \\
      2 & 15 \\
      5 & 31 \\
    \end{tabular}
    &
    \huge{=}
    &
    \begin{tabular}{r|r|r}
      \textsf{Id} & \textsf{V1} & \textsf{V2} \\
      \hline
      1 & H & 10\\
      2 & H & 15\\
      3 & F & NA\\
    \end{tabular}
    \\
  \end{tabular}
\end{center}

\texttt{all.y} fait la même chose en conservant toutes les lignes du
second tableau. On peut enfin décider toutes les lignes des deux
tableaux en utilisant à la fois \texttt{all.x=TRUE} et
\texttt{all.y=TRUE}, ce qui donne~:

\begin{center}
  \begin{tabular}{ccccc}
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V1}  \\
      \hline
      1 & H  \\
      2 & H  \\
      3 & F  \\
    \end{tabular}
    &
    \huge{+}
    &
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V2}  \\
      \hline
      1 & 10 \\
      2 & 15 \\
      5 & 31 \\
    \end{tabular}
    &
    \huge{=}
    &
    \begin{tabular}{r|r|r}
      \textsf{Id} & \textsf{V1} & \textsf{V2} \\
      \hline
      1 & H & 10\\
      2 & H & 15\\
      3 & F & NA\\
      5 & NA & 31 \\
    \end{tabular}
    \\
  \end{tabular}
\end{center}

Parfois, l'un des identifiants est présent à plusieurs reprises dans
l'un des tableaux (par exemple lorsque l'une des tables est un
ensemble de ménages et que l'autre décrit l'ensemble des individus de
ces ménages). Dans ce cas les lignes de l'autre table sont dupliquées
autant de fois que nécessaires~:

\begin{center}
  \begin{tabular}{ccccc}
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V1}  \\
      \hline
      1 & H  \\
      2 & H  \\
      3 & F  \\
    \end{tabular}
    &
    \huge{+}
    &
    \begin{tabular}{r|r}
      \textsf{Id} & \textsf{V2}  \\
      \hline
      1 & 10 \\
      1 & 18 \\
      1 & 21 \\
      2 & 11 \\
      3 & 31 \\
    \end{tabular}
    &
    \huge{=}
    &
    \begin{tabular}{r|r|r}
      \textsf{Id} & \textsf{V1} & \textsf{V2} \\
      \hline
      1 & H & 10\\
      1 & H & 18\\
      1 & H & 21\\
      2 & H & 11\\
      3 & F & 31\\
    \end{tabular}
    \\
  \end{tabular}
\end{center}



\section{Organiser ses scripts}

Il ne s'agit pas ici de manipulation de données à proprement parler,
mais plutôt d'une conséquence de ce qui a été vu précédemment~: à
mesure que recodages et traitements divers s'accumulent, votre script 
\R risque de devenir rapidement très long et pas très pratique à éditer.

Il est très courant de répartir son travail entre différents fichiers,
ce qui est rendu très simple par la fonction \rfunc{source}. Celle-ci
permet de lire le contenu d'un fichier de script et d'exécuter son
contenu.

Prenons tout de suite un exemple. La plupart des scripts \R commencent
par charger les extensions utiles, par définir le répertoire de
travail à l'aide de \rfunc{setwd}, à importer les données, à effectuer
manipulations, traitements et recodages, puis à mettre en oeuvre les
analyses. Prenons le fichier fictif suivant~:

<<orga1,eval=FALSE,prompt=FALSE>>=
library(questionr)
library(foreign)

setwd("/home/julien/r/projet")

## IMPORT DES DONNÉES

d1 <- read.dbf("tab1.dbf")
d2 <- read.dbf("tab2.dbf")

d <- merge(d1, d2, by="id")

## RECODAGES

d$tx.chomage <- as.numeric(d$tx.chomage)

d$pcs[d$pcs == "Ouvrier qualifie"] <- "Ouvrier"
d$pcs[d$pcs == "Ouvrier specialise"] <- "Ouvrier"

d$age5cl <- cut(d$age, 5)

## ANALYSES

tab <- table(d$tx.chomage, d$age5cl)
tab
chisq.test(tab)
@


Une manière d'organiser notre script\footnote{Ceci n'est qu'une
  suggestion, la manière d'organiser (ou non) son travail étant bien
  évidemment très hautement subjective.} pourrait être de placer les
opérations d'import des données et celles de recodage dans deux
fichiers scripts séparés. Créons alors un fichier nommé
\texttt{import.R} dans notre répertoire de travail et copions les
lignes suivantes~:


<<orga2,eval=FALSE,prompt=FALSE>>=  
## IMPORT DES DONNÉES

d1 <- read.dbf("tab1.dbf")
d2 <- read.dbf("tab2.dbf")

d <- merge(d1, d2, by="id")
@ 


Créons également un fichier \texttt{recodages.R} avec le contenu suivant~:

<<orga3,eval=FALSE,prompt=FALSE>>=  
## RECODAGES

d$tx.chomage <- as.numeric(d$tx.chomage)

d$pcs[d$pcs == "Ouvrier qualifie"] <- "Ouvrier"
d$pcs[d$pcs == "Ouvrier specialise"] <- "Ouvrier"

d$age5cl <- cut(d$age, 5)
@ 


Dès lors, si nous rajoutons les appels à la fonction \rfunc{source}
qui vont bien, le fichier suivant sera strictement équivalent à notre
fichier de départ~:


<<orga4,eval=FALSE,prompt=FALSE>>=  
library(questionr)
library(foreign)

setwd("/home/julien/r/projet")

source("import.R")
source("recodages.R")

## ANALYSES

tab <- table(d$tx.chomage, d$age5cl)
tab
chisq.test(tab)
@


Au fur et à mesure du travail sur les données, on placera les
recodages que l'on souhaite conserver dans le fichier
\texttt{recodages.R}.

Cette méthode présente plusieurs avantages~:

\begin{itemize}
\item bien souvent, lorsqu'on effectue des recodages on se retrouve
  avec des variables recodées qu'on ne souhaite pas conserver. Si on
  prend l'habitude de placer les recodages intéressants dans le
  fichier \texttt{recodages.R}, alors il suffit d'exécuter les cinq
  premières lignes du fichier pour se retrouver avec un tableau de
  données \texttt{d} propre et complet.
\item on peut répartir ses analyses dans différents scripts. Il suffit
  alors de copier les cinq premières lignes du fichier précédent dans
  chacun des scripts, et on aura l'assurance de travailler sur
  exactement les mêmes données.
\end{itemize}

Le premier point illustre l'une des caractéristiques de \R~: il est
rare que l'on stocke les données modifiées. En général on repart
toujours du fichier source original, et les recodages sont conservés
sous forme de scripts et recalculés à chaque fois qu'on recommence à
travailler. Ceci offre une traçabilité parfaite du traitement effectué
sur les données.


\section{Exercices}


\begin{exo}{manip_rename}
  Renommer la variable \texttt{clso} du jeu de données
  \texttt{hdv2003} en \texttt{classes.sociales}, puis la renommer en
  \texttt{clso}.
\end{exo}
  

\begin{exo}{manip_factor}
  Réordonner les niveaux du facteur \texttt{clso} pour que son tri
  à plat s'affiche de la manière suivante~:
  
<<echo=FALSE>>=
tmp <- factor(d$clso, levels=c("Non", "Ne sait pas", "Oui"))
table(tmp)              
@ 
\end{exo}


\begin{exo}{manip_index_direct}
  Affichez~:
  \begin{itemize}
  \item les 3 premiers éléments de la variable \texttt{cinema}
  \item les éléments 12 à 30 de la variable \texttt{lecture.bd}
  \item les colonnes 4 et 8 des lignes 5 et 12 du jeu de données \texttt{hdv2003}
  \item les 4 derniers éléments de la variable \texttt{age}
  \end{itemize}
\end{exo}

\begin{exo}{manip_souspop}
  Construisez les sous-tableaux suivants avec la fonction \texttt{subset}~:
  \begin{itemize}
  \item âge et sexe des lecteurs de BD
  \item ensemble des personnes n'étant pas chômeur (variable
    \texttt{occup}), sans la variable \texttt{cinema}
  \item identifiants des personnes de plus de 45 ans écoutant du hard rock
  \item femmes entre 25 et 40 ans n'ayant pas fait de sport dans les
    douze derniers mois
  \item hommes ayant entre 2 et 4 frères et s\oe{}urs et faisant la
    cuisine ou du bricolage
  \end{itemize}
\end{exo}

\begin{exo}{manip_tapply}
  Calculez le nombre moyen d'heures passées devant la télévision chez
  les lecteurs de BD, d'abord en construisant les sous-populations,
  puis avec la fonction \texttt{tapply}.
\end{exo}


\begin{exo}{manip_convert}
  Convertissez la variable \texttt{freres.soeurs} en variable de type
  caractères. Convertissez cette nouvelle variable en facteur. Puis
  convertissez à nouveau ce facteur en variable numérique. Vérifiez
  que votre variable finale est identique à la variable de départ.
\end{exo}


\begin{exo}{manip_decoup}
  Découpez la variable \texttt{freres.soeurs}~:
  \begin{itemize}
  \item en cinq classes d'amplitude égale
  \item en catégories <<~de 0 à 2~>>, <<~de 2 à 4~>>, <<~plus de 4~>>, avec les
    étiquettes correspondantes
  \item en quatre classes d'effectif équivalent
  \item d'où vient la différence d'effectifs entre les deux découpages
    précédents~?
  \end{itemize}
\end{exo}

\begin{exo}{manip_regroup}
  Recodez la variable \texttt{trav.imp} en \texttt{trav.imp2cl} pour
  obtenir les modalités <<~Le plus ou aussi important~>> et <<~moins
  ou peu important~>>. Vérifiez avec des tris à plat et un tableau croisé.
  
  Recodez la variable \texttt{relig} en \texttt{relig.4cl} en
  regroupant les modalités <<~Pratiquant regulier~>> et <<~Pratiquant
  occasionnel~>> en une seule modalité <<~Pratiquant~>>, et en
  remplaçant la modalité <<~NSP ou NVPR~>> par des valeurs
  manquantes. Vérifiez avec un tri croisé.
\end{exo}

\begin{exo}{manip_combine}
  Créez une variable ayant les modalités suivantes~:
  \begin{itemize}
  \item Homme de plus de 40 ans lecteur de BD
  \item Homme de plus de 30 ans
  \item Femme faisant du bricolage
  \item Autre
  \end{itemize}
  Vérifier avec des tris croisés.
\end{exo}

\begin{exo}{manip_tri}
  Ordonner le tableau de données selon le nombre de frères et soeurs
  croissant.  Afficher le sexe des 10 individus regardant le plus la
  télévision.
\end{exo}






\chapter{Statistique bivariée}

On entend par statistique bivariée l'étude des relations entre deux
variables, celles-ci pouvant être quantitatives ou qualitatives.

Comme dans la partie précédente, on travaillera sur les jeux de données
fournis avec l'extension \questionr \marqr et tiré de l'enquête
\textit{Histoire de vie} et du recensement 1999~:

<<>>=
data(hdv2003)
d <- hdv2003
data(rp99)
@ 


\section{Deux variables quantitatives}
\label{deux_var_quant}

La comparaison de deux variables quantitatives se fait en premier lieu
graphiquement, en représentant l'ensemble des couples de valeurs. On
peut ainsi représenter les valeurs du nombre d'heures passées devant
la télévision selon l'âge  (figure~\ref{fig_tvage} \vpageref{fig_tvage}).

\begin{figure}
<<>>=
plot(d$age, d$heures.tv)
@ 
\caption{Nombre d'heures de télévision selon l'âge}
\label{fig_tvage}
\end{figure}


Le fait que des points sont superposés ne facilite pas la lecture du
graphique. On peut utiliser une représentation avec des points
semi-transparents  (figure~\ref{fig_tvagealpha} \vpageref{fig_tvagealpha}).

\begin{figure}
<<>>=
plot(d$age, d$heures.tv, pch=19, col=rgb(1,0,0,0.1))
@ 
\caption{Nombre d'heures de télévision selon l'âge avec semi-transparence}
\label{fig_tvagealpha}
\end{figure}


Plus sophistiqué, on peut faire une estimation locale de densité et
représenter le résultat sous forme de <<~carte~>>. Pour cela on
commence par isoler les deux variables, supprimer les observations
ayant au moins une valeur manquante à l'aide de la fonction
\rfunc{complete.cases}, estimer la densité locale à l'aide de la
fonction \rfunc{kde2d} de l'extension
\textsf{MASS}\footnote{\textsf{MASS} est installée par défaut avec la
  version de base de \R.} et représenter le tout à l'aide d'une des
fonctions \rfunc{image}, \rfunc{contour} ou
\rfunc{filled.contour}\ldots Le résultat est donné
figure~\ref{fig_filledcontour} \vpageref{fig_filledcontour}.

\begin{figure}
<<filled-contour,fig.keep="last">>=
library(MASS)
tmp <- d[,c("age","heures.tv")]
tmp <- tmp[complete.cases(tmp),]
filled.contour(kde2d(tmp$age,tmp$heures.tv),color=terrain.colors)
@ 
\caption{Représentation de l'estimation de densité locale}
\label{fig_filledcontour}
\end{figure}


Dans tous les cas, il n'y a pas de structure très nette qui semble se
dégager. On peut tester ceci mathématiquement en calculant le
c\oe{}fficient de corrélation entre les deux variables à l'aide de la
fonction \rfunc{cor}~:

<<>>=
cor(d$age, d$heures.tv, use="complete.obs")
@ 

L'option \texttt{use} permet d'éliminer les observations pour
lesquelles l'une des deux valeurs est manquante. Le c\oe{}fficient de
corrélation est très faible.

On va donc s'intéresser plutôt à deux variables présentes dans le jeu
de données \texttt{rp99}, la part de diplômés du supérieur et la
proportion de cadres dans les communes du Rhône en 1999.

À nouveau, commençons par représenter les deux variables
(figure~\ref{fig_cadresup} \vpageref{fig_cadresup}). Ça ressemble déjà
beaucoup plus à une relation de type linéaire.

\begin{figure}
<<>>=
plot(rp99$dipl.sup, rp99$cadres,  ylab="Part des cadres", xlab="Part des diplomês du supérieur")
@ 
\caption{Proportion de cadres et proportion de diplômés du supérieur}
\label{fig_cadresup}
\end{figure}

Calculons le coefficient de corrélation~:

<<>>=
cor(rp99$dipl.sup, rp99$cadres)
@ 

C'est beaucoup plus proche de 1. On peut alors effectuer une
régression linéaire complète en utilisant la fonction \rfunc{lm}~:

<<>>=
reg <- lm(cadres ~ dipl.sup, data=rp99)
summary(reg)
@ 


Le résultat montre que les c\oe{}fficients sont significativement
différents de 0. La part de cadres augmente donc avec celle de
diplômés du supérieur (ô surprise). On peut très facilement
représenter la droite de régression à l'aide de la fonction
\rfunc{abline} (figure~\ref{fig_regcad} \vpageref{fig_regcad}).

\begin{figure}
<<>>=
plot(rp99$dipl.sup, rp99$cadres, ylab="Part des cadres", xlab="Part des diplômés du supérieur")
abline(reg, col="red")
@ 
\caption{Régression de la proportion de cadres par celle de diplômés
  du supérieur}
\label{fig_regcad}
\end{figure}

\label{astuce_formules}
\begin{astuce}
  On remarquera que le premier argument passé à la fonction \rfunc{lm} a une 
  syntaxe un peu particulière. Il s'agit d'une \textit{formule}, utilisée de
  manière générale dans les modèles statistiques. On indique la variable 
  d'intérêt à gauche et la variable explicative à droite, les deux étant 
  séparées par un tilde \~{} (obtenu sous \textsf{Windows} en appuyant 
  simultanément sur les touches \texttt{<Alt Gr>} et \texttt{<2>}). On 
  remarquera que les noms des colonnes de notre tableau de données ont été 
  écrites sans guillemets. Dans le cas présent, nous avons calculé une 
  régression linéaire simple entre deux variables, d'où l'écriture 
  \texttt{cadres \~{} dipl.sup}. Si nous avions
  voulu expliquer une variable \texttt{z} par deux variables \texttt{x} et 
  \texttt{y}, nous aurions écrit \texttt{z \~{} x + y}. Il est possible de 
  spécifier des modèles encore plus complexes. Pour un aperçu de la syntaxe des
  formules sous \R, voir 
  \url{http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html}.
\end{astuce}

\section{Une variable quantitative et une variable qualitative}

Quand on parle de comparaison entre une variable quantitative et une
variable qualitative, on veut en général savoir si la distribution des
valeurs de la variable quantitative est la même selon les modalités de
la variable qualitative. En clair~: est ce que l'âge de ceux qui
écoutent du hard rock est différent de l'âge de ceux qui n'en écoutent
pas~?

Là encore, l'idéal est de commencer par une représentation
graphique. Les boîtes à moustaches sont parfaitement adaptées pour
cela.

Si on a construit des sous-populations d'individus écoutant ou non du
hard rock, on peut utiliser la fonction \rfunc{boxplot} comme indiqué
figure~\ref{fig_boxplotage} \vpageref{fig_boxplotage}.


\begin{figure}
<<>>=
d.hard <- subset(d, hard.rock=="Oui")
d.non.hard <- subset(d, hard.rock=="Non")
boxplot(d.hard$age, d.non.hard$age)
@ 
\caption{\textit{Boxplot} de la répartition des âges (sous-populations)}
\label{fig_boxplotage}
\end{figure}

Mais construire les sous-populations n'est pas nécessaire. On peut
utiliser directement la version de \rfunc{boxplot} prenant une
\textit{formule} en argument (figure~\ref{fig_boxplotageb}
\vpageref{fig_boxplotageb}).

\begin{figure}
<<>>=
boxplot(age ~ hard.rock, data=d)
@ 
\caption{\textit{Boxplot} de la répartition des âges (formule)}
\label{fig_boxplotageb}
\end{figure}

À première vue, ô surprise, la population écoutant du hard rock a
l'air sensiblement plus jeune. Peut-on le tester mathématiquement~? On
peut calculer la moyenne d'âge des deux groupes en utilisant la
fonction \rfunc{tapply}\footnote{Fonction décrite
  \vpageref{sec_tapply}.}~:

<<>>=
tapply(d$age, d$hard.rock, mean)
@ 

L'écart est très important. Est-il statistiquement significatif~? Pour
cela on peut faire un test \textit{t} de comparaison de moyennes à
l'aide de la fonction \rfunc{t.test}~:

<<>>=
t.test(d$age ~ d$hard.rock)
@ 

Le test est extrêmement significatif. L'intervalle de confiance à
95~\% de la différence entre les deux moyennes va de 14,5 ans à 21,8 ans.

\begin{important}
  La valeur affichée pour \textit{p} est de \texttt{1.611e-07}. Cette valeur 
  peut paraître étrange pour les non avertis. Cela signifie tout simplement
  1,611 multiplié par 10 à la puissance -7, autrement dit 0,0000001611. Cette
  manière de représenter un nombre est couramment appelée \textit{notation
  scientifique}. Voir aussi \url{http://fr.wikipedia.org/wiki/Notation_scientifique}
\end{important}

Nous sommes cependant allés un peu vite en besogne, car nous avons
négligé une hypothèse fondamentale du test \textit{t}~: les ensembles
de valeur comparés doivent suivre approximativement une loi normale et
être de même variance\footnote{Concernant cette seconde condition, \R
  propose une option nommée \texttt{var.equal} qui permet d'utiliser
  une approximation dans le cas où les variances ne sont pas
  égales}. Comment le vérifier~?

D'abord avec un petit graphique, comme sur la
figure~\ref{fig_histnorm} \vpageref{fig_histnorm}.

\begin{figure}
<<fig.width=10>>=
par(mfrow=c(1,2))
hist(d$age[d$hard.rock=="Oui"], main="Hard rock", col="red")
hist(d$age[d$hard.rock=="Non"], main="Sans hard rock", col="red")
@ 
\caption{Distribution des âges pour appréciation de la normalité}
\label{fig_histnorm}
\end{figure}


Ça a l'air à peu près bon pour les <<~Sans hard rock~>>, mais un peu
plus limite pour les fans de \textit{Metallica}, dont les effectifs
sont d'ailleurs assez faibles. Si on veut en avoir le c\oe{}ur net on
peut utiliser le test de normalité de Shapiro-Wilk avec la fonction
\rfunc{shapiro.test}~:

<<>>=
shapiro.test(d$age[d$hard.rock=="Oui"])
shapiro.test(d$age[d$hard.rock=="Non"])
@ 

Visiblement, le test estime que les distributions ne sont pas
suffisamment proches de la normalité dans les deux cas.

Et concernant l'égalité des variances~? 

<<>>=
tapply(d$age, d$hard.rock, var)
@ 

L'écart n'a pas l'air négligeable. On peut le vérifier avec le test
fourni par la fonction \rfunc{var.test}~:

<<>>=
var.test(d$age ~ d$hard.rock)
@ 

La différence est très significative. En toute rigueur le test
\textit{t} n'aurait donc pas pu être utilisé.

\textit{Damned~!} Ces maudits tests statistiques vont-ils nous
empêcher de faire connaître au monde entier notre fabuleuse découverte
sur l'âge des fans de \textit{Sepultura}~? Non~! Car voici qu'approche
à l'horizon un nouveau test, connu sous le nom de
\textit{Wilcoxon/Mann-Whitney}. Celui-ci a l'avantage d'être
\textit{non-paramétrique}, c'est à dire de ne faire aucune hypothèse
sur la distribution des échantillons comparés. Par contre il ne
compare pas des différences de moyennes mais des différences de
médianes~:
<<>>=
wilcox.test(d$age ~ d$hard.rock)
@ 

Ouf~! La différence est hautement significative\footnote{Ce test peut
  également fournir un intervalle de confiance avec l'option
  \texttt{conf.int=TRUE}.}. Nous allons donc pouvoir entamer la
rédaction de notre article pour la \textit{Revue française de
  sociologie}.


\section{Deux variables qualitatives}

La comparaison de deux variables qualitatives s'appelle en général un
\textit{tableau croisé}. C'est sans doute l'une des analyses les plus
fréquentes lors du traitement d'enquêtes en sciences sociales.

\subsection{Tableau croisé}

La manière la plus simple d'obtenir un tableau croisé est d'utiliser
la fonction \rfunc{table} en lui donnant en paramètres les deux
variables à croiser. En l'occurrence nous allons croiser un recodage
du niveau de qualification regroupé avec le fait de pratiquer un
sport.

On commence par calculer la variable recodée et par afficher le tri à
plat des deux variables~:

<<>>=
d$qualreg <- as.character(d$qualif)
d$qualreg[d$qualif %in% c("Ouvrier specialise", "Ouvrier qualifie")] <- "Ouvrier"
d$qualreg[d$qualif %in% c("Profession intermediaire", "Technicien")] <- "Intermediaire"
table(d$qualreg)
table(d$sport)
@ 

Le tableau croisé des deux variables s'obtient de la manière
suivante~:

<<>>=
table(d$sport, d$qualreg)
@ 

\begin{astuce}
  Il est tout à fait possible de croiser trois variables ou plus. Par exemple :
  
  <<>>=
  table(d$sport,d$cuisine,d$sexe)
  @ 
\end{astuce}

On n'a cependant que les effectifs, ce qui rend difficile les
comparaisons. L'extension \questionr \marqr fournit des fonctions
permettant de calculer les pourcentages lignes, colonnes et totaux
d'un tableau croisé. 

Les pourcentages lignes s'obtiennent avec la fonction
\rfunc{lprop}. Celle-ci s'applique au tableau croisé généré par
\rfunc{table}~:

<<>>=
tab <- table(d$sport, d$qualreg)
lprop(tab)
@ 

Les pourcentages ligne ne nous intéressent guère ici. On ne cherche
pas à voir quelle est la proportion de cadres parmi ceux qui pratiquent
un sport, mais plutôt quelle est la proportion de sportifs chez les
cadres. Il nous faut donc des pourcentages colonnes, que l'on obtient
avec la fonction \rfunc{cprop}~:

<<>>=
cprop(tab)
@ 

Dans l'ensemble, le pourcentage de personnes ayant pratiqué un sport
est de 35,6~\%. Mais cette proportion varie fortement d'une catégorie
professionnelle à l'autre~: 55,0~\% chez les cadres contre 23,0~\%
chez les ouvriers.

À noter qu'on peut personnaliser l'affichage de ces tableaux de
pourcentages à l'aide de différentes options, dont \texttt{digits},
qui règle le nombre de décimales à afficher, et \texttt{percent}, qui
indique si on souhaite ou non rajouter un symbole \% dans chaque case
du tableau. Cette personnalisation peut se faire directement au moment
de la génération du tableau, et dans ce cas elle sera utilisée par défaut~:

<<>>=
ctab <- cprop(tab, digits=2, percent=TRUE)
ctab
@ 

Ou bien ponctuellement en passant les mêmes arguments aux fonctions
\rfunc{print} (pour affichage dans \R) ou \rfunc{copy} (pour export
vers un logiciel externe)~:

<<>>=
ctab <- cprop(tab)
print(ctab, percent=TRUE)
@ 


\subsection{\texorpdfstring{$\chi^2$}{X\texttwosuperior} et dérivés}

Pour tester l'existence d'un lien entre les modalités des deux
variables, on va utiliser le très classique test du
$\chi^2$\footnote{On ne donnera pas plus d'indications sur le test du
  $\chi^2$ ici. Les personnes désirant une présentation plus détaillée
  pourront se reporter (attention, séance d'autopromotion~!) à la page
  suivante~:
  \url{http://alea.fr.eu.org/pages/khi2}.}. Celui-ci
s'obtient grâce à la fonction \rfunc{chisq.test}, appliquée au
tableau croisé obtenu avec \rfunc{table}\footnote{On peut aussi
  appliquer directement le test en spécifiant les deux variables à
  croiser \textit{via} \texttt{chisq.test(d\$qualreg, d\$sport)}}~:

<<>>=
chisq.test(tab)
@ 

Le test est hautement significatif, on ne peut pas considérer qu'il y
a indépendance entre les lignes et les colonnes du tableau. 

On peut affiner l'interprétation du test en déterminant dans quelle
case l'écart à l'indépendance est le plus significatif en utilisant
les \textit{résidus} du test. Ceux-ci sont notamment affichables avec
la fonction \rfunc{chisq.residuals} \marqr de \questionr~:

<<>>=
chisq.residuals(tab)
@ 

Les cases pour lesquelles l'écart à l'indépendance est significatif
ont un résidu dont la valeur est supérieure à 2 ou inférieure à
-2. Ici on constate que la pratique d'un sport est sur-représentée
parmi les cadres et, à un niveau un peu moindre, parmi les professions
intermédiaires, tandis qu'elle est sous-représentée chez les ouvriers.

Enfin, on peut calculer le c\oe{}fficient de contingence de Cramer du
tableau, qui peut nous permettre de le comparer par la suite à
d'autres tableaux croisés. On peut pour cela utiliser la fonction
\rfunc{cramer.v}\marqr de \questionr~:

<<>>=
cramer.v(tab)
@ 


\begin{astuce}
  Pour un tableau à 2x2 entrées, il est possible de calculer le test exact de
  Fisher avec la fonction \rfunc{fisher.test}. On peut soit lui passer le 
  résultat de \rfunc{table}, soit directement les deux variables à croiser.
  
  <<>>=
lprop(table(d$sexe,d$cuisine))
fisher.test(table(d$sexe,d$cuisine))
@ 
\end{astuce}

\subsection{Représentation graphique}

Enfin, on peut obtenir une représentation graphique synthétisant
l'ensemble des résultats obtenus sous la forme d'un graphique en
mosaïque, grâce à la fonction \rfunc{mosaicplot}. Le résultat est
indiqué figure~\ref{fig_mosaicplot} \vpageref{fig_mosaicplot}.

\begin{figure}
<<>>=
mosaicplot(qualreg ~ sport, data=d, shade=TRUE, main="Graphe en mosaïque")
@ 
\caption{Exemple de graphe en mosaïque}
\label{fig_mosaicplot}
\end{figure}


Comment interpréter ce graphique haut en couleurs\footnote{Sauf s'il
  est imprimé en noir et blanc\ldots}~? Chaque rectangle
représente une case de tableau. Sa largeur correspond au pourcentage
des modalités en colonnes (il y'a beaucoup d'employés et d'ouvriers et
très peu d'<<~autres~>>). Sa hauteur correspond aux
pourcentages-colonnes~: la proportion de sportifs chez les cadres est
plus élevée que chez les employés. Enfin, la couleur de la case
correspond au résidu du test du $\chi^2$ correspondant~: les cases en
rouge sont sous-représentées, les cases en bleu sur-représentées, et
les cases blanches sont statistiquement proches de l'hypothèse
d'indépendance.


\begin{figure}
<<>>=
barplot(cprop(tab,total=FALSE),
		main="Pratique du sport selon le niveau de qualification")
@ 
\caption{Exemple de barres cumulées }
\label{fig_barplot_cum}
\end{figure}

Lorsque l'on s'intéresse principalement aux variations d'une variable selon une
autre, par exemple ici à la pratique du sport selon le niveau de qualification,
il peut être intéressant de présenter les pourcentages en colonne sous la forme
de barres cumulées. Voir figure~\ref{fig_barplot_cum} \vpageref{fig_barplot_cum}.




\chapter{Régression logistique}
\label{chapitre_reglog}

La régression logistique est fréquemment utilisée en sciences sociales
car elle permet d'effectuer un raisonnement dit \textit{toutes choses étant égales
par ailleurs}. Plus précisément, la régression logistique a pour but d'isoler les
effets de chaque variable, c'est-à-dire d'identifier les effets 
résiduels d'une \textit{variable explicative}
sur une \textit{variable d'intérêt}, une fois pris en compte les autres variables 
explicatives introduites dans le modèle. La régression logistique est ainsi 
prisée en épidémiologie pour identifier les facteurs associés 
à telle ou telle pathologie.

La régression logistique ordinaire ou régression logistique binaire vise à 
expliquer une variable d'intérêt binaire (c'est-à-dire de type <<~Oui/Non~>>). Les
variables explicatives qui seront introduites dans le modèle peuvent être 
quantitatives ou qualitatives.

\section{Préparation des données}

Dans ce chapite, nous allons encore une fois utiliser les données de l'enquête
\textit{Histoire de vie}, fournies avec l'extension \questionr et
décrites dans l'annexe~\ref{sec_hdv2003}, page~\pageref{sec_hdv2003}.

<<eval=FALSE>>=
library(questionr)
data(hdv2003)
d <- hdv2003
@ 

À titre d'exemple, nous allons étudier l'effet de l'âge, du sexe, du niveau 
d'étude, de la pratique religieuse et du nombre moyen d'heures passées à 
regarder la télévision par jour.

En premier lieu, il importe de vérifier que notre variable d'intérêt 
(ici \texttt{sport}) est correctement codée. Une possibilité consiste à
créer une variable booléenne(vrai / faux) selon que l'individu a pratiqué du 
sport ou non~:

<<>>=
d$sport2 <- FALSE
d$sport2[d$sport=="Oui"] <- TRUE
@

Dans le cas présent, cette variable n'a pas de valeur manquante. Mais le cas 
échéant il faut bien renseigner \texttt{NA} pour les valeurs manquantes, les
individus en question étant alors exclu de l'analyse.

Il n'est pas forcément nécessaire de transformer notre variable d'intérêt en
variable booléenne. En effet, \R accepte sans problème une variable de type 
facteur. Cependant, l'ordre des valeurs d'un facteur a de l'importance. En effet,
\R considère toujours la première modalité comme étant la \textit{modalité de référence}. 
Dans le cas de la variable d'intérêt, la modalité de référence correspond au fait
de ne pas remplir le critère étudié, dans notre exemple au fait de ne pas avoir
eu d'activité sportive au cours des douze derniers mois. Pour connaître l'ordre
des modalités d'une variable de type facteur, on peut utiliser la fonction 
\rfunc{levels} ou bien encore tout simplement la fonction \rfunc{freq}~:

<<>>=
levels(d$sport)
freq(d$sport)
@

Dans notre exemple, la modalité <<~Non~>> est déjà la première modalité. Il n'y 
a donc pas besoin de modifier notre variable. Si ce n'est pas le cas, il faudra
modifier la modalité de référence avec la fonction \rfunc{relevel} comme nous 
allons le voir un peu plus loin.

\begin{important}
  Il est possible d'indiquer un facteur à plus de deux modalités. Dans une telle situation, 
  \R considérera que tous les modalités, sauf la modalité de référence, est une
  réalisation de la variable d'intérêt. Cela serait correct par exemple notre
  variable \texttt{sport} était codée ainsi~: <<~Non~>>, <<~Oui, toutes les 
  semaines~>>, <<~Oui, au moins une fois par mois~>>, <<~Oui, moins d'une fois
  par mois~>>. Cependant, afin d'éviter tout risque d'erreur ou de mauvaise 
  interprétation, il est vivement conseillé de recoder au préalable sa variable
  d'intérêt en un facteur à deux modalités.
\end{important}

La notion de modalité de référence s'applique également aux variables explicatives
qualitatives. En effet, dans un modèle, tous les coefficients sont calculés par 
rapport à la modalité de référence. Il importe de choisir une modalité de référence
qui fasse sens afin de faciliter l'interprétation. Par ailleurs, ce choix peut 
également dépendre de la manière dont on souhaite présenter les résultats. De manière 
générale on évitera de choisir comme référence une modalité peu représentée dans
l'échantillon ou bien une modalité correspondant à une situation atypique.

Prenons l'exemple de la variable \texttt{sexe}. Souhaite-t-on connaitre l'effet
d'être une femme par rapport au fait d'être un homme ou bien l'effet d'être un
homme par rapport au fait d'être une femme~? Si l'on opte pour le second, alors
notre modalité de référence sera le sexe féminin. Comme est codée cette variable~?

<<>>=
freq(d$sexe)
@

La modalité \texttt{Femme} s'avère ne pas être la première modalité. Nous devons 
appliquer la fonction \rfunc{relevel}~:

<<>>=
d$sexe <- relevel(d$sexe,"Femme")
freq(d$sexe)
@

Les variables \texttt{age} et \texttt{heures.tv} sont des variables quantitatives.
Il importe de vérifier qu'elles sont bien enregistrées en tant que variables
numériques. En effet, il arrive parfois que dans le fichier source les variables
quantitatives soient renseignées sous forme de valeur textuelle et non sous forme
numérique.

<<>>=
str(d$age)
str(d$heures.tv)
@

Nos deux variables sont bien renseignées sous forme numérique.

Cependant, l'effet de l'âge est rarement linéaire. Un exemple trivial est par 
exemple le fait d'occuper un emploi qui sera moins fréquent aux jeunes âges et
aux âges élevés. Dès lors, on pourra transformer la variable <<~âge~>> en
groupe d'âges(voir section \ref{decouper_var_num} \vpageref{decouper_var_num})~:

<<>>=
d$grpage <- cut(d$age,c(16,25,45,65,93),right=FALSE,include.lowest=TRUE)
freq(d$grpage)
@


Jetons maintenant un \oe{}il à la variable \texttt{nivetud}~:

<<>>=
freq(d$nivetud)
@

En premier lieu, cette variable est détaillée en pas moins de huit modalités
dont certaines sont peu représentées (seulement 39 individus soit 2~\% n'ont
jamais fait d'études par exemple). Afin d'améliorier notre modèle logistique, il
peut être pertinent de regrouper certaines modalités (voir section 
\ref{regrouper_modalites} \vpageref{regrouper_modalites})~:

<<>>=
d$etud <- d$nivetud
levels(d$etud) <- c("Primaire","Primaire","Primaire","Secondaire","Secondaire","Technique/Professionnel","Technique/Professionnel","Supérieur")
freq(d$etud)
@

Notre variable comporte également 112 individus avec une valeur manquante. Si nous
conservons cette valeur manquante, ces 112 individus seront, par défaut, exclus
de l'analyse. Ces valeurs manquantes n'étant pas négligeable (5,6~\%), nous pouvons
également faire le choix de considérer ces valeurs manquantes comme une modalité
supplémentaire. Auquel cas, nous utiliserons la fonction \rfunc{addNA}~:

<<>>=
levels(d$etud)
d$etud <- addNA(d$etud)
levels(d$etud)
@

\section{Régression logistique binaire}

La fonction \rfunc{glm} (pour \textit{generalized linear models}) permet de calculer 
une grande variété de modèles statistiques. La régression logistique ordinaire
correspond au modèle \textit{logit} de la famille des modèles binomiaux, ce que
l'on indique à \rfunc{glm} avec l'argument \texttt{family=binomial(logit)}.

Le modèle proprement dit sera renseigné sous la forme d'une \textit{formule} 
(que nous avons déjà rencontrée \vpageref{astuce_formules}). On indiquera d'abord
la variable d'intérêt, suivie du signe \texttt{\~{}} puis de la liste des variables 
explicatives séparées par un signe \texttt{+}. Enfin, l'argument \texttt{data}
permettra d'indiquer notre tableau de données.

<<>>=
reg <- glm(sport ~ sexe + grpage + etud + relig + heures.tv, data=d, family=binomial(logit))
reg
@

\begin{remarque}
  Il est possible de spécifier des modèles plus complexes. Par exemple, 
  \texttt{x:y} permet d'indiquer l'interaction entre les variables \texttt{x} et
  \texttt{y}. \texttt{x * y} sera équivalent à \texttt{x + y + x:y}. Pour aller
  plus loin, voir \url{http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html}.
\end{remarque}

Une présentation plus complète des résultats est obtenue avec \rfunc{summary}~:

<<>>=
summary(reg)
@

Dans le cadre d'un modèle logistique, généralement on ne présente pas les 
coefficients du modèle mais leur valeur exponentielle, cette dernière correspondant
en effet à des \textit{odds ratio}, également appelé \textit{rapport des cotes}.
L'odds ratio diffère du \textit{risque relatif}. Cependent son interprétation est
similaire. Un odds ratio de 1 signifie l'absence d'effet. Un odds ratio largement
supérieur à 1 correspond à une augmentation du phénomène étudié et un odds ratio
largement inféieur à 1 correspond à une diminution du phénomène étudié\footnote{
Pour plus de détails, voir \url{http://www.spc.univ-lyon1.fr/polycop/odds%20ratio.htm}}.

La fonction \rfunc{coef} permet d'obtenir les coefficients d'un modèle, 
\rfunc{confint} leurs intervalles de confiance et 
\rfunc{exp} de calculer l'exponentiel. Les odds ratio et leurs intervalles de
confiance s'obtiennent ainsi~:

<<>>=
exp(coef(reg))
exp(confint(reg))
@

On pourra faciliter la lecture en combinant les deux~:

<<>>=
exp(cbind(coef(reg),confint(reg)))
@

Pour savoir si un odds ratio diffère significativement de 1 (ce qui est identique
au fait que le coefficient soit différent de 0), on pourra se référer à la colonne
\texttt{Pr(>|z|)} obtenue avec \rfunc{summary}.

Il y a également une petite fonction bien pratique appelée \rfunc{odds.ratio} et 
disponible à cette adresse~: \url{http://joseph.larmarange.net/?Calculer-les-Odds-Ratio-d-une}.
En premier lieu, on va copier le code de cette fonction et l'exécuter dans \R~:

<<>>=
odds.ratio <- function(reg, level=0.95, digits=3) {
	if ("glm" %in% class(reg)) {
		if(reg$family$family == "binomial"){
			r <- cbind(exp(coef(reg)),exp(confint(reg, level=level))
					   ,summary(reg)$coefficients[,4])
			r[,1:3] <- round(r[,1:3],digits=digits)
			colnames(r)[1] <- "OR"
			colnames(r)[4] <- "p"
			printCoefmat(r,signif.stars=TRUE,has.Pvalue=TRUE)
		} else {
			stop('reg should be a glm with family=binomial or the result of multinom.')
		}
	} else if ("multinom" %in% class(reg)) {
		coef <- summary(reg)$coefficients
		ci <- confint(reg,level=level)
		# From http://www.ats.ucla.edu/stat/r/dae/mlogit.htm
		z <- summary(reg)$coefficients/summary(reg)$standard.errors
		p <- p <- (1 - pnorm(abs(z), 0, 1)) * 2
		d <- dim(ci)
		r <- array(NA,c(d[1]*d[3],d[2]+2))
		dimnames(r)[[1]]<-rep("",d[1]*d[3])
		for (i in 1:d[3]) {
			fl <- (i-1)*d[1] + 1 #first line
			ll <- i*d[1] #last line
			r[fl:ll,] <- cbind(coef[i,],ci[,,i],p[i,])
			rownames(r)[fl:ll] <- paste0(rownames(coef)[i],"/",colnames(coef))
		}
		r[,1:3] <- round(r[,1:3],digits=digits)
		colnames(r) <- c("OR",dimnames(ci)[[2]],"p")
		printCoefmat(r,signif.stars=TRUE,has.Pvalue=TRUE)
	}
	else 
		stop('reg should be a glm with family=binomial or the result of multinom.')
}
@

Ensuite, il n'y a plus qu'à l'éxécuter~:

<<>>=
odds.ratio(reg)
@


L'extension \textsf{effects} propose une représentation graphique résumant les
effets de chaque variable du modèle. Attention~: il y a un bug
dans les versions antérieures à la 2.3-0. Il est recommandé d'installer la dernière 
version à partir de R-Forge en utilisant la commande suivante~:

<<eval=FALSE>>=
install.packages("effects", repos="http://R-Forge.R-project.org")
@

Nous allons appliquer la fonction \rfunc{plot}
au résultat de la fonction \rfunc{allEffects}. Nous obtenons alors la figure 
\ref{fig_alleffects_reg} \vpageref{fig_alleffects_reg}. 

\begin{figure}
<<>>=
library(effects)
plot(allEffects(reg))
@ 
\caption{Représentation graphique de l'effet de chaque variable du modèle logistique}
\label{fig_alleffects_reg}
\end{figure}

Une manière de tester la qualité d'un modèle est le calcul d'une \textit{matrice 
de confusion}, c'est-à-dire le tableau croisé des valeurs observées et celles
des valeurs prédites en appliquant le modèle aux données d'origine.

La fonction \rfunc{predict} avec l'argument \texttt{type="response"} permet 
d'appliquer notre modèle logistique à un tableau de données et renvoie pour 
chaque individu la probabilité qu'il ait vécu le phénomène étudié.

<<>>=
sport.pred <- predict(reg,type="response",newdata=d)
head(d$sport.pred)
@

Or notre variable étudiée est de type binaire. Nous devons donc transformer 
nos probabilités prédites en une variable du type <<~oui/non~>>. Usuellement,
les probabilités prédites seront réunies en deux groupes selon qu'elles soient
supérieures ou inférieures à la moitié. La matrice de confusion est alors égale à~:

<<>>=
table(sport.pred>0.5,d$sport)
@

Nous avons donc 583 (384+199) prédictions incorrectes sur un total de 1993, soit 
un taux de mauvais classement de 29,3~\%.

\section{Sélection de modèles}

Il est toujours tentant lorsque l'on recherche les facteurs associés à un 
phénomène d'inclure un nombre important de variables explicatives potentielles
dans un mmodèle logistique. Cependant, un tel modèle ne pas forcément le plus
efficace et certaines variables n'auront probablement pas d'effet significatif
sur la variable d'intérêt.

La technique de \textit{sélection descendante pas à pas} est une approche visant
à améliorer son modèle explicatif\footnote{
  Il existe également des méthodes de \textit{sélection ascendante pas à pas}, 
  mais nous les aborderons pas ici.
}. On réalise un premier modèle avec toutes les
variables spécifiées, puis on regarde s'il est possible d'améliorer le modèle
en supprimant une des variables du modèle. Si plusieurs variables permettent
d'améliorer le modèle, on supprimera la variable dont la suppression améliorera
le plus le modèle. Puis on recommence le même procédé pour voir si la suppression
d'une seconde variable peut encore améliorer le modèle et ainsi de suite. Lorsque 
le modèle ne peut plus être améliorer par la suppresion d'une variable, on s'arrête.

Il faut également définir un critère pour déterminer la qualité d'un modèle.
L'un des plus utilisés est le \textit{Akaike information criterion} ou AIC.
Plus l'AIC sera faible, meilleure sera le modèle.

La fonction \rfunc{step} permet justement de sélectionner le meilleur modèle par
une procédure pas à pas descendante basée sur la minimisation de l'AIC. La fonction
affiche à l'écran les différentes étapes de la sélection et renvoie le modèle final.

<<>>=
reg2 <- step(reg)
@

Le modèle initial a un AIC de 2114,8. À la première étape, il apparait que la
suppression de la variable religion permet diminuer l'AIC à 2109,1. Lors de la 
seconde étape, toute suppression d'une autre variable ferait augmenter l'AIC. La
procédure s'arrête donc.

\section{Régression logistique multinomiale}

La régression logistique multinomiale est une extension de la régression logistique
aux variables qualitatives à trois modalités ou plus. Dans ce cas de figure, chaque 
modalité de la variable d'intérêt sera comparée à la modalité de réference. Les odds
ratio seront donc exprimés par rapport à cette dernière.

Nous allons prendre pour exemple la variable \texttt{trav.satisf}, à savoir la 
satisfaction ou l'insatisfaction au travail.

<<>>=
freq(d$trav.satisf)
@

Nous allons choisir comme modalité de référence la position intermédiaire, à savoir
l'<<~équilibre~>>. De plus, nous n'allons conserver pour l'analyse 

<<>>=
d$trav.satisf <- relevel(d$trav.satisf, "Equilibre")
@

Enfin, nous allons aussi en profiter pour raccourcir les étiquettes de la variable
\texttt{trav.imp}~:

<<>>=
levels(d$trav.imp) <- c("Le plus","Aussi","Moins","Peu")
@


Pour calculer un modèle logistique multinomial, nous allons utiliser la fonction
\rfunc{multinom} de l'extension \textsf{nnet}\footnote{Une alternative est d'avoir
recours à l'extension \textsf{mlogit} que nous n'aborderons pas ici.}. Si l'extension
n'est pas disponible, vous devrez l'installer (voir section \ref{installation_extensions}
\vpageref{installation_extensions}). La syntaxe de \rfunc{multinom} est similaire 
à celle de \texttt{glm}, le paramètre \texttt{family} en moins.

<<>>=
library(nnet)
regm <- multinom(trav.satisf ~ sexe + etud + grpage + trav.imp, data=d)
@

Comme pour la régression logistique, il est possible de réaliser une sélection
pas à pas descendante~:

<<>>=
regm2 <- step(regm)
@

La plupart des fonctions vues précédemment fonctionnent\footnote{Voir 
\url{http://www.ats.ucla.edu/stat/r/dae/mlogit.htm} (en anglais) pour plus de
détails.}, de même que la représentation graphique des effets (figure 
\ref{fig_alleffects_regm} \vpageref{fig_alleffects_regm}).

<<>>=
summary(regm2)
odds.ratio(regm2)
@

\begin{figure}
<<>>=
library(effects)
plot(allEffects(regm2))
@ 
\caption{Représentation graphique de l'effet de chaque variable du modèle logistique}
\label{fig_alleffects_regm}
\end{figure}

De même, il est possible de calculer la matrice de confusion~:

<<>>=
table(predict(regm2,newdata=d),d$trav.satisf)
@

\section{Exercices}

\begin{exo}{reg_log}
  Nous allons utiliser le fichier de données \texttt{Aids2} fourni par l'extension
  \textsf{MASS}. Chargez cette extension en mémoire, puis utilisez la commande
  \texttt{data(Aids2)} pour charger ce fichier de données. Un descriptif (en anglais)
  de ce tableau de données est disponible via la commande \texttt{?Aids2}. Calculer
  un modèle de régression logistique évaluant l'effet du sexe, de la région 
  (variable \texttt{state}), de l'âge et de la catégorie de transmission sur la 
  probabilité d'être toujours en vie.  Représentez graphiquement l'effet des variables
  explicatives. Calculez des groupes d'âges et refaite le modèle ainsi que le graphique.
  Calculez les odds ratio. Enfin, réalisez une sélection descendante pas à pas.
\end{exo}

\chapter{Données pondérées}

S'il est tout à fait possible de travailler avec des données pondérées
sous \R, cette fonctionnalité n'est pas aussi bien intégrée que dans
la plupart des autres logiciels de traitement statistique.  En
particulier, il y a plusieurs manières possibles de gérer la pondération.

Dans ce qui suit, on utilisera le jeu de données tiré de l'enquête
\textit{Histoire de vie} et notamment sa variable de pondération
\texttt{poids}\footnote{On notera que cette variable est utilisée à
  titre purement illustratif. Le jeu de données étant un extrait
  d'enquête et la variable de pondération n'ayant pas été recalculée,
  elle n'a ici à proprement parler aucun sens.}.


<<>>=
data(hdv2003)
d <- hdv2003
range(d$poids)
@ 


\section{Options de certaines fonctions}

Tout d'abord, certaines fonctions de \R acceptent en argument un
vecteur permettant de pondérer les observations (l'option est en
général nommée \texttt{weights} ou \texttt{row.w}). C'est le cas par
exemple des méthodes d'estimation de modèles linéaires (\rfunc{lm}) ou
de modèles linéaires généralisés (\rfunc{glm}), ou dans les analyses
de correspondances des extensions \textsf{ade4} (\rfunc{dudi.acm}) ou
\textsf{FactoMineR} (\texttt{MCA}).

Par contre cette option n'est pas présente dans les fonctions de base
comme \rfunc{mean}, \rfunc{var}, \rfunc{table} ou \rfunc{chisq.test}.


\section{Fonctions de l'extension \questionr}

L'extension \questionr propose quelques fonctions permettant de
calculer des statistiques simples pondérées\footnote{Les fonctions
  \rfunc{wtd.mean} et \rfunc{wtd.var} sont des copies conformes des
  fonctions du même nom de l'extension \textsf{Hmisc} de Frank
  Harrel. \textsf{Hmisc} étant une extension <<~de taille~>>, on a
  préféré recopié les fonctions pour limiter le poids des dépendances.}~:

\begin{description}
\item[\rfunc{wtd.mean}] moyenne pondérée
\item[\rfunc{wtd.var}] variance pondérée
\item[\rfunc{wtd.table}] tris à plat et tris croisés pondérés
\end{description}

On les utilise de la manière suivante~:

<<wtdmean,warning=FALSE,message=FALSE,cache=FALSE>>=
mean(d$age)
library(questionr)
wtd.mean(d$age, weights=d$poids)
wtd.var(d$age, weights=d$poids)
@ 

Pour les tris à plat, on utilise la fonction \rfunc{wtd.table} à
laquelle on passe la variable en paramètre~:

<<>>=
wtd.table(d$sexe, weights=d$poids)
@ 

Pour un tri croisé, il suffit de passer deux variables en paramètres~:

<<>>=
wtd.table(d$sexe, d$hard.rock, weights=d$poids)
@ 

Ces fonctions admettent notamment les deux options suivantes~:
\begin{description}
\item[\texttt{na.rm}] si \texttt{TRUE}, on ne conserve que les
  observations sans valeur manquante
\item[\texttt{normwt}] si \texttt{TRUE}, on normalise les poids pour
  que les effectifs totaux pondérés soient les mêmes que les effectifs
  initiaux. Il faut utiliser cette option, notamment si on souhaite
  appliquer un test sensible aux effectifs comme le $\chi^2$.
\end{description}


Ces fonctions rendent possibles l'utilisation des statistiques
descriptives les plus simples et le traitement des tableaux croisés
(les fonctions \rfunc{lprop}, \rfunc{cprop} ou \rfunc{chisq.test}
peuvent être appliquées au résultat d'un \rfunc{wtd.table}) mais
restent limitées en termes de tests statistiques ou de
graphiques\ldots

\section{Présentation de l'extension \textsf{survey}}

L'extension \textsf{survey} est spécialement dédiée au traitement
d'enquêtes ayant des techniques d'échantillonnage et de pondération
potentiellement très complexes. L'extension s'installe comme la
plupart des autres~:

<<eval=FALSE>>=
install.packages("survey",dep=TRUE)
@ 

Le site officiel (en anglais) comporte beaucoup d'informations, mais
pas forcément très accessibles~:

\url{http://faculty.washington.edu/tlumley/survey/}

Pour utiliser les fonctionnalités de l'extension, on doit d'abord
définir un \textit{design} de notre enquête. C'est-à-dire indiquer
quel type de pondération nous souhaitons lui appliquer. Dans notre cas
nous utilisons le \textit{design} ou \textit{plan d'échantillonnage}
le plus simple, avec une variable de
pondération déjà calculée. Ceci se fait à l'aide de la fonction
\rfunc{svydesign}~:

<<>>=
library(survey)
dw <- svydesign(ids=~1,data=d,weights=~d$poids)
@ 

Cette fonction crée un nouvel objet, que nous avons nommé
\texttt{dw}. Cet objet n'est pas à proprement parler un tableau de
données, mais plutôt un tableau de données \textit{plus} une méthode
de pondération. \texttt{dw} et \texttt{d} sont des objets distincts,
les opérations effectuées sur l'un n'ont pas d'influence sur l'autre.
On peut cependant retrouver le contenu de \texttt{d} depuis
\texttt{dw} en utilisant \texttt{dw\$variables}~:

<<>>=
mean(d$age)
mean(dw$variables$age)
@ 

Lorsque notre \textit{design} est déclaré, on peut lui appliquer une
série de fonctions permettant d'effectuer diverses opérations
statistiques en tenant compte de la pondération. On citera notamment~:

\begin{description}
\item[\rfunc{svymean}, \rfunc{svyvar}, \rfunc{svytotal}, \rfunc{svyquantile}] statistiques univariées 
\item[\rfunc{svytable}] tableaux croisés
\item[\rfunc{svychisq}] test du $\chi^2$
\item[\rfunc{svyby}] statistiques selon un facteur
\item[\rfunc{svyglm}] modèles linéaires généralisés
\item[\rfunc{svyplot}, \rfunc{svyhist}, \rfunc{svyboxplot}] fonctions graphiques
\end{description}


D'autres fonctions sont disponibles, comme \texttt{svyratio}, mais elles ne seront pas abordées ici.

Pour ne rien arranger, ces fonctions prennent leurs arguments sous
forme de formules, c'est-à-dire pas de la manière habituelle. En
général l'appel de fonction se fait en spécifiant d'abord les
variables d'intérêt sous forme de formule, puis l'objet \textit{design}.
L'intervalle de confiance d'une moyenne s'obtient avec \rfunc{confint} et
celui d'une proportion avec \rfunc{svyciprop}.

Voyons tout de suite quelques exemples\footnote{
  Pour d'autres exemples, voir 
  \url{http://www.ats.ucla.edu/stat/r/faq/svy_r_oscluster.htm} (en anglais).
}~:

<<>>=
svymean(~age, dw)
confint(svymean(~age, dw)) # Intervalle de confiance
svyquantile(~age, dw, quantile=c(0.25,0.5,0.75),ci=TRUE)
svyvar(~heures.tv, dw, na.rm=TRUE)
svytable(~sexe, dw)
svyciprop(~sexe, dw) # Intervalle de confiance
svytable(~sexe+clso, dw)
@ 

En particulier, les tris à plat se déclarent en passant comme argument
le nom de la variable précédé d'un symbole \texttt{\~}, tandis que les
tableaux croisés utilisent les noms des deux variables séparés par un
\texttt{+} et précédés par un \texttt{\~}.

On peut récupérer le tableau issu de \rfunc{svytable} dans un objet et
le réutiliser ensuite comme n'importe quel tableau croisé~:

<<>>=
tab <- svytable(~sexe+clso, dw)
tab
lprop(tab)
svychisq(~sexe + clso, dw)
@ 

Les fonctions \rfunc{lprop} et \rfunc{cprop} de \questionr
\marqr sont donc tout à fait compatibles avec l'utilisation de
\textsf{survey}. La fonction \rfunc{freq} peut également être utilisée
si on lui passe en argument non pas la variable elle-même, mais son
tri à plat obtenu avec \rfunc{svytable}~:

<<>>=
tab <- svytable(~peche.chasse, dw)
freq(tab, total=TRUE)
@ 

Par contre, il \textbf{ne faut pas} utiliser \rfunc{chisq.test} sur un tableau
généré par \rfunc{svytable}. Les effectifs étant extrapolés à partir de la
pondération, les résultats du test seraient complètement faussés. Si on veut
faire un test du $\chi^2$ sur un tableau croisé pondéré, il faut utiliser
\rfunc{svychisq}~:

<<>>=
svychisq(~sexe + clso, dw)
@ 

Le principe de la fonction \rfunc{svyby} est similaire à celui de \rfunc{tapply}
(voir section \ref{sec_tapply} \vpageref{sec_tapply}). Elle permet de calculer
des statistiques selon plusieurs sous-groupes définis par un facteur. 
Par exemple~:

<<>>=
svyby(~age, ~sexe, dw, svymean)
@

Enfin, \textsf{survey} est également capable de produire des
graphiques à partir des données pondérées. Des exemples sont donnés
figure~\ref{fig_surveyplot} \vpageref{fig_surveyplot}.

\begin{figure}
<<>>=
par(mfrow=c(2,2))
svyplot(~age+heures.tv,dw,col="red",main="Bubble plot")
svyhist(~heures.tv, dw, col="peachpuff", main="Histogramme")
svyboxplot(age~1,dw,main="Boxplot simple", ylab="Âge")
svyboxplot(age~sexe,dw,main="Boxplot double", ylab="Âge", xlab="Sexe")
@ 
\caption{Fonctions graphiques de l'extension \textsf{survey}}
\label{fig_surveyplot}
\end{figure}

Enfin, \textsf{survey} fournit une fonction \rfunc{svyglm} permettant de calculer
un modèle statistique tout en prenant en compte le plan d'échantillonnage spécifié.
La syntaxe de \rfunc{svyglm} est proche de celle \rfunc{glm} :

<<>>=
reg <- svyglm(sport ~ sexe + age + relig + heures.tv, dw, family=binomial(logit))
@

Le résultat obtenu est similaire à celui de \rfunc{glm} et l'on peut utiliser sans
problème les fonctions \rfunc{coef}, \rfunc{confint}, \rfunc{odds.ratio} ou
\rfunc{predict} abordées au chapitre \ref{chapitre_reglog}.

Par contre, la sélection descendante pas à pas d'un modèle par minimisation de 
l'AIC avec \texttt{step} ne fonctionnera pas. En effet, il semble qu'il n'existe
pas encore d'analogue de l'AIC dans le contexte d'un plan d'échantillonage 
complexe\footnote{Voir cette discussion 
\url{https://groups.google.com/forum/#!topic/r-help-archive/XcrSW9s7kwI}.}. 
On pourra se rabattre néanmoins sur une sélection raisonnée des variables à 
conserver ne prenant par exemple en compte que celles ayant un effet 
signicatif à 5~\% ou 10~\%.

Par ailleurs, la fonction \rfunc{allEffects} est elle aussi incompatible avec
\rfunc{svyglm}\footnote{Compatibilité qui pourra éventuellement être introduite
dans une future version de l'exteion \textsf{effects}.}.

\section{Définir un plan d'échantillonage complexe avec \textsf{survey}}

L'extension \textsf{survey} ne permet pas seulement d'indiquer une variable
de pondération mais également de prendre les spécificités du plan 
d'échantillonnage (strates, grappes, ...). Le plan d'échantillonnage ne joue 
pas seulement sur la pondération des données, mais influence le calcul des 
variances et par ricochet tous les tests statistiques. Deux échantillons identiques
avec la même variable de pondération mais des designs différents produiront les
mêmes moyennes et proportions mais des intervalles de confiance différents.

\subsection{Différents types d'échantillonnage}

L'\textit{échantillonnage aléatoire simple} ou \textit{échantillonnage équiprobable} 
est une méthode pour laquelle tous les échantillons possibles (de même taille) ont 
la même probabilité d'être choisis et tous les éléments de la population ont une 
chance égale de faire partie de l'échantillon. C'est l'échantillonnage le plus simple~: 
chaque individu à la même probabilité d'être sélectionné.

L'\textit{échantillonnage stratifié} est une méthode qui consiste d'abord à subdiviser
la population en groupes homogènes (strates) pour ensuite extraire un échantillon aléatoire
de chaque strate. Cette méthode suppose la connaissance de la structure de la population. 
Pour estimer les paramètres, les résultats doivent être pondérés par l'importance relative 
de chaque strate dans la population.

L'\textit{échantillonnage par grappes} est une méthode qui consiste à choisir un échantillon 
aléatoire d'unités qui sont elles-mêmes des sous-ensembles de la population (<<~grappes~>>).
Cette méthode suppose que les unités de chaque grappe sont représentatives. Elle possède 
l'avantage d'être souvent plus économique.

Il est possible de combiner plusieurs de ces approches. Par exemple, les \textit{enquêtes
démographiques et de santé}\footnote{
  Vaste programme d'enquêtes réalisées à intervalles réguliers dans les
  pays en développement, disponibles sur \url{http://www.measuredhs.com/}.}
(EDS) sont des enquêtes stratifiées en grappes à deux degrés. Dans un premier temps,
la population est divisée en strates par région et milieu de résidence. Dans chaque
strate, des zones d'enquêtes, correspondant à des unités de recensement, sont tirées
au sort avec une probabilité proportionnelle au nombre de ménages de chaque zone au
dernier recensement de population. Enfin, au sein de chaque zone d'enquête sélectionnée,
un recensement de l'ensemble des ménages est effectué puis un nombre identique de ménages
par zone d'enquête est tiré au sort de manière alétoire simple.

\subsection{Les options de \texttt{svydesign}}

La fonction \rfunc{svydesign} accepte plusieus arguments décrits sur sa page d'aide
(obtenue avec la commande \texttt{?svydesign}).

L'agument \texttt{data} permet de spécifier le tableau de données contenant les observations.

L'argument \texttt{ids} est obligatoire et spécifie sous la forme d'une formule les 
identifiants des différents niveaux d'un tirage en grappe. S'il s'agit d'un échantillon
aléatoire simple, on entrera \texttt{ids=\~{}1}. Autre situation~: supposons une étude portant
sur la population française. Dans un premier temps, on a tiré au sort un certain nombre de 
départements français. Dans un second temps, on tire au sort dans chaque département des 
communes. Dans chaque commune sélectionnée, on tire au sort des quartiers. Enfin, on 
interroge de manière exhaustive toutes les personnes habitant les quartiers enquêtés. Notre 
fichier de données devra donc comporter pour chaque observation les variables 
\texttt{id\textunderscore{}departement}, \texttt{id\textunderscore{}commune} et 
\texttt{id\textunderscore{}quartier}. On écrira alors pour 
l'argument \texttt{ids} la valeur suivante~: 
\texttt{ids=\~{}id\textunderscore{}departement+id\textunderscore{}commune+id\textunderscore{}quartier}.

Si l'échantillon est stratifié, on spécifiera les strates à l'aide de l'argument 
\texttt{strata} en spécifiant la variable contenant l'identifiant des strates. Par 
exemple~: \texttt{strata=\~{}id\textunderscore{}strate}.

Il faut encore spécifier les probabilités de tirage de chaque cluster ou bien 
la pondération des individus. Si l'on dispose de la probabilité de chaque 
observation d'être sélectionnée, on utilisera l'argument \texttt{probs}. Si,
par contre, on connaît la pondération de chaque observation (qui doit être 
proportionnelle à l'inverse de cette probabilité), on utilisera l'argument 
\texttt{weights}.

Si l'échantillon est stratifié, qu'au sein de chaque strate les individus ont été
tirés au sort de manière aléatoire et que l'on connaît la taille de chaque strate,
il est possible de ne pas avoir à spécifier la probabilité de tirage ou la 
pondération de chaque observation. Il est préférable de fournir une variable 
contenant la taille de chaque strate à l'argument \texttt{fpc}. De plus, dans 
ce cas-là, une petite correction sera appliquée au modèle pour prendre en compte
la taille finie de chaque strate.

Quelques exemples~:

<<eval=FALSE>>=
# Échantillonnage aléatoire simple
plan <- svydesign(ids=~1, data=donnees)

# Échantillonnage stratifié à un seul niveau (la taille de chaque strate est connue)
plan <- svydesign(ids=~1, data=donnees, fpc=~taille)

# Échantillonnage en grappes avec tirages à quatre degrés 
# (departement, commune, quartier, individus).
# La probabilité de tirage de chaque niveau de cluster est connue.
plan <- svydesign	(ids=~id_departement+id_commune+id_quartier, data=donnees,
			Probs=~proba_departement+proba_commune+proba_quartier)

# Échantillonnage stratifié avec tirage à deux degrés (clusters et individus).
# Le poids statistiques de chaque observation est connu.
plan <- svydesign(ids=~id_cluster, data=donnees, strata=~id_strate, weights=~poids)
@

Prenons l'exemple d'une enquête démographique et de santé. Le nom des différentes
variables est standardisé et commun quelle que soit l'enquête. Nous supposerons
que vous avez importé le fichier \textit{individus} dans un tableau de données
nommés \texttt{eds}. Le poids statistique de chaque individu est fourni par la
variable \texttt{V005} qui doit au préalable être divisée par un million. Les grappes
d'échantillonnage au premier degré sont fournies par la variable \texttt{V021} 
(\textit{primary sample unit}). Si elle n'est pas renseignée par le numéro de 
grappe \texttt{V001}
Enfin, le milieu de résidence (urbain / rural) est fourni par \texttt{V025} et la
région par \texttt{V024}. Pour rappel, l'échantillon a été stratifié à la fois
par région et par mileu de résidence. Certaines enquêtes fournissent directement
un numéro de strate via \texttt{V022}. Si tel est le cas, on pourra préciser le 
plan d'échantillonnage ainsi~:

<<eval=FALSE>>=
eds$poids <- eds$V005 / 1000000
design.eds <- svydesign(ids=~V021, data=eds, strata=~V022, weights=~poids)
@

Si \texttt{V022} n'est pas fourni mais que l'enquête a bien été stratifié par
région et milieu de résidence (vérifiez toujours le premier chapitre du rapport
d'enquête), on pourra créer une variable \texttt{strate} ainsi\footnote{
  L'astuce consiste à utiliser \texttt{as.integer} pour obetnir le code des facteurs
  et non leur valeur textuelle. L'addition des deux valeurs après multiplication
  du code de la région par 10 permet d'obtenir une valeur unique pour chaque 
  combinaison des deux variables. On retransforme le résultat en facteurs puis
  on modifie les étiquettes des modalités.
}~:

<<eval=FALSE>>=
eds$strate <- as.factor(as.integer(eds$V024)*10+as.integer(eds$V025))
levels(eds$strate) <- c(paste(levels(eds$V024),"Urbain"),paste(levels(eds$V024),"Rural"))
design.eds <- svydesign(ids=~V021, data=eds, strata=~strate, weights=~poids)
@

\subsection{Extraire un sous-échantillon}

Si l'on souhaite travailler sur un sous-échantillon tout en gardant les 
informations d'échantillonnage, on utilisera la fonction \rfunc{subset} présentée
en détail section \ref{subset} \vpageref{subset}.

<<>>=
sous <- subset(dw,sexe == "Femme" & age >=40)
@

\section{Conclusion}

En attendant mieux, la gestion de la pondération sous \R n'est sans
doute pas ce qui se fait de plus pratique et de plus simple. On pourra
quand même donner les conseils suivants~:

\begin{itemize}
\item utiliser les options de pondération des fonctions usuelles ou les
  fonctions d'extensions comme \questionr pour les cas les
  plus simples~;
\item si on utilise \textsf{survey}, effectuer tous les recodages et
  manipulations sur les données non pondérées autant que possible~;
\item une fois les recodages effectués, on déclare le \textit{design}
  et on fait les analyses en tenant compte de la pondération~;
\item surtout ne jamais modifier les variables du
  \textit{design}. Toujours effectuer recodages et manipulations sur
  les données non pondérées, puis redéclarer le \textit{design} pour que
  les mises à jour effectuées soient disponibles pour l'analyse~;
\end{itemize}


\chapter{Analyse des correspondances multiples (ACM)}
\label{chapitre_acm}

Il existe plusieurs techniques d'\textit{analyse factorielle} dont les plus courantes sont 
l'\textit{analyse en composante principale} (ACP) porte sur des 
variables quantitatives, l'\textit{analyse factorielle des correspondances}
(AFC) porte sur deux variables qualitatives et l'\textit{analyse des
correspondances multiples} (ACM) sur plusieurs variables qualitatives (il s'agit
d'une extension de l'AFC). Pour combiner des variables à la fois quantitatives
et qualitatives, on pourra avoir recours à l'analyse mixte de Hill et Smith.

Bien que ces techniques soient disponibles dans les extensions standards de \R,
il st souvent préférable d'avoir recours à deux autres extensions plus complètes,
\textsf{ade4} et \textsf{FactoMineR}, chacune ayant ses avantages et des possibilités
différentes. Voici les fonctions les plus fréquentes~:

\begin{center}
  \begin{tabular}{>{\raggedright}p{2cm}>{\raggedright}p{3.5cm}>{\raggedright}p{2.5cm}>{\raggedright}p{2.5cm}>{\raggedright}p{2.5cm}}
    \textbf{Analyse} & \textbf{Variables} & \textbf{Fonction standard} & \textbf{Fonction}\newline\textsf{ade4} & \textbf{Fonction}\newline\textsf{FactoMineR} \tabularnewline
    \hline
    ACP & plusieurs variables quantitatives & \rfunc{princomp} (\textsf{stats}) & \rfunc{dudi.pca} & \rfunc{PCA} \tabularnewline
    AFC & deux variables qualitatives & \rfunc{corresp} (\textsf{MASS}) & \rfunc{dudi.coa} & \rfunc{CA} \tabularnewline
    ACM & plusieurs variables qualitatives & \rfunc{mca} (\textsf{MASS}) & \rfunc{dudi.acm} & \rfunc{MCA} \tabularnewline
    Analyse mixte de Hill et Smith & plusieurs variables quantitatives et/ou qualitatives & --- & \rfunc{dudi.mix} & --- \tabularnewline
  \end{tabular}
\end{center}

Dans la suite de ce chapitre, nous n'arboderons que l'analyse des correspondances
multiples (ACM).

\begin{remarque}
  On trouvera également de nombreux supports de cours en français sur l'analyse factorielle sur
  le site de François Gilles Carpentier~: \url{http://geai.univ-brest.fr/~carpenti/}.
\end{remarque}

\section{Principe général}

L'analyse des correspondances multiples est une technique descriptive visant à
résumer l'information contenu dans un grand nombre de variables afin de faciliter
l'interprétention des corrélations existantes entre ces différentes variables.
On cherche à savoir quelles sont les modalités corrélées entre elles.

L'idée générale est la suivante\footnote{
  Pour une présentation plus détaillée, 
  voir \url{http://www.math.univ-toulouse.fr/~baccini/zpedago/asdm.pdf}.
}. L'ensemble des individus peut être représenté
dans un espace à plusieurs dimensions où chaque axe représente les différentes
variables utilisées pour décrire chaque individu. Plus précisément, pour chaque 
variable qualitative, il y a autant d'axes que de modalités moins un. Ainsi il 
faut trois axes pour décrire une variable à quatre modalités. Un tel nuage de 
points est aussi difficile à interpréter que de lire directement le fichier de
données. On ne voit pas es corrélations qu'il peut y avoir entre modalités, par
exemple qu'aller au cinéma est plus fréquent chez les personnes habitant en 
milieu urbain. Afin de mieux représenter ce nuage de points, on va procéder à
un changement de systèmes de coordonnées. Les individus seront dès lors 
projetés et représentés sur un nouveau système d'axe. Ce nouveau système d'axes
est choisis de telle manière que la majorité des variations soit concentrées sur
les premiers axes. Les deux-trois premiers axes permettront d'expliquer
la majorité des différences observées dans l'échantillon, les autres axes 
n'apportant qu'une faible part additionnelle d'information. Dès lors, l'analyse
pourra se concentrer sur ses premiers axes qui constitueront un bon résumé des
variations observables dans l'échantillon.

Avant toute ACM, il est indispensable de réaliser une analyse préliminaire de chaque variable,
afin de voir si toutes les classes sont aussi bien représentées ou s'il existe un
déséquilibre. L'ACM est sensible aux effectifs faibles, aussi regrouper les classes
quand cela est nécessaire.

\section{ACM avec \textsf{ade4}}

Si l'extension \textsf{ade4} n'est pas présente sur votre PC, il vous faut 
l'installer~:

<<eval=FALSE>>=
install.packages("ade4",dep=TRUE)
@

Comme précédemment, nous utiliserons le fichier de données \texttt{hdv2003}
fourni avec l'extension \textsf{questionr}.

<<>>=
library(questionr)
data(hdv2003)
d <- hdv2003
@

En premier lieu, comme dans le chapitre \ref{chapitre_reglog} sur la régression
logistique, nous allons créer une variable groupe d'âges et regrouper les modalités
de la variable <<~niveau d'étude~>>.

<<>>=
d$grpage <- cut(d$age,c(16,25,45,65,93),right=FALSE,include.lowest=TRUE)
d$etud <- d$nivetud
levels(d$etud) <- c("Primaire","Primaire","Primaire","Secondaire","Secondaire","Technique/Professionnel","Technique/Professionnel","Supérieur")
@

Ensuite, nous allons créer un tableau de données ne contenant que les variables
que nous souhaitons prendre en compte pour notre analyse factorielle.

<<>>=
dt <- d[,c("grpage","sexe","etud","peche.chasse","cinema","cuisine","bricol","sport","lecture.bd")]
@

Le calcul de l'ACM se fait tout simplement avec la fonction \rfunc{dudi.acm}. 

<<eval=FALSE>>=
acm <- dudi.acm(dt)
@

Par défaut, la fonction affichera le graphique des valeurs propres de chaque axe
(nous y reviendrons) et vous demandera le nombre d'axes que vous souhaitez 
conserver dans les résultats. Le plus souvent, cinq axes seront largement plus
que suffisants. Vous pouvez également éviter cette étape en indiquant directement
à \rfunc{dudi.acm} de vous renvoyer les cinq premiers axes ainsi.

<<>>=
acm <- dudi.acm(dt, scannf=FALSE, nf=5)
@

\begin{figure}
<<>>=
screeplot(acm)
@ 
\caption{Valeurs propres ou inerties de chaque axe}
\label{fig_acm_eig}
\end{figure}

Le graphique des valeurs propres peut être reproduit avec \rfunc{screeplot} 
(voir figure \ref{fig_acm_eig} \vpageref{fig_acm_eig}).
Les mêmes valeurs pour les premiers axes s'obtiennent 
également avec \rfunc{summary}\footnote{
  On pourra également avoir recours à la fonction \rfunc{inertia.dudi} pour
  l'ensemble des axes.
}.

<<>>=
summary(acm)
@

L'inertie totale est de 1,451 et l'axe 1 en explique 0,1474 soit 17~\%. L'inertie
projetée cumulée nous indique que les deux premiers axes expliquent à eux seuls
29~\% des variations observées dans notre échantillon.

Pour comprendre la signification des différents axes, il importe d'identifier
quelles sont les variables/modalités qui contribuent le plus à chaque axe. Une 
première représentation graphique est le cercle de corrélation des modalités. 
Pour cela, on aura recours à \rfunc{s.corcicle} (voir figure \ref{fig_corcircle}
\vpageref{fig_corcircle}). On indiquera d'abord \texttt{acm\$co} si l'on 
souhaite représenter les modalités ou \texttt{acm\$li} si l'on souhaite 
représenter les individus. Les deux chiffres suivant indiquent les
deux axes que l'on souhaite afficher (dans le cas présent les deux premiers axes).
Enfin, le paramètre \texttt{clabel} permet de modifier la taille des étiquettes.

\begin{figure}
<<>>=
s.corcircle(acm$co, 1, 2, clabel=0.7)
@ 
\caption{Cercle de corrélations des modalités sur les deux premiers axes}
\label{fig_corcircle}
\end{figure}

On pourra avoir également recours à \rfunc{boxplot} pour visualiser comment se
répartissent les modalités de chaque variable sur un axe donné\footnote{
  La fonction \rfunc{score} constituera également une aide à l'interprétation
  des axes.
}. Voir les figures
\ref{fig_acm_boxplot1} et \ref{fig_acm_boxplot2} \vpageref{fig_acm_boxplot1}.

\begin{figure}
<<>>=
boxplot(acm)
@ 
\caption{Répartition des modalités selon le premier axe}
\label{fig_acm_boxplot1}
\end{figure}

\begin{figure}
<<>>=
boxplot(acm, 2)
@ 
\caption{Répartition des modalités selon le second axe}
\label{fig_acm_boxplot2}
\end{figure}

Le tableau \texttt{acm\$cr} contient les rapports de corrélation (variant de 0 à 1)
entre les variables et les axes choisis au départ de l'ACM. Pour représenter graphiquement
ces rapports, utiliser \texttt{barplot(acm\$cr[,num],names.arg=row.names(
acm\$cr),las=2)} où \texttt{num} est le numéro de l'axe à représenter (voir figure
\ref{fig_rapports_corr} \vpageref{fig_rapports_corr}). Pour l'interprétation des 
axes, se concentrer sur les variables les plus structurantes,
c'est-à-dire dont le rapport de corrélation est le plus proche de 1.

\begin{figure}
<<>>=
par(mfrow = c(2,2))
for (i in 1:4)
  barplot(acm$cr[,i],names.arg=row.names(acm$cr),las=2, main=paste("Axe",i))
par(mfrow = c(1,1))
@ 
\caption{Rapports de corrélation des variables sur les 4 premiers axes}
\label{fig_rapports_corr}
\end{figure}

Pour représenter, les individus ou les modalités dans le plan factoriel, 
on utilisera la fonction \rfunc{s.label}. Il est bien sur possible de préciser 
les axes à représenter. L'argument \texttt{boxes} permet d'indiquer si l'on 
souhaite tracer une boîte pour chaque modalité.

\begin{figure}
<<>>=
s.label(acm$co, clabel=0.7)
@ 
\caption{Répartition des modalités selon les deux premiers axes}
\label{fig_acm_mod_plan12}
\end{figure}

\begin{figure}
<<>>=
s.label(acm$co, 3, 4, clabel=0.7, boxes=FALSE)
@ 
\caption{Répartition des modalités selon les axes 3 et 4}
\label{fig_acm_mod_plan34}
\end{figure}

\begin{figure}
<<>>=
s.label(acm$li, clabel=0, pch=17)
@ 
\caption{Répartition des individus selon les deux premiers axes}
\label{fig_acm_ind_plan12}
\end{figure}

La figure \ref{fig_acm_mod_plan12} \vpageref{fig_acm_mod_plan12} représente les
modalités sur les deux premiers axes tandis que la figure \ref{fig_acm_mod_plan34}
les représente selon les axes 3 et 4. La figure \ref{fig_acm_ind_plan12}, quant
à elle, représente les individus selon les deux premiers axes. En indiquant 
\texttt{clabel=0} (une taille nulle pour les étiquettes), \rfunc{s.label} remplace
chaque observation par un symbole qui peut être spécifié avec \texttt{pch} (pour
les différentes valeurs possibles, voir la figure \ref{fig_pch} \vpageref{fig_pch}).

\begin{astuce}
  Lorsque l'on réalise une ACM, il n'est pas rare que plusieurs observations 
  soient identiques, c'est-à-dire correspondent à la même combinaison de modalités.
  Dès lors, ces observations seront projetées sur le même point dans le plan factoriel.
  Une représentation classique des observations avec \rfunc{s.label} ne permettra 
  pas de rendre des effectifs de chaque point. Vous trouverez à cette adresse 
  (\url{http://joseph.larmarange.net/?article147}) une petite fonction \rfunc{s.freq}
  représentant chaque point par un carré proportionnel au nombre d'individus.
\end{astuce}

\begin{astuce}
  Gaston Sanchez propose un graphique amélioré des modalités dans le plan factoriel
  à cette adresse~: \url{http://rpubs.com/gaston/MCA}.
\end{astuce}

La fonction \rfunc{s.value} permet notamment de représenter un troisième axe factoriel. 
Sur la figure \ref{fig_acm_svalue} \vpageref{fig_acm_svalue}, nous projettons 
les individus selon les deux premiers axes factoriels. La taille et la couleur 
des carrés dépendent pour leur part de la coordonnée des individus 
sur le troisième axe factoriel. Le paramètre \texttt{csi} permet d'ajuster la
taille des carrés.

\begin{figure}
<<>>=
s.value(acm$li, acm$li[,3], 1, 2, csi=0.5)
@ 
\caption{Répartition des individus selon les trois premiers axes}
\label{fig_acm_svalue}
\end{figure}

\rfunc{s.arrow} permet de représenter les vecteurs variables ou les vecteurs 
individus sous la forme d'une flèche allant de l'origine du plan factoriel 
aux coordonnées des variables/individus (voir figure \ref{fig_acm_sarrow}
\vpageref{fig_acm_sarrow}).

\begin{figure}
<<>>=
s.arrow(acm$co, clabel=0.7)
@ 
\caption{Vecteurs des modalités selon les deux premiers axes}
\label{fig_acm_sarrow}
\end{figure}

\rfunc{s.hist} permet de représenter des individus (ou des modalités) sur le plan
factoriel et d'afficher leur distribution sur chaque axe (figure \ref{fig_acm_shist} 
\vpageref{fig_acm_shist}).

\begin{figure}
<<>>=
s.hist(acm$li, clabel=0, pch=15)
@ 
\caption{Distribution des individus dans le plan factoriel}
\label{fig_acm_shist}
\end{figure}

\rfunc{s.class} et \rfunc{s.chull} permettent de représenter les différentes 
observations classées en plusieurs catégories. Cela permet notamment de projeter
certaines variables. \rfunc{s.class} représente les observations par des points,
lie chaque observation au barycentre de la modalité à laquelle elle appartient
et dessine une ellipse représentant la forme générale du nuage de points 
(figure \ref{fig_acm_sclass} \vpageref{fig_acm_sclass}). 
\rfunc{s.chull} représente les barycentres de chaque catégorie et dessine des 
lignes de niveaux représentant la distribution des individus de cette catégorie.
Les individus ne sont pas directement représentés (figure \ref{fig_acm_schull}
\vpageref{fig_acm_schull}).

\begin{figure}
<<>>=
library(RColorBrewer)
s.class(acm$li,dt$sexe,col=brewer.pal(4,"Set1"))
@ 
\caption{Individus dans le plan factoriel selon le sexe (\texttt{s.class})}
\label{fig_acm_sclass}
\end{figure}

\begin{figure}
<<>>=
s.chull(acm$li,dt$sexe,col=brewer.pal(4,"Set1"))
@ 
\caption{Individus dans le plan factoriel selon le sexe (\texttt{s.chull})}
\label{fig_acm_schull}
\end{figure}


\begin{remarque}
  Il est préférable de fournir une liste de couleurs (via le paramètre \texttt{col})
  pour rendre le graphique plus lisible. Si vous avez installé l'extension
  \textsf{RColorBrewer}, vous pouvez utiliser les différentes palettes de couleurs
  proposées. Pour afficher les palettes disponibles, utilisez \rfunc{display.brewer.all}.
  Pour obtenir une palette de couleurs, utilisez la fonction \rfunc{brewer.pal} avec 
  les arguments \texttt{n} (nombre de couleurs demandées) et \texttt{pal} (nom de la
  palette de couleurs désirée, voir figure \ref{fig_RColorBrewer} 
  \vpageref{fig_RColorBrewer}).
\end{remarque}

\begin{astuce}
  La variable catégorielle transmise à \rfunc{s.class} ou \rfunc{s.chull} n'est pas
  obligatoirement une des variables retenues pour l'ACM. Il est tout à fait possible
  d'utiliser une autre variable. Par exemple~:
  
  <<eval=FALSE>>=
  s.class(acm$li,d$trav.imp,col=brewer.pal(4,"Set1"))
  @
\end{astuce}

\begin{figure}
<<>>=
library(RColorBrewer)
display.brewer.all(8)
@ 
\caption{Palettes de couleurs disponibles dans \textsf{RColorBrewer}}
\label{fig_RColorBrewer}
\end{figure}

Les fonctions \rfunc{scatter} et \rfunc{biplot} sont équivalentes~: elles appliquent
\rfunc{s.class} à chaque variable utilisée pour l'ACM. Voir la figure 
\ref{fig_acm_scatter} \vpageref{fig_acm_scatter}.

\begin{figure}
<<>>=
scatter(acm, col=brewer.pal(4,"Set1"))
@ 
\caption{La fonction \textsf{scatter} appliquée au résultat d'une ACM}
\label{fig_acm_scatter}
\end{figure}

\section{ACM avec \textsf{FactoMineR}}
\label{acm_factominer}

Comme avec \textsf{ade4}, il est nécessaire de préparer les données au préalable
(voir section précédente). L'ACM se calcule avec la fonction \rfunc{MCA}, 
\texttt{ncp} permettant de choisir le nombre d'axes à retenir~:

<<>>=
acm2 <- MCA(dt, ncp=5, graph=FALSE)
acm2
acm2$eig
sum(acm2$eig$eigenvalue)
@

En premier lieu, il apparait que l'inertie totale obtenue avec \rfunc{MCA} est 
différente de celle observée avec \rfunc{dudi.acm}. Cela est dû à un traitement
différents des valeurs manquantes. Alors que \rfunc{dudi.acm} exclu les valeurs
manquantes, \rfunc{MCA} les considèrent, par défaut, comme une modalité 
additionnelle. Pour calculer l'ACM uniquement sur les individus n'ayant pas de
valeur manquante, on aura recours à \rfunc{complete.cases}~:

<<>>=
acm2 <- MCA(dt[complete.cases(dt),], ncp=5, graph=FALSE)
acm2$eig
sum(acm2$eig$eigenvalue)
@

Les possibilités graphiques de \rfunc{FactoMineR} sont différentes de celles
de \rfunc{ade}.
Un recours à la fonction \rfunc{plot} affichera par défaut les individus, les 
modalités et les variables. La commande \texttt{?plot.MCA} affichera toutes les
options graphiques. L'argument \texttt{choix} permet de spécifier ce que l'on
souhaite afficher (<<~ind~>> pour les individus et les catégories, <<~var~>>
pour les variables). L'argument \texttt{invisible} quant à lui permet de spécifier
ce que l'on souhaite masquer. Les axes à afficher se précisent avec \texttt{axes}.
Voir figures \ref{fig_acm_plot_mca1}, \ref{fig_acm_plot_mca2}, \ref{fig_acm_plot_mca3},
\ref{fig_acm_plot_mca4} et \ref{fig_acm_plot_mca5}.

\begin{figure}
<<>>=
plot(acm2)
@ 
\caption{Plan factoriel (deux premiers axes)}
\label{fig_acm_plot_mca1}
\end{figure}

\begin{figure}
<<>>=
plot(acm2, axes=c(3,4))
@ 
\caption{Plan factoriel (axes 3 et 4)}
\label{fig_acm_plot_mca2}
\end{figure}

\begin{figure}
<<>>=
plot(acm2, choix="ind")
@ 
\caption{Plan factoriel (seulement les individus et les catégories)}
\label{fig_acm_plot_mca3}
\end{figure}

\begin{figure}
<<>>=
plot(acm2, choix="ind", invisible="ind")
@ 
\caption{Plan factoriel (seulement les catégories)}
\label{fig_acm_plot_mca4}
\end{figure}

\begin{figure}
<<>>=
plot(acm2, choix="var")
@ 
\caption{Plan factoriel (seulement les variables)}
\label{fig_acm_plot_mca5}
\end{figure}

La fonction \rfunc{plotellipses} trace des ellipses de confiance atour des 
modalités de variables qualitatives. L'objectif est de voir si les modalités
d'une variable qualitative sont significativement différentes les unes des autres.
Par défaut (\texttt{means=TRUE}), les ellipses de confiance sont calculées 
pour les coordonnées moyennes de chaque catégorie (voir figure 
\ref{fig_acm_plotellipses} \vpageref{fig_acm_plotellipses}). L'option 
\texttt{means=FALSE} calculera les ellipses de confiance pour l'ensemble
des coordonnées des observations relevant de chaque catégorie (voir figure
\ref{fig_acm_plotellipses2} \vpageref{fig_acm_plotellipses2}).

\begin{figure}
<<warning=FALSE>>=
plotellipses(acm2)
@ 
\caption{Ellipses de confiance (\texttt{means=TRUE}) dans le plan factoriel}
\label{fig_acm_plotellipses}
\end{figure}

\begin{figure}
<<warning=FALSE>>=
plotellipses(acm2, means=FALSE)
@ 
\caption{Ellipses de confiance (\texttt{means=FALSE}) dans le plan factoriel}
\label{fig_acm_plotellipses2}
\end{figure}

\clearpage

La fonction \rfunc{dimdesc} aide à décrire et interpréter les dimensions de l'ACM.
Cette fonction est très utile quand le nombre de variables est élevé.
Elle permet de voir à quelles variables les axes sont le plus liés : 
quelles variables et quelles modalités décrivent le mieux chaque axe.

<<~\textit{Pour les variables qualitatives, un modèle d'analyse de variance à un facteur 
est réalisé pour chaque dimension~; les variables à expliquer sont les coordonnées des 
individus et la variable explicative est une des variables qualitatives. Un test F permet
de voir si la variable a un effet significatif sur la dimension et des tests T sont réalisés
modalité par modalité (avec le contraste somme des alpha\textunderscore{}i=0). 
Cela montre si les coordonnées 
des individus de la sous-population définie par une modalité sont significativement différentes
de celles de l'ensemble de la population (i.e. différentes de 0). Les variables et modalités 
sont triées par probabilité critique et seules celles qui sont significatives sont gardées dans 
le résultat.}~>>

Source~: \url{http://factominer.free.fr/factosbest/description-des-dimensions.html}

<<>>=
dimdesc(acm2, axes=1:2)
@


\chapter{Classification ascendante hiérarchique (CAH)}
\label{chapitre_cah}

Il existe de nombreuses techniques statistiques visant à partinionner une population
en différentes classes ou sous-groupes. La \textit{classification ascendante 
hiérarchique} (CAH) est l'une d'entre elles. On cherche à ce que les individus
regroupés au sein d'une même classe (homogénéité intra-classe) soient le plus semblables 
possibles tandis que les classes soient le plus dissemblables (hétérogénéité inter-classe).

Le principe de la CAH est de rassembler des individus selon un critère de 
ressemblance défini au préalable qui s'exprimera sous la forme d'une \textit{matrice
de distances}, exprimant la distanc existant entre chaque individu pris deux à deux. 
Deux observations identiques auront une distance nulle. Plus les deux observations seront
dissemblables, plus la distance sera importante. La CAH va ensuite rassembler les 
individus de manière itérative afin de produire un \textit{dendrogramme} ou \textit{arbre de
classification}. La classification est \textit{ascendante} car elle part des observations 
individuelles~; elle est \textit{hiérarchique} car elle produit des classes ou 
groupes de plus en plus vastes, incluant des sous-groupes en leur sein. En découpant
cet arbre à une certaine hauteur choisie, on produira la partition désirée.

\begin{remarque}
  On trouvera également de nombreux supports de cours en français sur la CAH sur
  le site de François Gilles Carpentier~: \url{http://geai.univ-brest.fr/~carpenti/}.
\end{remarque}

\section{Calculer une matrice des distances}

La notion de \textit{ressemblance} entre observations est évaluée par une distance
entre individus. Plusieurs type de ditances existent selon les données utilisées.

Il existe de nombreuses distances mathématiques pour les variables quantitatives
(euclidiennes, Manhattan...) que nous n'aborderons pas ici\footnote{Pour une 
présentation de ces différentes distances, on pourra se référer à
\url{http://old.biodiversite.wallonie.be/outils/methodo/similarite_distance.htm}
ou encore à ce support de cours par D. Chessel, J. Thioulouse et A.B. Dufour disponible
à \url{http://pbil.univ-lyon1.fr/R/pdf/stage7.pdf}.}.
La plupart peuvent être calculées avec la fonction \rfunc{dist}.

Usuellement, pour un ensemble de variables qualitatives, on aura recours à la
distance du $\Phi^2$ qui est celle utilisée pour l'analyse des correspondances 
multiples (voir chapitre \ref{chapitre_acm} \vpageref{chapitre_acm}). Avec
l'extension \textsf{ade4}, la distance du $\Phi^2$ s'obtient avec la fonction
\rfunc{dist.dudi}\footnote{Cette même fonction peut aussi être utilisée pour 
calculer une distance après une analyse en composantes principales ou une 
analyse mixte de Hill et Smith.}. Le cas particulier de la CAH avec l'extension
\textsf{FactoMineR} sera abordée dans une section spécifique (section 
\ref{cah_factominer} \vpageref{cah_factominer}). Nous évoquerons également la 
distance de Gower qui peut s'appliquer à un ensemble de variables à la fois 
qualitatives et quantitatives et qui se calcule avec la fonction \rfunc{daisy} de 
l'extension \textsf{cluster}. Enfin, dans le chapitre \ref{chapitre_sequences}
sur l'analyse de séquences \vpageref{chapitre_sequences}, nous verrons également
la fonction \rfunc{seqdist} (extension \textsf{TraMineR}) permettant de calculer
une distance entre séquences.

\subsection{Distance de Gower}

En 1971, Gower a proposé un indice de similarité qui porte son nom. L'objectif de 
cet indice consiste à mesurer dans quelle mesure deux individus sont semblables. 
L'indice de Gower varie entre 0 et 1. Si l'indice vaut 1, les deux individus sont 
identiques. À l'opposé, s'il vaut 0, les deux individus considérés n'ont pas de 
point commun. Si l'on note $S_{g}$ l'indice de similarité de Gower, la distance 
de Gower $D_{g}$ s'obtient simplement de la manière suivante : $D_{g} = 1 – S_{g}$.
Ainsi, la distance sera nulle entre deux individus identiques et elle sera égale 
à 1 entre deux individus totalement différents. Cette distance s'obtient sous \R
avec la fonction \rfunc{daisy} du package \textsf{cluster}.

L'indice de similarité de Gower entre deux individus $x_{1}$ et $x_{2}$ se calcule
de la manière suivante~:

\begin{equation}
S_{g}(x_{1},x_{2})=\frac{1}{p}\sum_{j=1}^{p}s_{12j}
\end{equation}
 
\noindent
$p$ représente le nombre total de caractères (ou de variables) descriptifs utilisés
pour comparer les deux individus. $s_{12j}$ représente la similarité partielle 
entre les individus 1 et 2 concernant le descripteur $j$. Cette similarité partielle
se calcule différemment s'il s'agit d'une variable qualitative ou quantitative~:
\begin{itemize}
\item \textbf{variable qualitative~:} $s_{12j}$ vaut 1 si la variable $j$ prend 
la même valeur pour les individus 1 et 2, et vaut 0 sinon. Par exemple, si 1 et 2
sont tous les deux <<~grand~>>, alors $s_{12j}$ vaudra 1. Si 1 est <<~grand~>> 
et 2 <<~petit~>>, $s_{12j}$ vaudra 0.
\item \textbf{variable quantitative~:} la différence absolue entre les valeurs des deux 
variables est tout d'abord calculée, soit $\left | y_{1j} - y_{2j} \right |$. 
Puis l'écart maximum observé sur l'ensemble du fichier est déterminé et noté $R_{j}$. 
Dès lors, la similarité partielle vaut $S_{12j}=\left | y_{1j} - y_{2j} \right |/R_{j}$.
\end{itemize}

\noindent
Dans le cas où l'on n'a que des variables qualitatives, la valeur de l'indice de 
Gower correspond à la proportion de caractères en commun. Supposons des 
individus 1 et 2 décris ainsi~:
\begin{enumerate}
\item homme / grand / blond / étudiant / urbain
\item femme / grande / brune / étudiante / rurale
\end{enumerate}

\noindent
Sur les 5 variables utilisées pour les décrire, 1 et 2 ont deux caractéristiques 
communes~: ils sont grand(e)s et étudiant(e)s. Dès lors, l'indice de similarité 
de Gower entre 1 et 2 vaut $2/5=0,4$ (soit une distance de $1-0,4=0,6$).

\noindent
Plusieurs approches peuvent être retenues pour traiter les valeurs manquantes~:
\begin{itemize}
\item supprimer tout individu n'étant pas renseigné pour toutes les variables de l'analyse~;
\item considérer les valeurs manquantes comme une modalité en tant que telle~;
\item garder les valeurs manquantes en tant que valeurs manquantes.
\end{itemize}

\noindent
Le choix retenu modifiera les distances de Gower calculées. Supposons que l'on ait~:
\begin{enumerate}
\item homme / grand / blond / étudiant / urbain
\item femme / grande / brune / étudiante / \textit{manquant}
\end{enumerate}

\noindent
Si l'on supprime individus ayant des valeurs manquantes, 2 est retirée du fichier 
d'observations et aucune distance n'est calculée. 
Si l'on traite les valeurs manquantes comme une modalité particulière, 1 et 2 
partagent alors 2 caractères sur les 5 analysés, la distance de Gower entre eux 
est alors de $1-2/5=1-0,4=0,6$.
Si on garde les valeurs manquantes, l'indice de Gower est dès lors calculé sur 
les seuls descripteurs renseignés à la fois pour 1 et 2. La distance de Gower 
sera calculée dans le cas présent uniquement sur les 4 caractères renseignés et 
vaudra $1-2/4=0,5$.

\subsection{Distance du $\Phi^2$}

Il s'agit de la distance utilisée dans les analyses de correspondance multiples (ACM).
C'est une variante de la distance du $\chi^2$.
Nous considérons ici que nous avons $Q$ questions (soit $Q$ variables initiales 
de type facteur). À chaque individu est associé un \textit{patron} c'est-à-dire 
une certaine combinaison de réponses aux $Q$ questions. La distance entre deux 
individus  correspond à la distance entre leurs deux patrons. Si les deux 
individus présentent le même patron, leur distance sera nulle. La distance du 
$\Phi^2$ peut s'exprimer ainsi~:

\begin{equation}
d_{\Phi^2}^2(L_i,L_j)=\frac{1}{Q}\sum_{k}\frac{(\delta_{ik}-\delta_{jk})^2}{f_k}
\end{equation}

\noindent
où $L_i$ et $L_j$ sont deux patrons, $Q$ le nombre total de questions. 
$\delta_{ik}$ vaut 1 si la modalité $k$ est présente dans le patron $L_i$, 0 sinon. 
$f_k$ est la fréquence de la modalité $k$ dans l'ensemble de la population.

\noindent
Exprimé plus simplement, on fait la somme de l'inverse des modalités non communes
aux deux patrons, puis on divise par le nombre total de question. Si nous reprenons
notre exemple précédent~:
\begin{enumerate}
\item homme / grand / blond / étudiant / urbain
\item femme / grande / brune / étudiante / rurale
\end{enumerate}

\noindent
Pour calculer la distance entre 1 et 2, il nous faut connaître la proportion des 
différentes modalités dans l'ensemble de la population étudiée. En l'occurrence~:
\begin{itemize}
\item hommes : 52~\% / femmes : 48~\%
\item grand : 30~\% / moyen : 45~\% / petit : 25~\%
\item blond : 15~\% / châtain : 45~\% / brun : 30~\% / blancs : 10~\%
\item étudiant : 20~\% / salariés : 65~\% / retraités : 15~\%
\item urbain : 80~\% / rural : 20~\%
\end{itemize}

\noindent
Les modalités non communes entre les profils de 1 et 2 sont~: homme, femme, 
blond, brun, urbain et rural. La distance du $\chi^2$ entre 1 et 2 est donc 
la suivante~:

\begin{equation}
d_{\Phi^2}^2(L_1,L_2)=\frac{1}{5}(\frac{1}{0,52}+\frac{1}{0,48}+\frac{1}{0,15}+\frac{1}{0,30}+\frac{1}{0,80}+\frac{1}{0,20})=4,05
\end{equation}
 
Cette distance, bien que moins intuitive que la distance de Gower évoquée 
précédemment, est la plus employée pour l'analyse d'enquêtes en sciences sociales. 
Il faut retenir que la distance entre deux profils est dépendante de la 
distribution globale de chaque modalité dans la population étudiée. Ainsi, si 
l'on recalcule les distances entre individus à partir d'un sous-échantillon, 
le résultat obtenu sera différent. De manière générale, les individus présentant 
des caractéristiques rares dans la population vont se retrouver éloignés des 
individus présentant des caractéristiques fortement représentées.

\subsection{Exemple}

Nous allons reprendre l'ACM calculée avec \texttt{dudi.acm} (\textsf{ade4}) au
chapitre \ref{chapitre_acm}. La matrice des distances s'obtient dès lors avec
la fonction \rfunc{dist.dudi}~:

<<>>=
md <- dist.dudi(acm)
@


\section{Calcul du dendrogramme}

Il faut ensuite choisir une méthode d'agrégation pour construire le dendrogramme.
De nombreuses solutions existent (saut minimum, distance maximum, moyenne, Ward...). 
Chacune d'elle produira un dendrogramme différent. Nous ne détaillerons pas ici
ces différentes techniques\footnote{On pourra consulter le cours de FG Carpentier
déjà cité ou bien des ouvrages d'analyse statistique.}. Cependant, à l'usage, on
privilégiera le plus souvent la \textit{méthode de Ward}. Cette méthode se 
distingue de toutes les autres en ce sens qu'elle utilise une analyse de la 
variance approchée afin d'évaluer les distances entre groupes. La méthode 
de Ward se justifie bien lorsque lorsque l'on utilise le carré de la distance. 
Choisir de regrouper les deux individus les plus proches revient alors à choisir 
la paire de points dont l'agrégation entraîne la diminution minimale de l'inertie 
du nuage.  En résumé, cette méthode cherche à minimiser l'inertie intra-classe 
et à maximiser l'inertie inter-classe afin d'obtenir des classes les plus 
homogènes possibles.

En raison de la variété des distances possibles et de la variété des techniques 
d'agrégation, on pourra être amené à réaliser plusieurs dendrogrammes différents 
sur un même jeu de données jusqu'à obtenir une classification qui fait <<~sens~>>.

La fonction de base pour le calcul d'un dendrogramme est \rfunc{hclust} en précisant
le critère d'aggrégation avec \texttt{method}. Dans notre cas, nous allons opter
pour la méthode de Ward appliquée au carré des distances (ce qu'on indique avec
\texttt{md\^{}2})~:

<<>>=
arbre <- hclust(md^2, method="ward")
@

\begin{astuce}
  Le temps de calcul d'un dendrogramme peut être particulièrement important sur
  un gros fichier de données. L'extension \textsf{flashClust} permet de réduire
  significativement le temps de calcul. Il suffit d'installer puis d'appeler cette
  extension. La fonction \rfunc{hclust} sera automatiquement remplacée par cette
  version optimisée.
  
  <<eval=FALSE>>=
  library(flashClust)
  arbre <- hclust(md^2, method="ward")
  @
\end{astuce}

Le dendrogramme obtenu peut être affiché simplement avec \rfunc{plot}. Lorsque
le nombre d'individus est important, il peut être utile de ne pas afficher les
étiquettes des individus avec \texttt{labels=FALSE} (voir figure 
\ref{fig_plot_hclust} \vpageref{fig_plot_hclust}).

\begin{figure}
<<>>=
plot(arbre, labels=FALSE, main="Dendrogramme")
@ 
\caption{Dendrogramme obtenu avec \texttt{hclust}}
\label{fig_plot_hclust}
\end{figure}

La fonction \rfunc{agnes} de l'extension \textsf{cluster} peut également
être utilisée pour calculer le dendrogramme. Cependant, à l'usage, elle
semble être un peu plus lente que \rfunc{hclust}.

<<eval=FALSE>>=
arbre2 <- agnes(md^2, method="ward")
@

Le résultat obtenu n'est pas au même format que celui de \rfunc{hclust}. Il est
possible de transformer un objet \texttt{agnes} au format \texttt{hclust} avec
\rfunc{as.hclust}.

\begin{astuce}
  De nombreuses possibilités graphiques sont possibles avec les dendrogrammes.
  Des exemples documentés sont disponibles à cette adresse~: 
  \url{http://rpubs.com/gaston/dendrograms}.
\end{astuce}


\section{Découper le dendrogramme}

Pour obtenir une partition de la population, il suffit de découper le
dendrogramme obtenu à une certaine hauteur. En premier lieu, une analyse
de la forme du dendrogramme pourra nous donner une indication sur le nombre
de classes à retenir. Dans notre exemple, deux branches bien distinctes 
apparaissent sur l'arbre. 

Pour nous aider, nous pouvons représenter les sauts d'inertie du dendrogramme
selon le nombre de classes retenues (voir figure \ref{fig_sauts_inertie}
\vpageref{fig_sauts_inertie}). On voit trois sauts assez nets, à 2, 5 et
8 classes, représentés respectivement en vert, en rouge et en bleu.

\begin{figure}
<<>>=
inertie <- sort(arbre$height, decreasing=TRUE)
plot(inertie[1:20], type="s", xlab="Nombre de classes", ylab="Inertie")
points(c(2,5,8),inertie[c(2,5,8)],col=c("green3","red3","blue3"),cex=2,lwd=3)
@ 
\caption{Sauts d'inertie du dendrogramme}
\label{fig_sauts_inertie}
\end{figure}

La fonction \rfunc{rect.hclust} permet de visualiser les différentes partitions
directement sur le dendrogramme (voir figure \ref{fig_rect_hclust} 
\vpageref{fig_rect_hclust}).

\begin{figure}
<<>>=
plot(arbre, labels=FALSE, main="Partition en 2, 5 ou 8 classes", xlab="", ylab="", sub="", axes=FALSE,hang=-1)
rect.hclust(arbre, 2, border="green3")
rect.hclust(arbre, 5, border="red3")
rect.hclust(arbre, 8, border="blue3")
@
\caption{Différentes partitions du dendrogramme}
\label{fig_rect_hclust}
\end{figure}

L'extension \textsf{FactoMineR} (que nous aborderons dans la section 
\ref{cah_factominer} \vpageref{cah_factominer}) suggère d'utiliser la partition 
ayant la plus grande perte relative d'inertie. Nous avons développer une fonction
\rfunc{best.cutree} qui permet de calculer cette indicateur à partir de n'importe
quel dendrogramme calculé avec \rfunc{hclust} ou \rfunc{agnes}. Le code de cette
fonction est disponible à \url{http://joseph.larmarange.net/?article149}. Il suffit
de le recopier et de le coller dans \R~:

<<>>=
best.cutree <- function(hc, min=3, max=20, loss=FALSE, graph=FALSE, ...){
  if (class(hc)!="hclust") hc <- as.hclust(hc)
  max <- min(max, length(hc$height))
  inert.gain <- rev(hc$height)
  intra <- rev(cumsum(rev(inert.gain)))
  relative.loss = intra[min:(max)]/intra[(min - 1):(max - 1)]
  best = which.min(relative.loss)
  names(relative.loss) <- min:max
  if (graph) {
  	temp <- relative.loss
  	temp[best] <- NA
  	best2 <- which.min(temp)
  	pch <- rep(1, max-min+1)
  	pch[best] <- 16
  	pch[best2] <- 21
  	plot(min:max, relative.loss, pch=pch, bg="grey75", ...)
  } else {
  	if (loss)
  	  relative.loss
    else
      best + min - 1
  }
}

best.cutree(arbre)
@

Par défaut, cette fonction regarde quelle serait la meilleure partition entre
3 et 20 classes, en l'occurence il s'agirait d'une partition en 5 classes. Il
est possible de modifier le minimum et le maximum des partitions recherchées
avec \texttt{min} et \texttt{max}.

<<>>=
best.cutree(arbre, min=2)
@

\begin{figure}
<<>>=
best.cutree(arbre, min=2, graph=TRUE, xlab="Nombre de classes", ylab="Perte relative d'inertie")
@
\caption{Perte relative d'inertie selon le nombre de classes}
\label{fig_bestcutree}
\end{figure}

On peut également représenter le graphique des pertes relatives d'inertie avec
\texttt{graph=TRUE} (voir figure \ref{fig_bestcutree} \vpageref{fig_bestcutree}).
La meilleure partition selon ce critère est représentée par un point noir et la
seconde par un point gris. Un découpage en deux classes minimise ce critère. 
Cependant, si l'on souhaite réaliser une analyse un peu plus fine, un nombre de 
classes plus élevé serait pertinent. Nous allons donc retenir un découpage en
cinq classes. Le découpage s'effectue avec la fonction \rfunc{cutree}.

<<>>=
typo <- cutree(arbre, 5)
freq(typo)
@

La typologie obtenue peut être représentée dans le plan factoriel avec 
\rfunc{s.class} (voir figure \ref{fig_sclass_cah} \vpageref{fig_sclass_cah}).

\begin{figure}
<<>>=
par(mfrow=c(1,2))
s.class(acm$li,as.factor(typo), col=brewer.pal(5,"Set1"), sub="Axes 1 et 2")
s.class(acm$li,as.factor(typo), 3, 4, col=brewer.pal(5,"Set1"), sub="Axes 3 et 4")
par(mfrow=c(1,1))
@
\caption{Projection de la typologie obtenue par CAH selon les 4 premiers axes}
\label{fig_sclass_cah}
\end{figure}

Enfin, Romain François a developpé une fonction \rfunc{A2Rplot} permettant de 
réaliser facilement un dendrogramme avec les branches colorées. En premier lieu,
il faut récupérer le code de cette fonction~:

<<eval=FALSE>>=
source("http://addictedtor.free.fr/packages/A2R/lastVersion/R/code.R")
@
<<echo=FALSE>>=
source("A2Rplot.r")
@

Puis réaliser le graphique en indiquant le nombre de classes et les 
couleurs à utiliser pour chaque branche de l'arbre (voir figure \ref{fig_a2rplot}
\vpageref{fig_a2rplot}).


\begin{figure}
<<>>=
op = par(bg = "#EFEFEF")
A2Rplot(arbre, k=5, boxes = FALSE, col.up = "gray50", col.down = brewer.pal(5,"Dark2"), show.labels=FALSE)
par(op)
@
\caption{Un dendrogramme coloré}
\label{fig_a2rplot}
\end{figure}


\section{CAH avec l'extension \textsf{FactoMineR}}
\label{cah_factominer}

L'extension \textsf{FactoMineR} fournit une fonction \rfunc{HCPC} permettant
de réaliser une classification hiérarchique à partir du résultats d'une
analyse factorielle réalisée avec la même extension (voir section
\ref{acm_factominer} \vpageref{acm_factominer}). \rfunc{HCPC} réalise à la
fois le calcul de la matrice des distances, du dendrogramme et le partitionnement
de la population en classes. Par défaut, \rfunc{HCPC} calcule le dendrogramme
à partir du carré des distances et avec la méthode de Ward. L'arbre est affiché
à l'écran et vous pouvez indiquer où vous souhaitez le couper à la souris.
Cependant, si vous utilisez \textsf{RStudio}, il y a un bug lié à cette 
fonctationnalité. Vous devrez dès lors appeler \rfunc{HCPC} avec \texttt{graph=FALSE}.
Utilisez l'argument \texttt{nb.clust} pour indiquer le nombre de classes désirées.
Si vous appelez la fonction avec \texttt{nb.clust=-1}, l'arbre sera coupé selon 
la partition ayant la plus grande perte relative d'inertie (comme avec \rfunc{best.cutree}).

<<message=FALSE>>=
cah <- HCPC(acm2, nb.clust=-1, graph=FALSE)
@

On pourra représenter le dendrogramme avec \rfunc{plot} et l'argument
\texttt{choice="tree"} (voir figure \ref{fig_hcpc_tree} \vpageref{fig_hcpc_tree}).

% NB : probleme avec plot.HCPC et knir
% On insère manuellement
\begin{figure}
<<eval=FALSE>>=
plot(cah, choice="tree")
@
\centering
\includegraphics[width=12cm]{img/fig_hcpc_tree1.png}
\caption{Dendrogramme obtenu avec \texttt{HCPC}}
\label{fig_hcpc_tree}
\end{figure}

Il apparait que le dendrogramme obtenu avec \rfunc{HCPC} diffère de celui
que nous avons calculé précédemment en utilisant la matrice des distances
fournies par \rfunc{dist.dudi}. Cela est dû au fait que \rfunc{HCPC} procède
différement pour calculer la matrice des distances en ne prenant en compte
que les axes retenus dans le cadre de l'ACM. Pour rappel, nous avions retenu
que 5 axes dans le cadre de notre ACM~:

<<eval=FALSE>>=
acm2 <- MCA(dt[complete.cases(dt),], ncp=5, graph=FALSE)
@

\rfunc{HCPC} n'a donc pris en compte que ces 5 premiers axes pour calculer les
distances entre les individus, considérant que les autres axes n'apportent que
du <<~bruit~>> rendant la classification instable. Cependant, comme le montre
\texttt{summary(acm2)}, nos cinq premiers axes n'expliquent que 54~\% de la
variance. Il usuellement préférable de garder un plus grande nombre d'axes afin 
de couvrir au moins 80 à 90~\% de la variance\footnote{
  Voir par exemple \url{http://factominer.free.fr/classical-methods/classification-hierarchique-sur-composantes-principales.html}
}. De son côté, \rfunc{dist.dudi} prends en compte l'ensemble des axes pour 
calculer la matrice des distances. On peut reproduire cela avec \textsf{FactoMineR}
en indiquant \texttt{ncp=Inf} lors du calcul de l'ACM.

<<>>=
acm2 <- MCA(dt[complete.cases(dt),], ncp=Inf, graph=FALSE)
cah <- HCPC(acm2, nb.clust=-1, graph=FALSE)
@

\begin{figure}
<<eval=FALSE>>=
plot(cah, choice="tree")
@
\centering
\includegraphics[width=12cm]{img/fig_hcpc_tree2.png}
\caption{Dendrogramme obtenu avec \texttt{HCPC}}
\label{fig_hcpc_tree2}
\end{figure}

On obtient bien cette fois-ci le même résultat (voir figure \ref{fig_hcpc_tree2}
\vpageref{fig_hcpc_tree2}). D'autres graphiques sont disponibles, essayez par
exemple les commandes suivantes~:

<<eval=FALSE>>=
plot(cah, choice="3D.map")
plot(cah, choice="bar")
plot(cah, choice="map")
@

L'objet renvoyé par \rfunc{HCPC} contient de nombreuses informations. La partition
peut notamment être récupérée avec \texttt{cah\$data.clust\$clust}. Il y a également
diverses statistiques pour décrire les catégories.

<<>>=
cah
freq(cah$data.clust$clust)
@


\chapter{Analyse de séquences}
\label{chapitre_sequences}

\begin{important}
  Le texte de ce chapitre reprend, avec l'aimable autorisation de son auteur,
  un article de Nicolas Robette\footnote{
    Maître de conférences à l'Université de Versailles Saint-Quentin-en-Yvelines.
  } intitulé \textit{L'analyse de séquences~: une 
  introduction avec le logiciel R et le package TraMineR} et publié le 24 octobre
  2012 sur sur le blog Quanti\footnote{\url{http://quanti.hypotheses.org/686/}}.
\end{important}

Depuis les années 1980, l'étude quantitative des trajectoires biographiques 
(\textit{life course analysis}) a pris une ampleur considérable dans le champ 
des sciences sociales. Les collectes de données micro-individuelles longitudinales
se sont développées, principalement sous la forme de panels ou d'enquêtes 
rétrospectives. Parallèlement à cette multiplication des données disponibles,
la méthodologie statistique a connu de profondes évolutions. L'analyse des 
biographies (\textit{event history analysis}) --- qui ajoute une dimension 
diachronique aux modèles économétriques \textit{mainstream} --- s'est rapidement 
imposée comme l'approche dominante~: il s'agit de modéliser la durée des 
situations ou le risque d'occurrence des événements.

\section{L'analyse de séquences}

Cependant, ces dernières années ont vu la diffusion d'un large corpus de méthodes
descriptives d'analyse de séquences, au sein desquelles l'appariement optimal
(\textit{optimal matching}) occupe une place centrale\footnote{
  Pour une analyse des conditions sociales de la diffusion de l'analyse de séquences
  dans le champ des sciences sociales, voir Robette, 2012.
}. L'objectif principal de ces méthodes est d'identifier --- dans la diversité
d'un corpus de séquences constituées de séries d'états successifs ---
les régularités, les ressemblances, puis le plus souvent de construire des typologies
de <<~séquences-types~>>. L'analyse de séquences constitue donc un moyen de décrire 
mais aussi de mieux comprendre le déroulement de divers processus.

La majeure partie des applications de l'analyse de séquences traite de trajectoires 
biographiques ou de carrières professionnelles. Dans ces cas, chaque trajectoire
ou chaque carrière est décrite par une séquence, autrement dit par une suite 
chronologiquement ordonnée de <<~moments~>> élémentaires, chaque moment correspondant
à un <<~état~>> déterminé de la trajectoire (par exemple, pour les carrières 
professionnelles~: être en emploi, au chômage ou en inactivité). Mais on peut bien 
sûr imaginer des types de séquences plus originaux~: Andrew Abbott\footnote{
  \url{http://home.uchicago.edu/~aabbott/}
}, le sociologue américain qui a introduit l'\textit{optimal matching} dans les 
sciences sociales dans les années 1980, s'en est par exemple servi pour étudier 
la structure rhétorique d'articles scientifiques ou des séquences de pas de danses 
traditionnelles.

En France, les premiers travaux utilisant l'appariement optimal sont ceux de 
Claire Lemercier\footnote{
  \url{http://lemercier.ouvaton.org/document.php?id=62}
} sur les carrières des membres des institutions consulaires parisiennes au XIXe
siècle (Lemercier, 2005), et de Laurent Lesnard\footnote{
  \url{http://laurent.lesnard.free.fr/article.php3?id_article=22}
} sur les emplois du temps (Lesnard, 2008). Mais dès les années 1980, les chercheurs 
du Céreq construisaient des typologies de trajectoires d'insertion à l'aide des 
méthodes d'analyse des données <<~à la française~>> (analyse des correspondances,
etc.)\footnote{
  Voir par exemple l'article d'Yvette Grelet (2002). 
}. Au final, on dénombre maintenant plus d'une centaine d'articles de sciences 
sociales contenant ou discutant des techniques empruntées à l'analyse de séquences.

Pour une présentation des différentes méthodes d'analyse de séquences disponibles 
et de leur mise en \oe{}uvre pratique, il existe un petit manuel en français, 
publié l'année dernière aux éditions du Ceped (collection <<~Les clefs pour~>>\footnote{
  \url{http://www.ceped.org/?Les-Clefs-pour}
}) et disponible en pdf\footnote{
  \url{http://nicolas.robette.free.fr/Docs/Robette2011_Manuel_TypoTraj.pdf}
} (Robette, 2011). De plus, un article récemment publié dans le Bulletin de 
Méthodologie Sociologique compare de manière systématique les résultats obtenus 
par les principales méthodes d'analyse de séquences (Robette \& Bry, 2012). La
conclusion en est qu'avec des données empiriques aussi structurées que celles que 
l'on utilise en sciences sociales, l'approche est robuste, c'est-à-dire qu'un 
changement de méthode aura peu d'influence sur les principaux résultats. Cependant,
l'article tente aussi de décrire les spécificités de chaque méthode et les différences
marginales qu'elles font apparaître, afin de permettre aux chercheurs de mieux 
adapter leurs choix méthodologiques à leur question de recherche.

Afin d'illustrer la démarche de l'analyse de séquences, nous allons procéder ici 
à la description <<~pas à pas~>> d'un corpus de carrières professionnelles, issues
de l'enquête \textit{Biographies et entourage} (INED, 2000)\footnote{
  Pour une analyse plus poussée de ces données, avec deux méthodes différentes, 
  voir Robette \& Thibault, 2008. Pour une présentation de l'enquête, voir 
  Lelièvre \& Vivier, 2001.
}. Et pour ce faire, on va utiliser le logiciel \R, qui propose la solution 
actuellement la plus complète et la plus puissante en matière d'analyse de 
séquences. Les méthodes d'analyse de séquences par analyses factorielles ou de 
correspondances ne nécessitent pas de logiciel spécifique~: tous les logiciels 
de statistiques généralistes peuvent être utilisés (\textsf{SAS}, \textsf{SPSS}, 
\textsf{Stata}, \R, etc.). En revanche, il n'existe pas de fonctions pour 
l'appariement optimal dans \textsf{SAS} ou \textsf{SPSS}. Certains logiciels 
gratuits implémentent l'appariement optimal (comme \textsf{Chesa}\footnote{
  \url{http://home.fsw.vu.nl/ch.elzinga/}
} ou \textsf{TDA}\footnote{
  \url{http://steinhaus.stat.ruhr-uni-bochum.de/tda.html}
}) mais il faut alors recourir à d'autres programmes pour dérouler l'ensemble de 
l'analyse (classification, représentation graphique). \textsf{Stata} propose le 
module \textsf{sq}\footnote{
  \url{http://www.stata-journal.com/article.html?article=st0111}
}, qui dispose d'un éventail de fonctions intéressantes. Mais c'est \R et le 
package \textsf{TraMineR}\footnote{
  \url{http://mephisto.unige.ch/traminer/}
}, développé par des collègues de l'Université de Genève (Gabadinho \textit{et al},
2011), qui fournit la solution la plus complète et la plus puissante à ce jour~:
on y trouve l'appariement optimal mais aussi d'autres algorithmes alternatifs, 
ainsi que de nombreuses fonctions de description des séquences et de représentation
graphique.

\section{Installer \textsf{TraMineR} et récupérer les données}

Tout d'abord, à quoi ressemblent nos données~? On a reconstruit à partir de 
l'enquête les carrières de 1000 hommes. Pour chacune, on connaît la position 
professionnelle chaque année, de l'âge de 14 ans jusqu'à 50 ans. Cette position 
est codée de la manière suivante~: les codes 1 à 6 correspondent aux groupes 
socioprofessionnels de la nomenclature des PCS de l'INSEE\footnote{
  \url{http://www.insee.fr/fr/methodes/default.asp?page=nomenclatures/pcs2003/pcs2003.htm}
} (agriculteurs exploitants~; artisans, commerçants et chefs d'entreprise~; cadres
et professions intellectuelles supérieures~; professions intermédiaires~;  employés~;
ouvriers)~; on y a ajouté <<~études~>> (code 7), <<~inactivité~> (code 8) et 
<<~service militaire~>> (code 9). Le fichier de données comporte une ligne par individu
et une colonne par année~: la variable 1 correspond à la position à 14 ans, 
la variable 2 à la position à 15 ans, etc. Par ailleurs, les enquêtés étant 
tous nés entre 1930 et 1950, on ajoute à notre base une variable <<~génération~>>
à trois modalités, prenant les valeurs suivantes~: 1=1930-1938~; 2=1939-1945~;
3=1946-1950. Au final, la base est constituée de 500 lignes et de 37+1=38 colonnes
et se présente sous la forme d'un fichier texte au format \texttt{csv} (téléchargeable à 
\url{http://nicolas.robette.free.fr/Docs/trajpro.csv}).

Une fois \R ouvert, on commence par installer les extensions nécessaires à ce 
programme (opération à ne réaliser que lors de leur première utilisation) et par 
les charger en mémoire. L'extension \textsf{TraMineR} propose de nombreuses 
fonctions pour l'analyse de séquences. L'extension \textsf{cluster} comprend un 
certain nombre de méthodes de classification automatique\footnote{
  Pour une présentation plus détaillée de la classification ascendante hiérarchique,
  voir le chapitre \ref{chapitre_cah} \vpageref{chapitre_cah}.
}.

<<eval=FALSE>>=
install.packages(c('TraMineR'))
@

<<message=FALSE>>=
library(TraMineR)
library(cluster)
@

On importe ensuite les données, on recode la variable <<~génération~>> pour lui 
donner des labels plus explicites. On jette également un coup d'\oe{}il à la 
structure du tableau de données~:

<<echo=FALSE>>=
donnees<-read.csv("trajpro.csv",header=T)
@
<<eval=FALSE>>=
donnees<-read.csv("http://nicolas.robette.free.fr/Docs/trajpro.csv",header=T)
@
<<>>=
donnees$generation <- factor(donnees$generation,labels=c('1930-38', '1939-45', '1946-50'))
str(donnees)
@

On a bien 1000 observations et 38 variables. On définit maintenant des labels 
pour les différents états qui composent les séquences et on crée un objet 
<<~séquence~>> avec \rfunc{seqdef}~:

% On déscative les messages car TraMineR produit un caractère >
% qui pose souci quand on génère le PDF
<<message=FALSE>>=
labels <- c("agric","acce","cadr","pint","empl","ouvr","etud","inact","smil")
seq <- seqdef(donnees[,1:37], states=labels)
@

\section{Appariement optimal et classification}

Ces étapes préalables achevées, on peut comparer les séquences en calculant 
les dissimilarités entre paires de séquences. On va ici utiliser la méthode la
plus répandue, l'appariement optimal (\textit{optimal matching}). Cette méthode
consiste, pour chaque paire de séquences, à compter le nombre minimal de modifications
(substitutions, suppressions, insertions) qu'il faut faire subir à l'une des 
séquences pour obtenir l'autre. On peut considérer que chaque modification 
est équivalente, mais il est aussi possible de prendre en compte le fait que les 
<<~distances~>> entre les différents états n'ont pas toutes la même <<~valeur~>>
(par exemple, la distance sociale entre emploi à temps plein et chômage est 
plus grande qu'entre emploi à temps plein et emploi à temps partiel), en 
assignant aux différentes modifications des <<~coûts~>> distincts. Dans notre 
exemple, on va créer avec \rfunc{seqsubm} une <<~matrice des coûts de substitution~>>
dans laquelle tous les coûts sont constants et égaux à 2\footnote{
  Le fonctionnement de l’algorithme d’appariement optimal --- et notamment le 
  choix des coûts --- est décrit dans le chapitre 3 du manuel de \textsf{TraMineR}.
}~:

<<message=FALSE>>=
  couts <- seqsubm(seq,method="CONSTANT", cval=2)
@


Ensuite, on calcule la matrice de distance entre les séquences (i.e contenant 
les <<~dissimilarités~>> entre les séquences) avec \rfunc{seqdist}, avec un 
coût d'insertion/suppression (\textit{indel})  que l'on fixe ici à 1,1~:

<<message=FALSE>>=
seq.om <- seqdist(seq, method="OM", indel=1.1, sm=couts)
@

Cette matrice des distances ou des dissimilarités entre séquences peut ensuite 
être utilisée pour une classification ascendante hiérarchique (CAH), qui permet 
de regrouper les séquences en un certain nombre de <<~classes~>> en fonction de 
leur proximité~:

<<>>=
seq.agnes <- agnes(as.dist(seq.om), method="ward", keep.diss=FALSE)
@

\begin{figure}
<<>>=
plot(as.dendrogram(seq.agnes), leaflab="none")
@
\caption{Dendrogramme de la classification des séquences}
\label{fig_dendrogramme_seq}
\end{figure}

\begin{figure}
<<>>=
plot(sort(seq.agnes$height, decreasing=TRUE)[1:20], type="s", xlab="nb de classes", ylab="inertie")
@
\caption{Sauts d'inertie de la classification des séquences}
\label{fig_sauts_inertie_seq}
\end{figure}

Avec la fonction \rfunc{plot}, il est possible de tracer l'arbre de la 
classification (dendrogramme). L'observation, sur ce dendogramme (figure 
\ref{fig_dendrogramme_seq} \vpageref{fig_dendrogramme_seq}) ou sur la courbe des 
sauts d'inertie (figure \ref{fig_sauts_inertie_seq} \vpageref{fig_sauts_inertie_seq}), 
des sauts d'inertie des dernières étapes de la classification peut servir de guide
pour déterminer le nombre de classes que l'on va retenir pour la suite des analyses.
Une première inflexion dans la courbe des sauts d'inertie apparaît au niveau d'une 
partition en 5 classes. On voit aussi une seconde inflexion assez nette à 7 classes.
Mais il faut garder en tête le fait que ces outils ne sont que des guides, le 
choix devant avant tout se faire après différents essais, en fonction de l'intérêt 
des résultats par rapport à la question de recherche et en arbitrant entre 
exhaustivité et parcimonie.

On fait ici le choix d'une partition en 5 classes~:

<<>>=
nbcl <- 5
seq.part <- cutree(seq.agnes, nbcl)
seq.part <- factor(seq.part,labels=paste('classe',1:nbcl,sep='.'))
@

\section{Représentations graphiques}

Pour se faire une première idée de la nature des classes de la typologie, il 
existe un certain nombre de représentations graphiques. Les chronogrammes 
(\textit{state distribution plots}) présentent une série de coupes transversales~:
pour chaque âge, on a les proportions d'individus de la classe dans les 
différentes situations (agriculteur, étudiant, etc.). Ce graphique s'obtient
avec \rfunc{seqdplot} (voir figure \ref{fig_seqdplot} \pageref{fig_seqdplot})

\begin{figure}
<<>>=
seqdplot(seq, group=seq.part, xtlab=14:50, border=NA, withlegend=T)
@
\caption{Chronogrammes}
\label{fig_seqdplot}
\end{figure}


Chacune des classes semble caractérisée par un groupe professionnel principal~:
profession intermédiaire pour la classe 1, ouvrier pour la 2, employé pour la 3,
cadre pour la 4 et indépendant pour la 5. Cependant, on aperçoit aussi des 
<<~couches~>> d'autres couleurs, indiquant que l'ensemble des carrières ne sont 
probablement pas stables.

Les <<~tapis~> (\textit{index plots}), obtenus avec \rfunc{seqiplot},
permettent de mieux visualiser la dimension
individuelle des séquences. Chaque segment horizontal représente une séquence, 
découpée en sous-segments correspondant aux aux différents états successifs qui 
composent la séquence (voir figure \ref{fig_seqiplot} \vpageref{fig_seqiplot}).

\begin{figure}
<<>>=
seqiplot(seq, group=seq.part, xtlab=14:50, tlim=0, space=0, border=NA, withlegend=T, yaxis=FALSE)
@
\caption{Tapis des séquences}
\label{fig_seqiplot}
\end{figure}

Il est possible de trier les séquences pour rendre les tapis plus lisibles 
(on trie ici par \textit{multidimensional scaling}, voir figure \ref{fig_seqiplot2}
\vpageref{fig_seqiplot2}).

\begin{figure}
<<>>=
ordre <- cmdscale(as.dist(seq.om),k=1)
seqiplot(seq, group=seq.part, sortv=ordre, xtlab=14:50, tlim=0, space=0, border=NA, withlegend=T, yaxis=FALSE)
@
\caption{Tapis des squences triés par multidimensional scaling}
\label{fig_seqiplot2}
\end{figure}

On voit mieux apparaître ainsi l'hétérogénéité de certaines classes. Les classes 
1, 3 et 4, par exemple, semblent regrouper des carrières relativement stables 
(respectivement de professions intermédiaires, d'employés et de cadres) et des 
carrières plus <<~mobiles~>> commencées comme ouvrier (classes 1 et 3, en orange)
ou comme profession intermédiaire (classe 4, en rouge). De même, la majorité des 
membres de la dernière classe commencent leur carrière dans un groupe professionnel 
distinct de celui qu'ils occuperont par la suite (indépendants). Ces distinctions 
apparaissent d'ailleurs si on relance le programme avec un nombre plus élevé de classes 
(en remplaçant le \texttt{5} de la ligne \texttt{nbcl <- 5} par \texttt{7}, 
seconde inflexion de la courbe des sauts d'inertie, et en exécutant de nouveau 
le programme à partir de cette ligne)~: les stables et les mobiles se trouvent 
alors dans des classes distinctes.

\begin{astuce}
  Une astuce publiée sur \url{http://joseph.larmarange.net/?article137} permet de
  représenter le tapis de l'ensemble des séquences selon l'ordre du dendrogramme.
  On commencera par recopier dans \R le code de la fonction \rfunc{seq.heatmap}
  qu'il suffit ensuite d'appeler.
  
  <<echo=FALSE>>=
	seq.heatmap <- function (seq, tree, with.missing=FALSE, ...) {
		if (class(tree)!="dendrogram") tree <- as.dendrogram(tree)
		mat <- seq
		for (i in 1:length(seq)){
			mat[mat[,i]=="%",i] <- NA
			mat[,i] <- as.numeric(mat[,i])
		}
		mat <- as.matrix(mat)
		col <- attr(seq,"cpal")
		if (with.missing) col <- c(col,attr(seq,"missing.color"))
		heatmap(mat, tree, NA,  na.rm=FALSE, col=col, scale="none", labRow=NA, ...)	
	}
  @
  <<warning=FALSE>>=
  seq.heatmap(seq,seq.agnes,labCol=14:50)	
  @
\end{astuce}

\clearpage
La distance moyenne des séquences d'une classe au centre de cette classe, obtenue
avec \rfunc{disscenter}, permet de mesurer plus précisément l'homogénéité des classes~:

<<>>=
round(aggregate(disscenter(as.dist(seq.om), group=seq.part), list(seq.part), mean)[,-1],1)
@

Cela nous confirme que les classes 1, 3 et 5 sont nettement plus hétérogènes que 
les autres, alors qua la classe 2 est la plus homogène.

D'autres représentations graphiques existent pour poursuivre l'examen de la typologie. 
On peut visualiser les 10 séquences les plus fréquentes de chaque classe avec 
\rfunc{seqfplot} (figure \ref{fig_seqfplot} \vpageref{fig_seqfplot}).

\begin{figure}
<<>>=
seqfplot(seq, group=seq.part, withlegend=T)
@
\caption{Séquences les plus fréquentes de chaque classe}
\label{fig_seqfplot}
\end{figure}


On peut aussi visualiser avec \rfunc{seqmsplot} l'état modal (celui qui 
correspond au plus grand nombre de séquences de la classe) à chaque âge 
(figure \ref{fig_seqmsplot} \vpageref{fig_seqmsplot}).

\begin{figure}
<<>>=
seqmsplot(seq, group=seq.part, xtlab=14:50, withlegend=T, title="classe")
@
\caption{Statut modal à chaque âge}
\label{fig_seqmsplot}
\end{figure}

On peut également représenter avec \rfunc{seqmtplot} les durées moyennes 
passées dans les différents états (figure \ref{fig_seqmtplot} \vpageref{fig_seqmtplot}).

\begin{figure}
<<>>=
seqmtplot(seq, group=seq.part, withlegend=T)
@
\caption{Durée moyenne dans chaque statut}
\label{fig_seqmtplot}
\end{figure}

Enfin, l'entropie transversale décrit l'évolution de l'homogénéité de la classe. 
Pour un âge donné, une entropie proche de 0 signifie que tous les individus de 
la classe (ou presque) sont dans la même situation. À l'inverse, l'entropie est 
de 1 si les individus sont dispersés dans toutes les situations. Ce type de 
graphique prduit par \rfunc{seqHtplot} peut être pratique pour localiser les 
moments de transition, l'insertion professionnelle ou une mobilité sociale 
ascendante (voir par exemple la figure \ref{fig_seqHtplot} \vpageref{fig_seqHtplot}).

\begin{figure}
<<>>=
seqHtplot(seq, group=seq.part, xtlab=14:50, withlegend=T)
@
\caption{Entropie transversale}
\label{fig_seqHtplot}
\end{figure}

On souhaite maintenant connaître la distribution de la typologie 
(en effectifs et en pourcentages)~:

<<>>=
freq(seq.part)
@

On poursuit ensuite la description des classes en croisant la typologie avec 
la variable \texttt{generation}~:

<<>>=
cprop(table(seq.part,donnees$generation))
chisq.test(table(seq.part,donnees$generation))
@

Le lien entre le fait d'avoir un certain type de carrières et la cohorte de 
naissance est significatif à un seuil de 15~\%. On constate par exemple 
l'augmentation continue de la proportion de carrières de type <<~professions 
intermédiaires~>> (classe 1) et, entre les deux cohortes les plus anciennes, 
l'augmentation de la part des carrières de type <<~employés~>> (classe 3) et 
la baisse de la part des carrières de type <<~cadres~>> (classe 4).

Bien d'autres analyses sont envisageables~: croiser la typologie avec d'autres 
variables (origine sociale, etc.), construire l'espace des carrières possibles, 
étudier les interactions entre trajectoires familiales et professionnelles, 
analyser la variance des dissimilarités entre séquences en fonction de plusieurs 
variables <<~explicatives~>>\footnote{
  L’articulation entre méthodes <<~descriptives~>> et méthodes « explicatives » 
  est un prolongement possible de l’analyse de séquences. Cependant, l’analyse 
  de séquences était envisagée par Abbott comme une alternative à la sociologie 
  quantitative \textit{mainstream}, i.e le <<~paradigme des variables~>> et ses hypothèses 
  implicites souvent difficilement tenables (Abbott, 2001). Une bonne description
  solidement fondée théoriquement vaut bien des <<~modèles explicatifs~>> (Savage, 2009).
}... Mais l'exemple proposé est sans doute bien suffisant pour une première 
introduction~!

\section{Bibliographie}

\begin{itemize}
\item Abbott A., 2001, \textit{Time matters. On theory and method}, The University of Chicago Press.
\item Abbott A., Hrycak A., 1990, « Measuring ressemblance in sequence data : an optimal matching analysis of musicians' careers », \text{American journal of sociology}, (96), p.144-185. \url{http://www.jstor.org/stable/10.2307/2780695}
\item Abbott A., Tsay A., 2000, « Sequence analysis and optimal matching methods in sociology: Review and prospect », \textit{Sociological methods \& research}, 29(1), p.3-33. \url{http://smr.sagepub.com/content/29/1/3.short}
\item Gabadinho, A., Ritschard, G., Müller, N.S. \& Studer, M., 2011, « Analyzing and visualizing state sequences in R with TraMineR », \textit{Journal of Statistical Software}, 40(4), p.1-37. \url{http://archive-ouverte.unige.ch/downloader/vital/pdf/tmp/4hff8pe6uhukqiavvgaluqmjq2/out.pdf}
\item Grelet Y., 2002, « Des typologies de parcours. Méthodes et usages », \textit{Document Génération} 92, (20), 47 p. \url{http://www.cmh.greco.ens.fr/programs/Grelet_typolparc.pdf}
\item Lelièvre É., Vivier G., 2001, « Évaluation d'une collecte à la croisée du quantitatif et du qualitatif : l'enquête Biographies et entourage », \textit{Population}, (6), p.1043-1073. \url{http://www.persee.fr/web/revues/home/prescript/article/pop_0032-4663_2001_num_56_6_7217}
\item Lemercier C., 2005, « Les carrières des membres des institutions consulaires parisiennes au XIXe siècle », \textit{Histoire et mesure}, XX (1-2), p.59-95. \url{http://histoiremesure.revues.org/786}
\item Lesnard L., 2008, « Off-Scheduling within Dual-Earner Couples: An Unequal and Negative Externality for Family Time », \textit{American Journal of Sociology}, 114(2), p.447-490. \url{http://laurent.lesnard.free.fr/IMG/pdf/lesnard_2008_off-scheduling_within_dual-earner_couples-2.pdf}
\item Lesnard L., Saint Pol T. (de), 2006, « Introduction aux Méthodes d'Appariement Optimal (Optimal Matching Analysis) », \textit{Bulletin de Méthodologie Sociologique}, 90, p.5-25. \url{http://bms.revues.org/index638.html}
\item Robette N., 2011, \textit{Explorer et décrire les parcours de vie: les typologies de trajectoires}, CEPED (Les Clefs pour), 86 p. \url{http://nicolas.robette.free.fr/Docs/Robette2011_Manuel_TypoTraj.pdf}
\item Robette N., 2012, « Du prosélytisme à la sécularisation. Le processus de diffusion de l'Optimal Matching Analysis », document de travail. \url{http://nicolas.robette.free.fr/Docs/Proselytisme_secularisation_NRobette.pdf}
\item Robette N., Bry X., 2012, « Harpoon or bait? A comparison of various metrics to fish for life course patterns », \textit{Bulletin de Méthodologie Sociologique}, 116, p.5-24. \url{http://nicolas.robette.free.fr/Docs/Harpoon_maggot_RobetteBry.pdf}
\item Robette N., Thibault N., 2008, « L'analyse exploratoire de trajectoires professionnelles: analyse harmonique qualitative ou appariement optimal? », \textit{Population}, 64(3), p.621-646. \url{http://www.cairn.info/revue-population-2008-4-p-621.htm}
\item Savage M., 2009, « Contemporary Sociology and the Challenge of Descriptive Assemblage », \textit{European Journal of Social Theory}, 12(1), p.155-174. \url{http://est.sagepub.com/content/12/1/155.short}
\end{itemize}


\chapter{Analyse de survie}

L'anlyse de survie sous \R s'effectue principalement avec l'extension 
\textsf{survival}. Nous n'aborderons pas l'analyse de survie ici. 
Mais plusieurs ressources sont disponibles en ligne~:

\begin{itemize}
\item \url{http://www-irma.u-strasbg.fr/~geffray/cours/cours-nantes/M2polyR2008-2009.pdf}
\item \url{http://www.mastergbi.fr/_media/members/slemler/courssurvie.pdf}
\item \url{http://www.mastergbi.fr/_media/members/slemler/survie5.pdf}
\item \url{http://ljk.imag.fr/membres/Anatoli.Iouditski/cours/M1MAI/dm_ch17.pdf}
\end{itemize}

\chapter{Exporter les résultats}

Cette partie décrit comment, une fois les analyses réalisées, on peut
exporter les résultats (tableaux et graphiques) dans un traitement de
texte ou une application externe.


\section{Export manuel de tableaux}

Les tableaux générés par \R (et plus largement, tous les types
d'objets) peuvent être exportés pour inclusion dans un traitement de
texte à l'aide de la fonction \rfunc{copy} \marqr de l'extension
\questionr\footnote{Celle-ci nécessite que l'extension \texttt{R2HTML} soit
  également installée sur le système \textit{via}
  \texttt{install.packages("R2HTML",dep=TRUE)}.}.

Il suffit pour cela de lui passer en argument le tableau ou l'objet
qu'on souhaite exporter. Dans ce qui suit on utilisera le tableau
suivant, placé dans un objet nommé \texttt{tab}~:

<<>>=
data(hdv2003)
tab <- table(hdv2003$sexe,hdv2003$bricol)
tab
@ 


\subsection{Copier/coller vers \textsf{Excel} et \textsf{Word} \textit{via} le presse-papier}

La première possibilité est d'utiliser les options par défaut de
\rfunc{copy}. Celle-ci va alors transformer le tableau (ou l'objet)
en \texttt{HTML} et placer le résultat dans le presse papier du
système. Ceci ne fonctionne malheureusement que sous
\textsf{Windows}\footnote{En fait cela fonctionne aussi sous
  \textsf{Linux} si le programme \texttt{xclip} est installé et
  accessible. Cela fonctionne peut-être aussi sous \textsf{Mac OS X}
  mais n'a pas pu être testé.}.

<<>>=
copy(tab)
@ 

On peut ensuite récupérer le résultat dans une feuille \textsf{Excel} en
effectuant un simple \textit{Coller}.

\begin{center}
  \includegraphics[width=6cm]{img/copie_tableau_excel.png}
\end{center}
  
On peut ensuite sélectionner le tableau sous \textsf{Excel}, le copier
et le coller dans \textsf{Word}~:

\begin{center}
  \includegraphics[width=6cm]{img/copie_tableau_word.png}
\end{center}


\subsection{Export vers \textsf{Word} ou \textsf{OpenOffice/LibreOffice} \textit{via} un fichier}

L'autre possibilité ne nécessite pas de passer par \textsf{Excel}, et
fonctionne sous \textsf{Word}, \textsf{OpenOffice} et \textsf{LibreOffice }sur
toutes les plateformes.

Elle nécessite de passer à la fonction \rfunc{copy} l'option
\texttt{file=TRUE} qui enregistre le contenu de l'objet dans un
fichier plutôt que de le placer dans le presse-papier~:

<<>>=
copy(tab, file=TRUE)
@ 


Par défaut le résultat est placé dans un fichier nommé
\texttt{temp.html} dans le répertoire courant, mais on peut modifier
le nom et l'emplacement avec l'option \texttt{filename}~:

<<eval=FALSE>>=
copy(tab, file=TRUE, filename="exports/tab1.html")
@ 

On peut ensuite l'intégrer directement dans \textsf{Word} ou dans
\textsf{OpenOffice} en utilisant le menu \textit{Insertion} puis
\textit{Fichier} et en sélectionnant le fichier de sortie généré précédemment.

\begin{center}
  \includegraphics[width=4cm]{img/export_tableau_ooo.png}
\end{center}


\section{Export de graphiques}

\subsection{Export \textit{via} l'interface graphique (\textsf{Windows} ou \textsf{Mac OS X})}

L'export de graphiques est très simple si on utilise l'interface
graphique sous \textsf{Windows}. En effet, les fenêtres graphiques
possèdent un menu \textit{Fichier} qui comporte une entrée
\textit{Sauver sous} et une entrée \textit{Copier dans le presse papier}.

L'option \textit{Sauver sous} donne le choix entre plusieurs formats
de sortie, vectoriels (\textsf{Metafile}, \textsf{Postscript}) ou
bitmaps (\textsf{jpeg}, \textsf{png}, \textsf{tiff}, etc.). Une fois
l'image enregistrée on peut ensuite l'inclure dans n'importe quel
document ou la retravailler avec un logiciel externe.

\begin{remarque}
  Une image \textit{bitmap} est une image stockée sous forme de
  points, typiquement une photographie. Une image \textit{vectorielle}
  est une image enregistrée dans un langage de description,
  typiquement un schéma ou une figure. Le second format présente
  l'avantage d'être en général beaucoup plus léger et d'être
  redimensionnable à l'infini sans perte de qualité. Pour plus
  d'informations voir
  \url{http://fr.wikipedia.org/wiki/Image_matricielle} et
  \url{http://fr.wikipedia.org/wiki/Image_vectorielle}.
\end{remarque}

L'option \textit{Copier dans le presse papier} permet de placer le
contenu de la fenêtre dans le presse-papier soit dans un format
vectoriel soit dans un format bitmap. On peut ensuite récupérer le
résultat dans un traitement de texte ou autre avec un simple
\textit{Coller}.

Des possibilités similaires sont offertes par l'interface sous
\textsf{Mac OS X}, mais avec des formats proposés un peu différents.

\begin{rstudio}
  Avec \textsf{RStudio}, les commandes d'export sont situées dans le menu
  \textit{Plots} qui comporte les entrées \textit{Save Plot as image} et
  \textit{Save Plot as PDF}. Ces mêmes commandes sont accessibles via le 
  bouton \textit{Export} situé au dessus du graphique dans le quadrant
  bas-droit. Les options d'export sont plus importantes que celle de l'interface
  graphique de base, avec notamment le support du format SVG ou encore la 
  possibilité de modifier la taille du graphique exporté.
\end{rstudio}

\subsection{Export avec les commandes de \R}

On peut également exporter les graphiques dans des fichiers de
différents formats directement avec des commandes \R. Ceci a
l'avantage de fonctionner sur toutes les plateformes, et de faciliter
la mise à jour du graphique exporté (on n'a qu'à relancer les
commandes concernées pour que le fichier externe soit mis à jour).

La première possibilité est d'exporter le contenu d'une fenêtre déjà
existante à l'aide de la fonction \rfunc{dev.}. On doit fournir à
celle-ci le format de l'export (option \texttt{device}) et le nom du
fichier (option \texttt{file}). Par exemple~:

<<devcopy,eval=FALSE>>=
boxplot(rnorm(100))
dev.print(device=png, file="export.png", width=600)
@ 

Les formats de sortie possibles varient selon les plateformes, mais
on retrouve partout les formats bitmap \textsf{bmp}, \textsf{jpeg},
\textsf{png}, \textsf{tiff}, et les formats vectoriels
\textsf{postscript} ou \textsf{pdf}. La liste complète disponible pour
votre installation de \R est disponible dans la page d'aide de \texttt{Devices}~:

<<helpdevices,eval=FALSE,tidy=FALSE>>=
?Devices
@ 

L'autre possibilité est de rediriger directement la sortie graphique
dans un fichier, avant d'exécuter la commande générant la figure. On
doit pour cela faire appel à l'une des commandes permettant cette
redirection. Les plus courantes sont \rfunc{bmp}, \rfunc{png},
\rfunc{jpeg} et \rfunc{tiff} pour les formats bitmap,
\rfunc{postscript}, \rfunc{pdf}, \rfunc{svg}\footnote{Ne fonctionne
  pas sous \textsf{Word}.} et \rfunc{win.metafile}\footnote{Ne
  fonctionne que sous \textsf{Word}.} pour les formats vectoriels.

Les formats vectoriels ont l'avantage de pouvoir être redimensionnés à volonté
sans perte de qualité, et produisent des fichiers en général de plus petite
taille. On pourra donc privilégier le format SVG, par exemple, si on utilise
\textsf{LibreOffice} ou \textsf{OpenOffice}.

Ces fonctions prennent différentes options permettant de personnaliser
la sortie graphique. Les plus courantes sont \texttt{width} et
\texttt{height} qui donnent la largeur et la hauteur de l'image
générée (en pixels pour les images bitmap, en pouces pour les images
vectorielles), et \texttt{pointsize} qui donne la taille de base des
polices de caractère utilisées.

<<eval=FALSE>>=
png(file="out.png", width=800, height=700)
plot(rnorm(100))
dev.off()

pdf(file="out.pdf", width=9, height=9, pointsize=10)
plot(rnorm(150))
dev.off()

@ 

Il est nécessaire de faire un appel à la fonction \rfunc{dev.off}
après génération du graphique pour que le résultat soit bien écrit
dans le fichier de sortie (dans le cas contraire on se retrouve avec
un fichier vide).


\section{Génération automatique de documents avec \textsf{OpenOffice} ou \textsf{LibreOffice}}

Les méthodes précédentes permettent d'exporter tableaux et graphiques,
mais cette opération reste manuelle, un peu laborieuse et répétitive,
et surtout elle ne permet pas de mise à jour facile des documents
externes en cas de modification des données analysées ou du code.

\R et son extension \textsf{odfWeave} permettent de résoudre en partie
ce problème. Le principe de base est d'inclure du code \R dans un
document de type traitement de texte, et de procéder ensuite au
remplacement automatique du code par le résultat sous forme de texte,
de tableau ou de figure.

À noter qu'\textsf{odfWeave} n'est pas la seule extension proposant ce type de
fonctionnalités, on citera notamment \textsf{knitr}, présentée
section~\ref{sec-knitr} \vpageref{sec-knitr}, plus utilisée et plus versatile.
\textsf{odfWeave} a l'avantage de fournir directement en sortie un document au
format \textsf{OpenDocument}, mais présente l'inconvénient de devoir saisir le
code \R dans \textsf{LibreOffice}, sans les facilités d'édition d'un outil
spécifique à \R, et sans pouvoir exécuter ce code de manière interactive.

\subsection{Prérequis}

\textsf{odfWeave} ne fonctionne qu'avec des documents au format
\textsf{OpenDocument} (extension \texttt{.odt}), donc en particulier avec
\textsf{OpenOffice} ou \textsf{LibreOffice} mais pas avec \textsf{Word}.
L'utilisation d'\textsf{OpenOffice} est cependant très proche de celle de
\textsf{Word}, et les documents générés peuvent être ensuite ouverts sous
\textsf{Word} pour édition.

L'installation de l'extension se fait de manière tout à fait
classique~:

<<eval=FALSE>>=
install.packages("odfWeave", dep=TRUE)
@ 

Un autre prérequis est de disposer d'applications permettant de
compresser et décompresser des fichiers au format \texttt{zip}. Or
ceci n'est pas le cas par défaut sous \textsf{Windows}. Pour les
récupérer, téléchargez l'archive à l'adresse suivante~:

\url{http://alea.fr.eu.org/public/files/zip.zip}

Décompressez-là et placez les deux fichiers qu'elle contient
(\texttt{zip.exe} et \texttt{unzip.exe}) dans votre répertoire
système, c'est à dire en général soit
\verb|c:\windows|, soit
\verb|c:\winnt|.


\subsection{Exemple}

Prenons tout de suite un petit exemple. Soit le fichier
\textsf{OpenOffice} représenté figure~\ref{odfweave1} \vpageref{odfweave1}.

\begin{figure}
    \centering
    \includegraphics[width=12cm]{img/odfweave_exemple1_in.png}
    \caption{Exemple de fichier odfWeave}
    \label{odfweave1}
\end{figure}
    
On voit qu'il contient à la fois du texte mis en forme (sous forme de
titre notamment) mais aussi des passages plus ésotériques qui
ressemblent plutôt à du code \R.

Ce code est séparé du reste du texte par les caractères
\texttt{<<>>=}, en haut, et \texttt{@}, en bas.

Créons maintenant un nouveau fichier \R dans le même répertoire que
notre fichier \textsf{OpenOffice}, et mettons-y le contenu suivant~:

<<eval=FALSE>>=
library(odfWeave)
odfWeave("odfWeave_exemple1.odt", "odfWeave_exemple1_out.odt")
@ 

Puis exécutons le tout... Nous devrions alors avoir un nouveau fichier
nommé \texttt{odfWeave\_exemple1\_out.odt} dans notre répertoire de
travail. Si on l'ouvre avec \textsf{OpenOffice}, on obtient le
résultat indiqué figure~\ref{odfweave2} \vpageref{odfweave2}.

\begin{figure}
    \centering
    \includegraphics[width=12cm]{img/odfweave_exemple1_out.png}
    \caption{Résultat de l'exemple de la figure~\ref{odfweave1}}
    \label{odfweave2}
\end{figure}

Que constate-t-on~? Le passage contenant du code \R a été remplacé par
le code \R en question, de couleur bleue, et par son résultat, en
rouge.

Tout ceci est bien sympathique mais un peu limité. La
figure~\ref{odfweave3} \vpageref{odfweave3}, montre un exemple plus
complexe, dont le résultat est indiqué figure~\ref{odfweave4},
\vpageref{odfweave4}.

\begin{figure}
    \centering
    \includegraphics[width=12cm]{img/odfweave_exemple2_in.png}
    \caption{Un fichier odfWeave un peu plus compliqué}
    \label{odfweave3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=12cm]{img/odfweave_exemple2_out.png}
    \caption{Résultat de l'exemple de la figure~\ref{odfweave3}}
    \label{odfweave4}
\end{figure}

Le premier bloc de code \R contient des options entre les séparateurs
\texttt{<<} et \texttt{>>=}. L'option \texttt{echo=FALSE} supprime
l'affichage du code \R (en bleu) dans le document résultat. L'option
\texttt{results=hide} supprime l'affichage du résultat du code (en
rouge). Au final, le code \texttt{library(questionr)} est exécuté, mais
caché dans le document final.

Dans le deuxième bloc, l'option \texttt{results=xml} indique que le
résultat du code ne sera pas du simple texte mais un objet déjà au
format \textsf{OpenOffice} (en l'occurrence un tableau). Le code
lui-même est ensuite assez classique, sauf la dernière instruction
\marqr \rfunc{odfTable.matrix}, qui, appliquée à un objet de type
\texttt{table}, produit le tableau mis en forme dans le document
résultat.

Plus loin, on a dans le cours du texte une chaîne
\texttt{\textbackslash{}Sexpr{sum(tab)}} qui a été remplacée par le
résultat du code qu'elle contient.

Enfin, dans le dernier bloc, l'option \texttt{fig=TRUE} indique que le
résultat sera cette fois une image. Et le bloc est bien remplacé par
la figure correspondante dans le document final.


\subsection{Utilisation}

Le principe est donc le suivant~: un document \textsf{OpenOffice}
classique, avec du texte mis en forme, stylé et structuré de manière
tout à fait libre, à l'intérieur duquel se trouve du code \R. Ce code
est délimité par les caractères \texttt{<<>>=} (avant le code) et
\texttt{@} (après le code). On peut indiquer des options concernant le
bloc de code \R entre les caractères \texttt{<<} et \texttt{>>} de la
chaîne ouvrante. Parmi les options possibles les plus importantes sont~:

\begin{description}
  \item[\texttt{eval}] si \texttt{TRUE} (par défaut), le bloc de code est
    exécuté. Sinon il est seulement affiché et ne produit pas de résultat.
  \item[\texttt{echo}] si \texttt{TRUE} (par défaut), le code \R du bloc est
    affiché dans le document résultat (par défaut en bleu). Si \texttt{FALSE},
    le code est masqué.
  \item[\texttt{results}] indique le type de résultat renvoyé par le
    bloc. Si l'option vaut \texttt{verbatim} (par défaut), le résultat
    de la commande est affiché tel quel (par défaut en rouge). Si elle
    vaut \texttt{xml}, le résultat attendu est un objet
    \textsf{OpenOffice}~: c'est l'option qu'on utilisera lorsqu'on
    fait appel à la fonction \rfunc{odfTable}. Si l'option vaut
    \texttt{hide}, le résultat est masqué.
  \item[\texttt{fig}] si \texttt{TRUE}, indique que le résultat du code
    est une image.
\end{description}

En résumé, si on souhaite utiliser un bloc pour charger des extensions
sans que des traces apparaissent dans le document final, on utilise
\texttt{<<echo=FALSE,results='hide'>>=}. Si on veut afficher un tableau
généré par \rfunc{odfTable}, on utilise
\texttt{<<echo=FALSE,results=xml>>=}. Si on souhaite insérer un
graphique, on utilise \texttt{<<echo=FALSE,fig=TRUE>>=}. Si on
souhaite afficher du code \R et son résultat <<~tel quel~>>, on
utilise simplement \texttt{<<>>=}

Pour générer le document résultat, on doit lancer une session \R
utilisant comme répertoire de travail celui où se trouve le document
\textsf{OpenOffice} source, et exécuter les deux commandes suivantes~:

<<eval=FALSE>>=
library(odfWeave)
odfWeave("fichier_source.odt","fichier_resultat.odt")
@ 

En pratique, on répartit en général son travail entre différents
fichiers \R qu'on appelle ensuite dans le document \textsf{OpenOffice}
à l'aide de la fonction \rfunc{source} histoire de limiter le code \R
dans le document au strict minimum. Par exemple, si on a regroupé le
chargement des données et les recodages dans un fichier nommé
\texttt{recodages.R}, on pourra utiliser le code suivant en début de
document~:

<<echo=TRUE,results='hide',eval=FALSE,prompt=FALSE>>=
source("recodages.R") 
@ 


Et se contenter dans la suite de générer les tableaux et graphiques
souhaités.

\begin{important}
  Il existe un conflit entre les extensions \textsf{R2HTML} et
  \textsf{odfWeave} qui peut empêcher la seconde de fonctionner
  correctement si la première est chargée en mémoire. En cas de
  problème on pourra enlever l'extension \textsf{R2HTML} avec la
  commande \verb!detach(package:R2HTML)!.
\end{important}

Enfin, différentes options sont disponibles pour personnaliser le
résultat obtenu, et des commandes permettent de modifier le style
d'affichage des tableaux et autres éléments générés. Pour plus
d'informations, on se référera à la documentation de l'extension~:

\url{http://cran.r-project.org/web/packages/odfWeave/index.html}

et notamment au document d'introduction en anglais~:

\url{http://cran.r-project.org/web/packages/odfWeave/vignettes/odfWeave.pdf}





\section{Génération automatique de documents avec \textsf{knitr}}
\label{sec-knitr}

\textsf{knitr} est une extension \R, développée par Yihui Xie, qui permet de
mélanger du code \R dans des documents de différents formats et de produire en
retour des documents comportant, à la place du code en question, le résultat de son
exécution (texte, tableaux, graphiques, etc.).

Site officiel de l'extension~:

\url{http://yihui.name/knitr/}

\textsf{knitr} est extrêmement versatile, et permet d'inclure du code R dans
des documents suivant différents formats. On pourra ainsi l'utiliser avec du
\LaTeX, du \textsf{Markdown} ou du \textsf{HTML}.

\textsf{RStudio}\footnote{Pour plus d'informations sur \textsf{RStudio}, voir
  section~\ref{sec_rstudio} \vpageref{sec_rstudio}.} propose une interface
pratique à \textsf{knitr}\footnotemark. On peut ainsi facilement créer un
fichier \textit{R Markdown}, \textit{R HTML} ou \textit{R Sweave} et, d'un
clic, générer des fichiers HTML pour les deux premiers formats, ou PDF pour le
dernier.

\footnotetext{\textsf{knitr} peut aussi parfaitement s'utiliser en ligne de
  commande sans passer par \textsf{RStudio}}


\subsection{Exemple}

Voyons tout de suite un exemple. Dans \textsf{RStudio}, choisissez le menu
\textit{File}, puis \textit{New}, puis \textit{R Markdown}. Un nouveau
document s'ouvre. Effacez son contenu et remplacez le par quelque chose comme~:

\begin{greyverb}
\begin{verbatim}
Exemple de titre
================

Ceci est un paragraphe avec du texte en *italique*, en **gras** et en 
`police à chasse fixe`.

Ensuite vient un bloc de code R qui affiche du texte :

```{r exemple1}
data(iris)
mean(iris$Sepal.Width)
```

Grâce à `xtable`, on peut aussi afficher des tableaux avec le code suivant :

```{r exempletab, results='asis'}
library(xtable)
tab <- table(iris$Species, cut(iris$Sepal.Width, breaks=3))
print(xtable(tab), type="html")
```

Et on peut, enfin, inclure des graphiques directement :

```{r exemplegraph, echo=FALSE}
plot(iris$Sepal.Width, iris$Sepal.Length, col="red")
```
\end{verbatim} 
\end{greyverb}


Enregistrez le fichier avec un nom de votre choix suivi de l'extension
\texttt{.Rmd}, puis cliquez sur le bouton \textit{Knit HTML}. Vous devriez
voir apparaître une fenêtre ressemblant à la figure~\ref{fig_knitr_html}
\vpageref{fig_knitr_html}.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{img/knitr_html_out.png}
  \end{center}
  \caption{Résultat de la génération d'un document HTML par \textsf{knitr}}
  \label{fig_knitr_html}
\end{figure}

Vous avez ensuite la possibilité d'enregistrer ce fichier HTML, ou même,
\textit{via} le bouton \textit{Publish}, de le mettre en ligne sur le site
\textit{Rpubs} (\url{http://rpubs.com}) pour pouvoir le partager facilement.

Si vous n'utilisez pas \textsf{RStudio}, vous pouvez appliquer \textsf{knitr}
à votre fichier \textit{R Markdown} en lançant R dans le même répertoire et en
utilisant le code suivant~:

<<r knitrcmdline, eval=FALSE>>=
library(knitr)
knit2html("test.Rmd")
@ 

Le résultat se trouvera dans le fichier \texttt{test.html} du même répertoire.


\subsection{Syntaxe}

Dans l'exemple précédent, il faut bien différencier ce qui relève de la
syntaxe de \textit{Markdown} et ce qui relève de la syntaxe de \textsf{knitr}.

\textit{Markdown} est un langage de balisage permettant de mettre en forme du
texte en désignant des niveaux de titre, du gras, des listes à puce, etc.
Ainsi, du texte placé entre deux astérisques sera mis en italique, une ligne
soulignée par des caractères \texttt{=} sera transformée en titre de niveau 1, etc.
Dans \textsf{RStudio}, choisissez le menu \textit{Help} puis \textit{Markdown
  Quick Reference} pour afficher un aperçu des différentes possibilités de
mise en forme.

Ensuite, le document contient plusieurs blocs de code \R. Ceux-ci sont
délimités par la syntaxe suivante\footnote{Ces délimiteurs seront différents
  pour d'autres formats de documents, comme \textit{Sweave} ou \texttt{Rhtml}.}~:

\begin{greyverb}
\begin{verbatim}
```{r nom_du_bloc, options}
```
\end{verbatim}
\end{greyverb}

Le bloc commence et se termine par trois quotes inverses suivie, entre
accolades, du langage utilisé dans le bloc (ici toujours \texttt{r}), du nom
du bloc (ce qui permet de l'identifier facilement en cas d'erreur), et d'une
liste d'options éventuelles séparées par des virgules.

Ces options permettent de modifier le comportement du bloc. Par exemple,
spécifier \texttt{echo=FALSE} fera que le code \R ne sera pas affiché dans le
document final, \texttt{fig.width=8} modifiera la largeur des images générées
pour un graphique, etc. Un aperçu des principales options peut être trouvé à
l'adresse suivante~:

\url{http://rpubs.com/gallery/options}

Et la liste exhaustive se trouve ici~:

\url{http://yihui.name/knitr/options}

\subsection{Aller plus loin}

L'objectif ici était de présenter un aperçu de l'intérêt et des
possibilités de \textsf{knitr}. Grâce à ce système, le code \R peut être
intégré directement aux analyses, et le document final contient le résultat de
l'exécution de ce code. La mise à jour de l'ensemble de ces résultats (en cas de
modification des données par exemple) peut alors se faire d'un simple clic ou
d'une seule commande. L'intérêt en terme de reproductibilité des recherches
est également énorme.

\textsf{knitr} est très versatile, permet de générer des documents dans de
nombreux formats, et évolue rapidement. L'utilisation de programmes
auxiliaires comme \textsf{pandoc} permettent même de générer des documents au
format traitement de texte, par exemple.

Pour aller au-delà de l'exemple donné ici, on trouvera de nombreuses
ressources en ligne sur les possibilités et l'utilisation de \textsf{knitr}.
L'extension propose même une démonstration permettant de modifier un fichier
\textit{R Markdown} et de voir le résultat juste en appuyant sur la touche
\texttt{F4}. Pour lancer cette démonstration~:

<<r demoknitr,eval=FALSE>>=
if (!require('shiny')) install.packages('shiny')
demo('notebook', package = 'knitr')
@ 

\chapter{Où trouver de l'aide}
\label{sec_aide}



\section{Aide en ligne}

\R dispose d'une aide en ligne très complète, mais dont l'usage n'est
pas forcément très simple. D'une part car elle est intégralement en
anglais, d'autre part car son organisation prend un certain temps à
être maîtrisée.

\subsection{Aide sur une fonction}

La fonction la plus utile est sans doute celle qui permet d'afficher
la page d'aide liée à une ou plusieurs fonctions. Celle-ci permet de
lister les arguments de la fonction, d'avoir des informations
détaillées sur son fonctionnement, les résultats qu'elle retourne,
etc.

Pour accéder à l'aide de la fonction \texttt{mean}, par exemple, il
vous suffit de saisir directement~:

<<help1,eval=FALSE>>=
help("mean")
@

Ou sa forme abrégée \texttt{?mean}.

Chaque page d'aide comprend plusieurs sections, en particulier~:

\begin{description}
\item[Description] donne un résumé en une phrase de ce que fait la fonction
\item[Usage] indique la ou les manières de l'utiliser
\item[Arguments] détaille tous les arguments possibles et leur signification
\item[Value] indique la forme du résultat renvoyé par la fonction
\item[Details] apporte des précisions sur le fonctionnement de la fonction
\item[Note] pour des remarques éventuelles
\item[References] pour des références bibliographiques ou des URL associées
\item[See Also] \textit{très utile}, renvoie vers d'autres fonctions
  semblables ou liées, ce qui peut être très utile pour découvrir ou
  retrouver une fonction dont on a oublié le nom
\item[Examples] série d'exemples d'utilisation
\end{description}

Les exemples peuvent être directement exécutés en utilisant la
fonction \rfunc{example}~:

<<helpexample>>=
example(mean)
@

\subsection{Naviguer dans l'aide}

La fonction \rfunc{help.start} permet d'afficher le contenu de l'aide
en ligne au format HTML dans votre navigateur Web. Pour comprendre ce
que cela signifie, saisissez simplement~:

<<help3,eval=FALSE>>=
help.start()
@

Ceci devrait lancer votre navigateur favori et afficher une page vous
permettant alors de naviguer parmi les différentes extensions
installées, d'afficher les pages d'aide des fonctions, de consulter les
manuels, d'effectuer des recherches, etc.

À noter qu'à partir du moment où vous avez lancé
\rfunc{help.start()}, les pages d'aide demandées avec
\texttt{help("lm")} ou \texttt{?plot} s'afficheront désormais dans
votre navigateur.

Si vous souhaitez rechercher quelque chose dans le contenu de l'aide
directement dans la console, vous pouvez utiliser la fonction
\rfunc{help.search} (ou \rfunc{??} qui est équivalente), qui renvoie une liste 
des pages d'aide contenant les termes recherchés. Par exemple~:

<<help4,eval=FALSE>>=
help.search("logistic") # equivalent a ??logistic
@

	
\begin{rstudio}
  Dans \textsd{RStudio}, les pages d'aide en ligne s'ouvriront dans le quadrant
  bas-droite sous l'onglet \textit{Help}. Un clic sur l'icône en forme de maison
  vous affichera la page d'accueil de l'aide.
\end{rstudio}

\section{Ressources sur le Web}

De nombreuses ressources existent en ligne, mais la plupart sont en
anglais.


\subsection{Moteur de recherche}

Le fait que le logiciel s'appelle \R ne facilite malheureusement pas
les recherches sur le Web\ldots La solution à ce problème a été
trouvée grâce à la constitution d'un moteur de recherche \textit{ad
  hoc} à partir de Google, nommé \textsf{Rseek}~:

\url{http://www.rseek.org/}

Les requêtes saisies dans \textsf{Rseek} sont exécutées dans des
corpus prédéfinis liés à \R, notamment les documents et manuels, les
listes de discussion ou le code source du programme.

Les requêtes devront cependant être formulées en anglais.


\subsection{Aide en ligne}

Le site \textsf{R documentation} propose un accès clair et rapide à la
documentation de \R et des extensions hébergées sur le CRAN. Il permet
notamment de rechercher et naviguer facilement entre les pages des différentes
fonctions~:

\url{http://www.rdocumentation.org/}

\subsection{Ressources officielles}

La documentation officielle de \R est accessible en ligne depuis le
site du projet~:

\url{http://www.r-project.org/}

Les liens de l'entrée \textit{Documentation} du menu de gauche vous
permettent d'accéder à différentes ressources.
\paragraph{Les manuels} sont des documents complets de présentation de
certains aspects de \R. Ils sont accessibles en ligne, ou
téléchargeables au format PDF~:

\url{http://cran.r-project.org/manuals.html}

On notera plus particulièrement \textit{An
  introduction to R}, normalement destiné aux débutants, mais qui
nécessite quand même un minimum d'aisance en informatique et en
statistiques~:

\url{http://cran.r-project.org/doc/manuals/R-intro.html}

\textit{R Data Import/Export} explique notamment comment importer des
données depuis d'autres logiciels~:

\url{http://cran.r-project.org/doc/manuals/R-data.html}

\paragraph{Les FAQ} regroupent des questions fréquemment posées et
leurs réponses. À lire donc ou au moins à parcourir avant toute
chose~:

\url{http://cran.r-project.org/faqs.html}

La FAQ la plus utile est la FAQ généraliste sur \R~:

\url{http://cran.r-project.org/doc/FAQ/R-FAQ.html}

Mais il existe également une FAQ dédiée aux questions liées à
\textsf{Windows}, et une autre à la plateforme \textsf{Mac OS X}.

\begin{astuce}
  Les manuels et les FAQ sont accessibles même si vous n'avez pas
  d'accès à Internet en utilisant la fonction \rfunc{help.start()}
  décrite précédemment.
\end{astuce}

\paragraph{Le Wiki} est un site dont les pages sont éditées par les
utilisateurs, à la manière de \textit{Wikipédia}. N'importe quel
visiteur du site peut ainsi rajouter ou modifier des informations sur
tel aspect de l'utilisation du logiciel~:

\url{http://wiki.r-project.org/}

\paragraph{R-announce} est la liste de diffusion électronique
officielle du projet. Elle ne comporte qu'un nombre réduit de messages
(quelques-uns par mois tout au plus) et diffuse les annonces
concernant de nouvelles versions de \R ou d'autres informations
particulièrement importantes. On peut s'y abonner à l'adresse
suivante~:

\url{https://stat.ethz.ch/mailman/listinfo/r-announce}


\paragraph{R Journal} est la <<~revue~>> officielle du projet \R, qui
a succédé début 2009 à la lettre de nouvelles \textit{R News}. Elle
paraît entre deux et cinq fois par an et contient des informations sur
les nouvelles versions du logiciel, des articles présentant des
extensions, des exemples d'analyse\ldots Les parutions sont
annoncées sur la liste de diffusion \textit{R-announce}, et les
numéros sont téléchargeables à l'adresse suivante~:

\url{http://journal.r-project.org/}


\paragraph{Autres documents} On trouvera de nombreux documents dans
différentes langues, en général au format PDF, dans le répertoire
suivant~:

\url{http://cran.r-project.org/doc/contrib/}

Parmi ceux-ci, les cartes de référence peuvent être très utiles, ce
sont des aides-mémoire recensant les fonctions les plus courantes~:

\url{http://cran.r-project.org/doc/contrib/Short-refcard.pdf}

On notera également un document d'introduction en anglais progressif
et s'appuyant sur des méthodes statistiques relativement simples~:

\url{http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf}

Pour les utilisateurs déjà habitués à \textsf{SAS} ou \textsf{SPSS},
le livre \textit{R for SAS and SPSS Users} et le document gratuit qui
en est tiré peuvent être de bonnes ressources, tout comme le site Web
\textit{Quick-R}~:

\url{http://rforsasandspssusers.com/}

\url{http://www.statmethods.net/}

\subsection{Revue} 

La revue \textit{Journal of Statistical Software} est une revue
électronique anglophone, dont les articles sont en accès libre, et qui
traite de l'utilisation de logiciels d'analyse de données dans un
grand nombre de domaines. De nombreux articles (la majorité) sont
consacrés à \R et à la présentation d'extensions plus ou moins
spécialisées.

Les articles qui y sont publiés prennent souvent la forme de tutoriels
plus ou moins accessibles mais qui fournissent souvent une bonne
introduction et une ressource riche en informations et en liens.

Adresse de la revue~:

\url{http://www.jstatsoft.org/}



\subsection{Ressources francophones}

Il existe des ressources en français sur l'utilisation de \R, mais peu
sont réellement destinées aux débutants, elles nécessitent en général
des bases à la fois en informatique et en statistique.

Le document le plus abordable et le plus complet est sans doute
\textit{R pour les débutants}, d'Emmanuel Paradis, accessible au
format PDF~:

\url{http://cran.r-project.org/doc/contrib/Paradis-rdebuts_fr.pdf}

La somme de documentation en français la plus importante liée à \R est
sans nulle doute celle mise à disposition par le \textit{Pôle
  bioinformatique lyonnais}. Leur site propose des cours complets de
statistique utilisant \R~:

\url{http://pbil.univ-lyon1.fr/R/enseignement.html}

La plupart des documents sont assez pointus niveau mathématique et
plutôt orientés biostatistique, mais on trouvera des documents plus
introductifs ici~:

\url{http://pbil.univ-lyon1.fr/R/html/cours1}

Dans tous les cas la somme de travail et de connaissances mise à
disposition librement est impressionnante...

Enfin, le site de Vincent Zoonekynd comprend de nombreuses notes
prises au cours de sa découverte du logiciel. On notera cependant que
l'auteur est normalien et docteur en mathématiques\ldots

\url{http://zoonek2.free.fr/UNIX/48_R_2004/all.html}


\section{Où poser des questions}

La communauté des utilisateurs de \R est très active et en général
très contente de pouvoir répondre aux questions (nombreuses) des
débutants et à celles (tout aussi nombreuses) des utilisateurs plus
expérimentés.

Dans tous les cas, les règles de base à respecter avant de poser une
question sont toujours les mêmes~: avoir cherché soi-même la réponse
auparavant, notamment dans les FAQ et dans l'aide en ligne, et poser
sa question de la manière la plus claire possible, de préférence avec
un exemple de code posant problème.

\subsection{Liste \texttt{R-soc}}

Une liste de discussion a été créée spécialement pour permettre aide
et échanges autour de l'utilisation de \R en sciences sociales. Elle
est hébergée par le CRU et on peut s'y abonner à l'adresse suivante~:

\url{https://listes.cru.fr/sympa/subscribe/r-soc}

Grâce aux services offerts par le site \texttt{gmane.org}, la liste
est également disponible sous d'autres formes (forum Web, blog,
\texttt{NNTP}, fils RSS) permettant de lire et de poster sans avoir à
s'inscrire et à recevoir les messages sous forme de courrier
électronique.

Pour plus d'informations :

\url{http://dir.gmane.org/gmane.comp.lang.r.user.french}


\subsection{StackOverflow}

Le site \textit{StackOverflow} (qui fait partie de la famille des sites
\textit{StackExchange}) comprend une section (anglophone) dédiée à \R qui
permet de poser des questions et en général d'obtenir des réponses assez
rapidement :

\url{http://stackoverflow.com/questions/tagged/r}

La première chose à faire, évidemment, est de vérifier que sa question n'a pas
déjà été posée.


\subsection{Forum Web en français}

Le Cirad a mis en ligne un forum dédié aux utilisateurs de \R, très
actif~:

\url{http://forums.cirad.fr/logiciel-R/index.php}

Les questions diverses et variées peuvent être posées dans la rubrique
\textit{Questions en cours}~:

\url{http://forums.cirad.fr/logiciel-R/viewforum.php?f=3}

Il est tout de même conseillé de faire une recherche rapide sur le
forum avant de poser une question, pour voir si la réponse ne s'y
trouverait pas déjà.


\subsection{Canaux IRC (chat)}

L'IRC, ou \textit{Internet Relay Chat} est le vénérable ancêtre
toujours très actif des messageries instantanées actuelles. Un canal (en
anglais) est notamment dédié aux échanges autour de \R (\texttt{\#R}).

Si vous avez déjà l'habitude d'utiliser IRC, il vous suffit de pointer
votre client préféré sur \textsf{Freenode} (\texttt{irc.freenode.net})
puis de rejoindre l'un des canaux en question.

Sinon, le plus simple est certainement d'utiliser l'interface Web de
Mibbit, accessible à l'adresse~:

\url{http://www.mibbit.com/}

Dans le champ \textit{Connect to IRC}, sélectionnez \textit{Freenode.net},
puis saisissez un pseudonyme dans le champ \textit{Nick} et \texttt{\#R} dans
le champ \textit{Channel}. Vous pourrez alors discuter directement avec les
personnes présentes.

Le canal \texttt{\#R} est normalement peuplé de personnes qui seront très
heureuses de répondre à toutes les questions, et en général l'ambiance y est
très bonne. Une fois votre question posée, n'hésitez pas à être patient et à
attendre quelques minutes, voire quelques heures, le temps qu'un des habitués
vienne y faire un tour.

\subsection{Listes de discussion officielles}

La liste de discussion d'entraide (par courrier électronique) officielle
du logiciel \R s'appelle \texttt{R-help}. On peut s'y abonner à
l'adresse suivante, mais il s'agit d'une liste avec de nombreux
messages~:

\url{https://stat.ethz.ch/mailman/listinfo/r-help}

Pour une consultation ou un envoi ponctuels, le mieux est sans doute
d'utiliser les interfaces Web fournies par \textsf{gmane}~:

\url{http://blog.gmane.org/gmane.comp.lang.r.general}

\texttt{R-help} est une liste avec de nombreux messages, suivie par
des spécialistes de \R, dont certains des développeurs
principaux. Elle est cependant à réserver aux questions
particulièrement techniques qui n'ont pas trouvé de réponses par
d'autres biais.

Dans tous les cas, il est nécessaire avant de poster sur cette liste
de bien avoir pris connaissance du \textit{posting guide}
correspondant~:

\url{http://www.r-project.org/posting-guide.html}


Plusieurs autres listes plus spécialisées existent également, elles
sont listées à l'adresse suivante~:

\url{http://www.r-project.org/mail.html}


\appendix

\chapter{Installer \R}
\label{sec_install}

\section{Installation de \R sous \textsf{Windows}}

Nous ne couvrons ici que l'installation de \R sous Windows. Rappelons
qu'en tant que logiciel libre, \R est librement et gratuitement
installable par quiconque.

La première chose à faire est de télécharger la dernière version du
logiciel. Pour cela il suffit de se rendre à l'adresse suivante~:

\url{http://cran.r-project.org/bin/windows/base/release.htm}

Vous allez alors vous voir proposer le téléchargement d'un fichier
nommé \texttt{R-3.X.X-win.exe} (les \texttt{X} étant remplacés par
les numéros de la dernière version disponible). Une fois ce fichier
sauvegardé sur votre poste, exécutez-le et procédez à l'installation
du logiciel~: celle-ci s'effectue de manière tout à fait classique,
c'est-à-dire en cliquant un certain nombre de fois\footnotemark{} sur
le bouton \textit{Suivant}.

\footnotetext{Voire un nombre de fois certain. Vous pouvez laisser les
options par défaut à chaque étape de l'installation.}

Une fois l'installation terminée, vous devriez avoir à la fois une
magnifique icône \R sur votre bureau ainsi qu'une non moins magnifique
entrée \R dans les programmes de votre menu \textit{Démarrer}. Il ne
vous reste donc plus qu'à lancer le logiciel pour voir à quoi il
ressemble.


\section{Installation de \R sous \textsf{Mac OS X}}

L'installation est très simple~:

\begin{enumerate}
\item Se rendre à la page suivante~: \url{http://cran.r-project.org/bin/macosx/}
\item Télécharger le fichier nommé \texttt{R-3.X.Y.pkg}
\item Procéder à l'installation.
\end{enumerate}



\section{Mise à jour de \R sous \textsf{Windows}}

La méthode conseillée pour mettre à jour \R sur les plateformes
\textsf{Windows} est la suivante\footnote{Méthode conseillée dans
  l'entrée correspondante de la FAQ de \R pour \textsf{Windows}~: \url{http://cran.r-project.org/bin/windows/rw-FAQ.html\#What_0027s-the-best-way-to-upgrade_003f}}~:

\begin{enumerate}
\item Désinstaller \R. Pour cela on pourra utiliser l'entrée
  \textit{Uninstall R} présente dans le groupe \R du menu \textit{Démarrer}.
\item Installer la nouvelle version comme décrit précédemment.
\item Se rendre dans le répertoire d'installation de \R, en général
  \verb|C:\Program Files\R|. Sélectionner le répertoire de l'ancienne
  installation de \R et copier le contenu du dossier nommé
  \texttt{library} dans le dossier du même nom de la nouvelle
  installation. En clair, si vous mettez à jour de \R 3.0.0 vers \R
  3.1.0, copiez tout le contenu du répertoire
  \verb|C:\Program Files\R\R-3.0.0\library| dans
  \verb|C:\Program Files\R\R-3.1.0\library|.
\item Lancez la nouvelle version de \R et exécuter la commande
  \rfunc{update.packages} pour mettre à jour les extensions.
\end{enumerate}


\section{Interfaces graphiques}
\label{sec_guis}

L'interface par défaut sous \textsf{Windows} est celle présentée
figure~\ref{fig_RGui} \vpageref{fig_RGui}. Il en existe d'autres,
plus ou moins sophistiquées, qui vont de la simple coloration
syntaxique à des interfaces plus complètes se rapprochant de modèles
du type \textsf{SPSS}. Une liste des projets en cours est disponible
sur la page suivante~:

\url{http://www.sciviews.org/_rgui/} (en anglais)

On pourra notamment regarder l'extension \textsf{R Commander}, qui propose une
interface graphique intégrée à \R pour certaines fonctions de traitement et
d'analyse de données~:

\url{http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/}

Ou encore \textsf{RKWard}, un logiciel multiplateforme proposant une interface
assez complète et des fonctions d'export de résultats~:

\url{http://rkward.sf.net/}

Par ailleurs, le projet \textsf{RStudio} tend à s'imposer comme l'environnement de
développement de référence pour \textsf{R}, d'autant qu'il a l'avantage d'être
libre, gratuit et multiplateforme. Son installation est décrite
section~\ref{sec_rstudio} \vpageref{sec_rstudio}.

Au final, ce document se basant toujours sur une utilisation de \R
basée sur la saisie de commandes textuelles, l'interface choisie
importe peu. Celles-ci ne diffèrent que par le niveau de confort ou
d'efficacité supplémentaires qu'elles apportent.

\section{\textsf{RStudio}}
\label{sec_rstudio}

\textsf{RStudio} est un environnement de développement intégré libre, gratuit,
et qui fonctionne sour \textsf{Windows}, \textsf{Mac OS X} et \textsf{Linux}.
Il fournit un éditeur de script avec coloration syntaxique, des
fonctionnalités pratiques d'édition et d'exécution du code, un affichage
simultané du code, de la console \R, des fichiers, graphiques et pages d'aide,
une gestion des extensions, une intégration avec des systèmes de contrôle de
versions comme \textsf{git}, etc.

Il est en développement actif et de nouvelles fonctionnalités sont ajoutées
régulièrement. Son seul défaut est d'avoir une interface uniquement anglophone.

Pour avoir un aperçu de l'interface de \textsf{RStudio}, on pourra se référer
à la page \textit{Screenshots} du site du projet~:

\url{http://www.rstudio.com/ide/screenshots/}

L'installation de \textsf{RStudio} est très simple, il suffit de se rendre sur
la page de téléchargement et de sélectionner le fichier correspondant à son
système d'exploitation :

\url{http://www.rstudio.com/ide/download/}

L'installation s'effectue ensuite de manière tout à fait classique.

NB : il est préférable d'installer d'abord \R avant de procéder à l'installation de \textsf{RStudio}.

La documentation de \textsf{RStudio} (en anglais) est disponible en ligne à :

\url{http://www.rstudio.com/ide/docs/}

\chapter{Extensions}
\label{sec_extensions}


\section{Présentation}

L'installation par défaut du logiciel \R contient le c\oe{}ur du
programme ainsi qu'un ensemble de fonctions de base fournissant un
grand nombre d'outils de traitement de données et d'analyse
statistiques.

\R étant un logiciel libre, il bénéficie d'une forte communauté
d'utilisateurs qui peuvent librement contribuer au développement du
logiciel en lui ajoutant des fonctionnalités supplémentaires. Ces
contributions prennent la forme d'extensions (\textit{packages})
pouvant être installées par l'utilisateur et fournissant alors
diverses fonctions supplémentaires.

Il existe un très grand nombre d'extensions (environ 1500 à ce
jour), qui sont diffusées par un réseau baptisé \textsf{CRAN}
(\textit{Comprehensive R Archive Network}).

La liste de toutes les extensions disponibles sur le \textsf{CRAN} est
disponible ici~:

\url{http://cran.r-project.org/web/packages/}

Pour faciliter un peu le repérage des extensions, il existe un
ensemble de regroupements thématiques (économétrie, finance,
génétique, données spatiales\ldots) baptisés \textit{Task views}~:

\url{http://cran.r-project.org/web/views/}

On y trouve notamment une \textit{Task view} dédiée aux sciences
sociales, listant de nombreuses extensions potentiellement utiles pour
les analyses statistiques dans ce champ disciplinaire~:

\url{http://cran.r-project.org/web/views/SocialSciences.html}

\section{Installation des extensions}
\label{installation_extensions}

Les interfaces graphiques sous \textsf{Windows} ou \textsf{Mac OS X}
permettent la gestion des extensions par le biais de boîtes de
dialogues (entrées du menu \textit{Packages} sous \textsf{Windows} par
exemple). Nous nous contenterons ici de décrire cette gestion
\textit{via} la console.

\begin{remarque}
  On notera cependant que l'installation et la mise à jour des extensions
  nécessite d'être connecté à l'Internet.
\end{remarque}

L'installation d'une extension se fait par la fonction
\rfunc{install.packages}, à qui on fournit le nom de l'extension. Ici
on souhaite installer l'extension \textsf{ade4}~:

<<instpack,eval=FALSE>>=  
install.packages("ade4", dep=TRUE)
@ 

L'option \texttt{dep=TRUE} indique à \R de télécharger et d'installer
également toutes les extensions dont l'extension choisie dépend
pour son fonctionnement.

En général \R va alors vous demander de choisir un \textit{miroir}
depuis lequel récupérer les données nécessaires. Le plus simple est de
sélectionner le premier miroir de la liste, baptisé \texttt{0-cloud}, qui est
une redirection automatique fournie par les éditeurs de \textsf{RStudio}.

Une fois l'extension installée, elle peut être appelée depuis la
console ou un fichier script avec la commande~:

<<library,eval=FALSE>>=
library(ade4)
@ 

À partir de là, on peut utiliser les fonctions de l'extension,
consulter leur page d'aide en ligne, accéder aux jeux de données
qu'elle contient, etc.

Pour mettre à jour l'ensemble des extensions installées, une seule
commande suffit~:

<<updatepack,eval=FALSE>>=
update.packages()
@ 


Si on souhaite désinstaller une extension précédemment installée, on
peut utiliser la fonction \rfunc{remove.packages}~:

<<removepack,eval=FALSE>>=
remove.packages("ade4")
@ 


\begin{important}
  Il est important de bien comprendre la différence entre
  \rfunc{install.packages} et \rfunc{library}. La première va chercher
  les extensions sur l'Internet et les installe en local sur le disque
  dur de l'ordinateur. On n'a besoin d'effectuer cette opération
  qu'une seule fois. La seconde lit les informations de l'extension
  sur le disque dur et les met à disposition de \R. On a besoin de
  l'exécuter à chaque début de session ou de script.
\end{important}


\section{L'extension \questionr}
\label{sec_questionr}

\questionr est une extension pour R comprenant quelques fonctions
potentiellement utiles pour l'utilisation du logiciel en sciences
sociales, ainsi que différents jeux de données. Elle est développée en
collaboration avec François Briatte.


\subsection{Installation}

L'installation nécessite d'avoir une connexion
active à Internet. L'extension est hébergée
sur le \textsf{CRAN} (\textit{Comprehensive R Archive Network}), le
réseau officiel de diffusion des extensions de \R. Elle est donc
installable de manière très simple, comme n'importe quelle autre
extension, par un simple~:

<<instquestionr,eval=FALSE>>=
install.packages("questionr",dep=TRUE)
@

Si vous souhaitez utiliser la toute dernière version en ligne sur Github, vous
pouvez utiliser la fonction \rfunc{install\_github}, de l'extension \textsf{devtools}~:

<<instgithubquestionr,eval=FALSE>>=
install_github("questionr", "juba")
@

L'extension s'utilise alors de manière classique grâce à l'instruction
\rfunc{library} en début de session ou de fichier \R~:

<<libquestionr,eval=FALSE>>=
library(questionr)
@ 


\subsection{Fonctions et utilisation}

Pour plus de détails sur la liste des fonctions de l'extension et son
utilisation, on pourra se reporter aux pages Web de l'extension, hébergées sur
\textsf{Github}~:

\url{https://github.com/juba/questionr}

Un document PDF (en anglais) regroupant les pages d'aide en ligne de l'extension
est disponible sur le CRAN~:

\url{http://cran.r-project.org/web/packages/questionr/questionr.pdf}

Le même document est également accessible en ligne sur \textsf{R documentation}~:

\url{http://www.rdocumentation.org/packages/questionr}


\subsection{Le jeu de données \texttt{hdv2003}}
\label{sec_hdv2003}

L'extension \questionr contient plusieurs jeux de données
(\textit{dataset}) destinés à l'apprentissage de \R. 

\texttt{hdv2003} est un extrait comportant 2000 individus et 20
variables provenant de l'enquête \textit{Histoire de Vie} réalisée par
l'INSEE en 2003.

L'extrait est tiré du fichier détail mis à disposition librement
(ainsi que de nombreux autres) par l'INSEE à l'adresse suivante~:

\url{http://www.insee.fr/fr/themes/detail.asp?ref_id=fd-HDV03}

Les variables retenues ont été parfois partiellement recodées. La
liste des variables est la suivante~:

\begin{center}
  \begin{tabular}{rl}
    \textbf{Variable} & \textbf{Description} \\
    \hline
    \texttt{id} & Identifiant (numéro de ligne) \\
    \texttt{poids} & Variable de pondération\footnotemark \\
    \texttt{age} & Âge \\
    \texttt{sexe} & Sexe \\
    \texttt{nivetud} & Niveau d'études atteint \\    
    \texttt{occup} & Occupation actuelle \\    
    \texttt{qualif} & Qualification de l'emploi actuel \\        
    \texttt{freres.soeurs} & Nombre total de frères, s\oe{}urs,
    demi-frères et demi-s\oe{}urs \\        
    \texttt{clso} & Sentiment d'appartenance à une classe sociale \\
    \texttt{relig} & Pratique et croyance religieuse \\
    \texttt{trav.imp} & Importance accordée au travail \\
    \texttt{trav.satisf} & Satisfaction ou insatisfaction au travail \\
    \texttt{hard.rock} & Ecoute du Hard rock ou assimilés \\
    \texttt{lecture.bd} & Lecture de bandes dessinées \\    
    \texttt{peche.chasse} & Pêche ou chasse pour le plaisir au cours
    des 12 derniers mois \\
    \texttt{cuisine} & Cuisine pour le plaisir au cours
    des 12 derniers mois \\
    \texttt{bricol} & Bricolage ou mécanique pour le plaisir au cours
    des 12 derniers mois \\
    \texttt{cinema} & Cinéma au cours des 12 derniers mois \\
    \texttt{sport} & Sport ou activité physique pour le plaisir au cours
    des 12 derniers mois \\
    \texttt{heures.tv} & Nombre moyen d'heures passées àregarder la
    télévision par jour \\
  \end{tabular}
\end{center}

\footnotetext{Comme il s'agit d'un extrait du fichier, cette variable
  de pondération n'a en toute rigueur aucune valeur statistique. Elle
  a été tout de même incluse à des fins <<~pédagogiques~>>.}


\subsection{Le jeu de données \texttt{rp99}}
\label{sec_rp99}

\texttt{rp99} est issu du recensement de la population de 1999 de
l'INSEE. Il comporte une petite partie des résultats pour l'ensemble
des communes du Rhône, soit 301 lignes et 21 colonnes


La liste des variables est la suivante~:

\begin{center}
  \begin{tabular}{rl}
    \textbf{Variable} & \textbf{Description} \\
    \hline
    \texttt{nom} & nom de la commune \\
    \texttt{code} & Code de la commune \\
    \texttt{pop.act} & Population active \\
    \texttt{pop.tot} & Population totale \\
    \texttt{pop15} & Population des 15 ans et plus \\    
    \texttt{nb.rp} & Nombre de résidences principales \\    
    \texttt{agric} & Part des agriculteurs dans la population active \\
    \texttt{artis} & Part des artisans, commerçants et chefs d'entreprises \\
    \texttt{cadres} & Part des cadres \\    
    \texttt{interm} & Part des professions intermédiaires \\
    \texttt{empl} & Part des employés \\
    \texttt{ouvr} & Part des ouvriers \\    
    \texttt{retr} & Part des retraités \\
    \texttt{tx.chom} & Part des chômeurs \\
    \texttt{etud} & Part des étudiants\\    
    \texttt{dipl.sup} & Part des diplômés du supérieur \\
    \texttt{dipl.aucun} &  Part des personnes sans diplôme\\
    \texttt{proprio} &  Part des propriétaires parmi les résidences principales\\    
    \texttt{hlm} &  Part des logements HLM parmi les résidences principales\\
    \texttt{locataire} &  Part des locataires parmi les résidences principales\\
    \texttt{maison} &  Part des maisons parmi les résidences principales \\    
  \end{tabular}
\end{center}


\chapter{Solutions des exercices}
\label{sec_solutions}

<<r initsoluces, echo=FALSE, results="hide">>=
opts_chunk$set(size="footnotesize")
@ 

\begin{solution}{simplec}
<<>>=
c(12,13,14,15,16)
@ 
\end{solution}

\begin{solution}{genseq}
<<>>=
c(1,2,3,4)
1:4
c(1,2,3,4,8,9,10,11)
c(1:4, 8:11)
c(2,4,6,8)
1:4 * 2
@ 
\end{solution}


\begin{solution}{sommevect}
<<>>=
chef <- c(1200, 1180, 1750, 2100)
conjoint <- c(1450, 1870, 1690, 0)
nb.personnes <- c(4, 2, 3, 2)
(chef + conjoint) / nb.personnes
@ 
\end{solution}



\begin{solution}{maxvect}
<<>>=
chef <- c(1200, 1180, 1750, 2100)
min(chef)
max(chef)
chef.na <- c(1200, 1180, 1750, NA)
min(chef.na)
max(chef.na)
min(chef.na, na.rm=TRUE)
max(chef.na, na.rm=TRUE)
@ 
\end{solution}


\begin{solution}{simplescript}
<<>>=
library(questionr)
data(hdv2003)
df <- hdv2003
str(df)
@ 
\end{solution}

\begin{solution}{editdf}
Utilisez la fonction suivante et corrigez manuellement les erreurs~:
<<eval=FALSE>>=
df.ok <- edit(df)
@ 
Attention à ne pas utiliser la fonction \texttt{fix} dans ce cas,
celle-ci modifierait directement le contenu de \texttt{df}.

Puis utilisez la fonction \texttt{head}~:

<<eval=FALSE>>=
head(df.ok, 4)
@ 

\end{solution}

\begin{solution}{varquanti}

<<eval=FALSE>>=
summary(df$age)
hist(df$age, breaks=10, main="Répartition des âges", xlab="Âge", ylab="Effectif")
boxplot(df$age)
plot(table(df$age), main="Répartition des âges", xlab="Âge", ylab="Effectif")
t.test(df$age)
@ 
  
\end{solution}

\begin{solution}{varquali}

<<eval=FALSE>>=
table(df$trav.imp)
summary(df$trav.imp)
freq(df$trav.imp)
dotchart(table(df$trav.imp))

@ 

\end{solution}

\begin{solution}{import_tableur}
  Utilisez la fonction \texttt{read.table} ou l'un de ses dérivés, en
  fonction du tableur utilisé et du format d'enregistrement.
  
  Pour vérifier que l'importation s'est bien passée, on peut utiliser
  les fonctions \texttt{str}, \texttt{dim}, éventuellement
  \texttt{edit} et faire quelques tris à plat.
\end{solution}


\begin{solution}{import_dbf}
  Utilisez la fonction \texttt{read.dbf} de l'extension \textsf{foreign}.
\end{solution}





\begin{solution}{manip_rename}
<<>>=
library(questionr)
data(hdv2003)
d <- hdv2003
d <- rename.variable(d, "clso", "classes.sociales")
d <- rename.variable(d, "classes.sociales", "clso")
@ 
\end{solution}

\begin{solution}{manip_factor}
<<>>=
d$clso <- factor(d$clso, levels=c("Non", "Ne sait pas", "Oui"))
table(d$clso)     
@ 
\end{solution}


\begin{solution}{manip_index_direct}
<<>>=
d$cinema[1:3]
d$lecture.bd[12:30]
d[c(5,12), c(4,8)]
longueur <- length(d$age)
tail(d$age,4)
@ 
 
\end{solution}

\begin{solution}{manip_souspop}
<<eval=FALSE>>=
subset(d, lecture.bd=="Oui", select=c(age, sexe))
subset(d, occup!="Chômeur", select=-cinema)
subset(d, age >= 45 & hard.rock=="Oui", select=id)
subset(d, sexe == "Femme" & age >= 25 & age <= 40 & sport == "Non")
subset(d, sexe == "Homme" & freres.soeurs >= 2 & freres.soeurs <= 4 & (cuisine=="Oui" | bricol=="Oui") )
@ 

\end{solution}

\begin{solution}{manip_tapply}
<<>>=
d.bd.oui <- subset(d, lecture.bd=="Oui")
d.bd.non <- subset(d, lecture.bd=="Non")
mean(d.bd.oui$heures.tv)
mean(d.bd.non$heures.tv,na.rm=TRUE)
tapply(d$heures.tv,d$lecture.bd,mean,na.rm=TRUE)
@ 
  
\end{solution}


\begin{solution}{manip_convert}
<<>>=
d$fs.char <- as.character(d$freres.soeurs)
d$fs.fac <- factor(d$fs.char)
d$fs.num <- as.numeric(as.character(d$fs.char))
table(d$fs.num == d$freres.soeurs)
@ 

  
\end{solution}


\begin{solution}{manip_decoup}
<<>>=
d$fs1 <- cut(d$freres.soeurs, 5)
table(d$fs1)
d$fs2 <- cut(d$freres.soeurs, breaks=c(0,2,4,19), include.lowest=TRUE, labels=c("de 0 à 2", "de 2 à 4", "plus de 4"))
table(d$fs2)
d$fs3 <- quant.cut(d$freres.soeurs, 3)
table(d$fs3)
@ 

\end{solution}


\begin{solution}{manip_regroup}
<<r solmanipregroup1,size="footnotesize">>=
d$trav.imp2cl[d$trav.imp == "Le plus important" | d$trav.imp == "Aussi important que le reste"] <- "Le plus ou aussi important"
d$trav.imp2cl[d$trav.imp == "Moins important que le reste" | d$trav.imp == "Peu important"] <- "moins ou peu important"
table(d$trav.imp)
table(d$trav.imp2cl)
table(d$trav.imp,d$trav.imp2cl)
@ 

<<r solmanipregroup2,size="footnotesize">>=
d$relig.4cl <- as.character(d$relig)
d$relig.4cl[d$relig == "Pratiquant regulier" | d$relig == "Pratiquant occasionnel"] <- "Pratiquant"
d$relig.4cl[d$relig=="NSP ou NVPR"] <- NA
table(d$relig.4cl, d$relig, exclude=NULL)
@ 

\end{solution}

\begin{solution}{manip_combine}
  Attention, l'ordre des opérations a toute son importance~!

<<r solmanipcombine,size="footnotesize">>=
d$var <- "Autre"
d$var[d$sexe == "Femme" & d$bricol=="Oui"] <- "Femme faisant du bricolage"
d$var[d$sexe=="Homme" & d$age > 30] <- "Homme de plus de 30 ans"
d$var[d$sexe=="Homme" & d$age > 40 & d$lecture.bd=="Oui"] <- "Homme de plus de 40 ans lecteur de BD"
table(d$var)
table(d$var, d$sexe)
table(d$var, d$bricol)
table(d$var, d$lecture.bd)
table(d$var, d$age > 30)
table(d$var, d$age > 40)
@ 

\end{solution}

\begin{solution}{manip_tri}

<<>>=
d.ord <- d[order(d$freres.soeurs), ]
d.ord <- d[order(d$heures.tv, decreasing=TRUE), c("sexe","heures.tv")]
head(d.ord,10)

@ 
\end{solution}


\begin{solution}{reg_log}
<<>>=
library(MASS)
data(Aids2)
freq(Aids2$status) # Pour visualier les modalités
# On souhaite la probabilité d'être en vie
# La modalité de référence est donc le décès
Aids2$status <- relevel(Aids2$status,"D") 
# Premier modèle
reg <- glm(status ~ state + sex + T.categ + age, data=Aids2, family=binomial(logit))
# Graphique des effets
library(effects)
plot(allEffects(reg))
# Groupes d'âges
Aids2$grpage <- quant.cut(Aids2$age,4)
# Régression
reg2 <- glm(status ~ state + sex + T.categ + grpage, data=Aids2, family=binomial(logit))
# Graphique des effets
plot(allEffects(reg2))
# Calcul des Odds Ratio
exp(coef(reg2))
odds.ratio(reg2) # Alternative
# Recherche d'un meilleur modèle
best.reg <- step(reg2)
@ 
 
\end{solution}

\listoffigures

\printindex


\end{document}
